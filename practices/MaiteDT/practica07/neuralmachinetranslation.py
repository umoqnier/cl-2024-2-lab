# -*- coding: utf-8 -*-
"""NeuralMachineTranslation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Un5kQrcfhrBHbSPDwee7Ath9GeFvxoU

Primero, montamos todo para los datos
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/neural_machine_translation

"""Se preparan las herramientas de procesamiento"""

!git clone https://github.com/ymoslem/MT-Preparation.git

!pip3 install -r MT-Preparation/requirements.txt

"""#Obtención de datos
Dentro del repositorio nuestros datos ya estaban separados en dev, train y test por lo que para entrenar nuestro propio modelo se decidió concatenarlos y preprocesarlos para dividirlos más adelante cuando vayamos a entrenar al modelo
"""

!git clone https://github.com/AmericasNLP/americasnlp2021.git

!cat americasnlp2021/data/nahuatl-spanish/dev.es americasnlp2021/data/nahuatl-spanish/train.es > MT-Preparation/americasnlp2021.es-nah.es

!tail MT-Preparation/americasnlp2021.es-nah.es

!cat americasnlp2021/data/nahuatl-spanish/dev.nah americasnlp2021/data/nahuatl-spanish/train.nah > MT-Preparation/americasnlp2021.nah-es.nah

!tail MT-Preparation/americasnlp2021.nah-es.nah

"""#Filtrado de datos"""

!wc -l MT-Preparation/americasnlp2021.nah-es.nah

!wc -l MT-Preparation/americasnlp2021.es-nah.es

# Arguments: source file, target file, source language, target language
!python3 MT-Preparation/filtering/filter.py MT-Preparation/americasnlp2021.nah-es.nah MT-Preparation/americasnlp2021.es-nah.es nah es

!wc -l MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah

!wc -l MT-Preparation/americasnlp2021.es-nah.es-filtered.es

"""Desde aquí podemos notar la diferencia de tamaño entre lenguajes más estudiados ya que en el laboratorio vimos que de francés a inglés teníamos 59719 palabras y aquí tenemos menos de la mitad :c

#Tokenización
"""

!ls MT-Preparation/subwording/

!python3 MT-Preparation/subwording/1-train_unigram.py MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah MT-Preparation/americasnlp2021.es-nah.es-filtered.es

!ls

!python3 MT-Preparation/subwording/2-subword.py source.model target.model MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah MT-Preparation/americasnlp2021.es-nah.es-filtered.es

!head -n 3 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah && echo "-----" && head -n 3 MT-Preparation/americasnlp2021.es-nah.es-filtered.es

##Con subword
!head -n 10 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword && echo "---" && head -n 10 MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword

"""## Dividimos los datos"""

!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 1000 1000 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword

# Line count for the subworded train, dev, test datatest
!wc -l MT-Preparation/*.subword.*

!echo "---First line---"
!head -n 1 MT-Preparation/*.{train,dev,test}

!echo -e "\n---Last line---"
!tail -n 1 MT-Preparation/*.{train,dev,test}

!pip install OpenNMT-py -U

"""#Configuración y entrenamiento del modelo
Para el dataset "pequeño" del lab se usaron los siguientes parámetros:

- `train_steps=3000`
- `valid_steps=1000`
- `warmup_steps=1000`

Sin embargo, aquí tenemos menos de la mitad de los datos, vemos como le va y consideramos ajustarlos aún más después
"""

# Creación del archivo de configuración
# Usando valores pequeños en vista de que tenemos un corpus limitado
# Para datasets grandes deberian aumentar los valores:
# train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint
SRC_DATA_NAME = "MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword"
TARGET_DATA_NAME = "MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword"

config = f'''# config.yaml

## Where the samples will be written
save_data: run

# Rutas de archivos de entrenamiento
#(previamente aplicado subword tokenization)
data:
    corpus_1:
        path_src: {SRC_DATA_NAME}.train
        path_tgt: {TARGET_DATA_NAME}.train
        transforms: [filtertoolong]
    valid:
        path_src: {SRC_DATA_NAME}.dev
        path_tgt: {TARGET_DATA_NAME}.dev
        transforms: [filtertoolong]

# Vocabularios (serán generados por `onmt_build_vocab`)
src_vocab: run/source.vocab
tgt_vocab: run/target.vocab

# Tamaño del vocabulario
#(debe concordar con el parametro usado en el algoritmo de subword tokenization)
src_vocab_size: 50000
tgt_vocab_size: 50000

# Filtrado sentencias de longitud mayor a n
# actuara si [filtertoolong] está presente
src_seq_length: 150
src_seq_length: 150

# Tokenizadores
src_subword_model: source.model
tgt_subword_model: target.model

# Archivos donde se guardaran los logs y los checkpoints de modelos
log_file: train.log
save_model: models/model.nahes

# Condición de paro si no se obtienen mejoras significativas
# despues de n validaciones
early_stopping: 4

# Guarda un checkpoint del modelo cada n steps
save_checkpoint_steps: 1000

# Mantiene los n ultimos checkpoints
keep_checkpoint: 3

# Reproductibilidad
seed: 3435

# Entrena el modelo maximo n steps
# Default: 100,000
train_steps: 3000

# Corre el set de validaciones (*.dev) despues de n steps
# Defatul: 10,000
valid_steps: 1000

warmup_steps: 1000
report_every: 100

# Numero de GPUs y sus ids
world_size: 1
gpu_ranks: [0]

# Batching
bucket_size: 262144
num_workers: 0
batch_type: "tokens"
batch_size: 4096
valid_batch_size: 2048
max_generator_batches: 2
accum_count: [4]
accum_steps: [0]

# Configuración del optimizador
model_dtype: "fp16"
optim: "adam"
learning_rate: 2
# warmup_steps: 8000
decay_method: "noam"
adam_beta2: 0.998
max_grad_norm: 0
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

# Configuración del Modelo
encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 6
dec_layers: 6
heads: 8
hidden_size: 512
word_vec_size: 512
transformer_ff: 2048
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]
'''

with open("/content/drive/MyDrive/neural_machine_translation/config.yaml", "w+") as config_yaml:
  config_yaml.write(config)

!cat /content/drive/MyDrive/neural_machine_translation/config.yaml

"""## Construyendo el vocabulario"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2

"""En esta parte, tuve que cambiar el entorno de ejecución manualmente, en la parte de arriba de colab, donde está lo de RAM y Disco. De CPU a T4 GPU"""

# Check if the GPU is active
!nvidia-smi -L

# Check if the GPU is visable to PyTorch

import torch

print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

gpu_memory = torch.cuda.mem_get_info(0)
print("Free GPU memory:", gpu_memory[0]/1024**2, "out of:", gpu_memory[1]/1024**2)

"""# Entrenamiento"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !onmt_train -config config.yaml

"""# Traducción"""

!ls models

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !onmt_translate -model models/model.nahes_step_3000.pt -src MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test -output MT-Preparation/americasnlp2021.es.practice.translated -gpu 0 -min_length 1

!tail MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test

!tail MT-Preparation/americasnlp2021.es.practice.translated

!python MT-Preparation/subwording/3-desubword.py source.model MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test

!python MT-Preparation/subwording/3-desubword.py target.model MT-Preparation/americasnlp2021.es.practice.translated

!tail MT-Preparation/americasnlp2021.es.practice.translated.desubword

"""#Evaluación del modelo
## BLEU
"""

!git clone https://github.com/ymoslem/MT-Evaluation.git

!pip install -r MT-Evaluation/requirements.txt

# Aplicando desubword para tener la traducción de referencia
!python MT-Preparation/subwording/3-desubword.py target.model MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test

!python MT-Evaluation/BLEU/compute-bleu.py MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test.desubword MT-Preparation/americasnlp2021.es.practice.translated.desubword

"""##ChrF"""

!python /content/drive/MyDrive/neural_machine_translation/americasnlp2021/evaluate.py -h

!python /content/drive/MyDrive/neural_machine_translation/americasnlp2021/evaluate.py --sys MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test --ref MT-Preparation/americasnlp2021.es.practice.translated

"""#Comparar resultados
Tal parece ser que los resultados del baseline para el náhuatl, que fue con el que decidí trabajar, con BLEU de 0.33	y ChrF de 0.182 mientras que mi modelo tuvo un ChrF de 42.71 y un BLEU de 18.18 lo cual probablemente es extremadamente malo pipi

# ¿Por qué usar ChrF?
chrF con las siglas para CHaRacter-level F-score, esto significa que calcula la similitud de las traducciones a nivel caracter en vez de a nivel palabra, esto debido a que usar el nivel palabra suele traer problemas en lenguas que tienen mucha morfología.

- ¿En qué se diferencia de BLEU?

Bleu usa un sistema de n-gramas o incluso bigramas, por lo que justo en lenguas que tengan mucha morfología en una sóla palabra va a estar ignorando mucha información. ChrF se va un nivel más abajo para evaluar por caracteres, pero esto ofrece un score más favorable para lenguas específicas.

- ¿Por qué es relevante usar otras medidas de evaluación?

Mi conjetura es que (al menos hasta donde conozco en el caso del nahuatl), en una lengua con palabras que están formadas por muchos _chunks_ de muchas otras palabras, el tomar la referencia por palabra completa no beneficia a la evaluación ya que las palabras en nahuatl tienen muchísima información contenida en una sóla palabra, por lo que nos conviene más usar esta separación por caracteres. En este caso, se necesita una evaluación previa de la lengua para poder tener más fiabilidad del sistema de evaluación que usemos.

#Referencias
- [BLEU for dummies](https://machinelearningmastery.com/calculate-bleu-score-for-text-python )
- [ChrF](https://machinetranslate.org/chrF)


"""