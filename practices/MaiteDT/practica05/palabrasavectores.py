# -*- coding: utf-8 -*-
"""PalabrasAVectores.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FbyjlGfUIMVR7U_7qSifxLwBlvJfIdQ4
"""

!pip install wikiextractor

import nltk

nltk.download("wordnet")
nltk.download('omw-1.4')
nltk.download('punkt')
nltk.download('stopwords')

from google.colab import drive
drive.mount('/content/drive')

import urllib.request
from tqdm import tqdm

CORPORA_DIR = "drive/MyDrive/"

url = "https://dumps.wikimedia.org/eswiki/latest/eswiki-latest-pages-articles3.xml-p693324p1897740.bz2"
filename = CORPORA_DIR + "eswiki-articles3.bz2"

with tqdm(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=filename) as t:
   urllib.request.urlretrieve(url, filename, reporthook=lambda block_num, block_size, total_size: t.update(block_size))

import multiprocessing
print("Procesadores: " , multiprocessing.cpu_count())
!cat /proc/cpuinfo

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !python -m wikiextractor.WikiExtractor "drive/MyDrive/eswiki-articles3.bz2" --processes 24 -o "drive/MyDrive/eswiki-dump-3"

import os
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


class WikiSentencesExtractor(object):

    def __init__(self, directory, max_lines):
        self.directory = directory
        self.max_lines = max_lines
        self.total_sentences = 0

    @staticmethod
    def preprocess(text: str) -> list:
        if len(text) <= 3 or text.startswith("<"):
            return []
        text = text.lower()
        text = re.sub(f'[^\w\s]', '', text)

        words = word_tokenize(text, language="spanish")

        stop_words = set(stopwords.words("spanish"))
        words = [token for token in words if token not in stop_words]
        words = [token for token in words if token.isalpha() and len(token) > 2]
        return words

    def get_sentences(self):
        for subdir_letter in os.listdir(self.directory):
            file_path = os.path.join(self.directory, subdir_letter)
            for file_name in os.listdir(file_path):
                with open(os.path.join(file_path, file_name)) as file:
                    for line in file:
                        if self.max_lines == self.total_sentences:
                            return
                        words = self.preprocess(line)
                        if not words:
                            continue
                        yield words
                        self.total_sentences += 1

    def __iter__(self):
        return self.get_sentences()

    def __len__(self):
        return self.total_sentences

directory = CORPORA_DIR + "eswiki-dump-3"
os.listdir(directory)

from gensim.models import word2vec, FastText

MODELS_DIR = "drive/MyDrive/"

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model_name = MODELS_DIR + "eswiki-test.model"

# Commented out IPython magic to ensure Python compatibility.
# %%time
# try:
#     print(f"Searching for model {model_name}")
#     model = word2vec.Word2Vec.load(model_name)
#     print("Model found!!!")
# except Exception as e:
#     print(f"Modelo {model_name} not found. Train a new one")
#     model = word2vec.Word2Vec(
#         list(WikiSentencesExtractor(directory, max_lines=100000)),
#         vector_size=100,
#         window=5,
#         workers=multiprocessing.cpu_count()
#         )
#     model.save(model_name)
#     print(f"Finish train for model {model_name}")

"""# Probar el modelo"""

from enum import Enum

class Algorithms(Enum):
    CBOW = "CBOW"
    SKIP_GRAM = "SKIP_GRAM"
    FAST_TEXT = "FAST_TEXT"

def load_model(model_path: str):
    try:
        return word2vec.Word2Vec.load(model_path)
    except:
        print(f"[WARN] Model not found in path {model_path}")
        return None

def train_model(sentences, model_name: str, vector_size: int, window=5, workers=2, algorithm = Algorithms.CBOW):
    model_name_params = f"{model_name}-vs{vector_size}-w{window}-{algorithm.value}.model"
    model_path = MODELS_DIR + model_name_params
    if load_model(model_path) is not None:
        print(f"Already exists the model {model_path}")
        return load_model(model_path)
    print(f"TRAINING: {model_path}")
    if algorithm in [Algorithms.CBOW, Algorithms.SKIP_GRAM]:
        algorithm_number = 1 if algorithm == Algorithms.SKIP_GRAM else 0
        model = word2vec.Word2Vec(
            sentences,
            vector_size=vector_size,
            window=window,
            workers=workers,
            sg = algorithm_number,
            seed=42,
            )
    elif algorithm == Algorithms.FAST_TEXT:
        model = FastText(sentences=sentences, vector_size=vector_size, window=window, workers=workers, seed=42, epochs=100)
    else:
        print("[ERROR] algorithm not implemented yet :p")
        return
    model.save(model_path)
    return model

def report_stats(model) -> None:
    """Print report of a model"""
    print("Number of words in the corpus used for training the model: ", model.corpus_count)
    print("Number of words in the model: ", len(model.wv.index_to_key))
    print("Time [s], required for training the model: ", model.total_train_time)
    print("Count of trainings performed to generate this model: ", model.train_count)
    print("Length of the word2vec vectors: ", model.vector_size)
    print("Applied context length for generating the model: ", model.window)

"""### Elegimos usar Skipgram"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# skip_gram_100 = train_model(WikiSentencesExtractor(directory, -1), "eswiki-medium", 100, 5, workers=24, algorithm=Algorithms.SKIP_GRAM)

import numpy as np
from gensim.models import Word2Vec
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import gensim.downloader as gensim_api

model = Word2Vec.load("drive/MyDrive/eswiki-medium-vs100-w5-SKIP_GRAM.model")


#words = list(model.wv.index_to_key)  # Obtener la lista de todas las palabras en el modelo
#word_vectors = Word2Vec.load("drive/MyDrive/eswiki-medium-vs100-w5-SKIP_GRAM.model.wv.vectors.npy")

word_vectors = model.wv.vectors_for_all(model.wv.key_to_index.keys())

word_vectors.similarity("cat", "dog")

# Aplicar t-SNE para reducir la dimensionalidad
tsne = TSNE(n_components=2, random_state=0)
np.set_printoptions(suppress=True)
word_vectors_reduced = tsne.fit_transform(word_vectors[word_vectors.index_to_key[300:600]])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.figure(figsize=(15,15))
idx = 0
for a in word_vectors_reduced[:300]:
    w = word_vectors.index_to_key[300+idx]
    plt.plot(a[0],a[1],'r.')
    plt.text(a[0],a[1],w)
    idx += 1
plt.show()

"""## PCA"""

from sklearn.decomposition import PCA

pca = PCA(n_components=2)  # Specify the number of components you want to reduce to
pca_reduced = pca.fit_transform(word_vectors[word_vectors.index_to_key[300:600]])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.figure(figsize=(15,15))
idx = 0
for a in pca_reduced[:300]:
    w = word_vectors.index_to_key[300+idx]
    plt.plot(a[0],a[1],'r.')
    plt.text(a[0],a[1],w)
    idx += 1
plt.show()

"""## SVD"""

U, S, Vt = np.linalg.svd(word_vectors[word_vectors.index_to_key[300:600]], full_matrices=False)

# Define the number of dimensions to retain (e.g., 2 for visualization)
n_components = 2

# Retain a subset of singular values and corresponding left singular vectors
U_reduced = U[:, :n_components]
S_reduced = np.diag(S[:n_components])

# Compute the reduced-dimensional representations of the word vectors
svd_reduced = np.dot(U_reduced, S_reduced)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.figure(figsize=(15,15))
idx = 0
for a in svd_reduced[:300]:
    w = word_vectors.index_to_key[300+idx]
    plt.plot(a[0],a[1],'r.')
    plt.text(a[0],a[1],w)
    idx += 1
plt.show()

"""# Analizar y comparar las topologías que se generan con cada algoritmo

- ¿Se guardan las relaciones semánticas? si o no y ¿porqué?

Parece ser que sí, aunque se usó el mismo rango de las palabras para todos los métodos de reducción de dimensionalidad, cada una tiene un acomodo diferente pero parece ser que por lo regular palabras relacionadas entre sí como "hija" y "esposa" que son terminos asociados a familia o mujer, aparecen juntas en las 3 representaciones. Sin embargo, a mi percepción svd está un poco más disperso y parece ser que pca o t-sne tiene una distribución un poco más uniforme de todas sus palabras.

- ¿Qué método de reducción de dimensionalidad consideras que es mejor?

Yo diría que ya sea PCA o T-SNE, pero puede que sea porque tienen metodos ya establecidos en sus bibliotecas y uno no ve la "magia" de cómo funcionan, mientras que SVD fue más casi casi a manita jaja
"""