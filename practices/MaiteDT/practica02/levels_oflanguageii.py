# -*- coding: utf-8 -*-
"""Levels OfLanguageII.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ww_96zD7JNIgQs7QUc56XPNG9KnrSFOM
"""

!pip install nltk
!pip install scikit-learn
!pip install unidecode

from google.colab import drive
drive.mount('/content/drive/')

def quitar_acentos(texto):
    # Para quitar los acentos del texto y que todo sea ascii
    mapeo_acentos = {
        'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',
        'ä':'a',
        'e̱':'e','i̱':'i','u̱':'u', 'u̱':'u',
        'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U',
        'ñ': 'n', 'Ñ': 'N'
    }

    texto_sin_acentos = ''.join(mapeo_acentos.get(caracter, caracter) for caracter in texto)
    return texto_sin_acentos

import nltk
#from unidecode
import unicodedata
corpus_otomi = {}
#with open('/content/drive/MyDrive/corpus_otomi','r', encoding='UTF-8') as f:
corpus_otomi = {}
with open('/content/drive/MyDrive/corpus_otomi_clean', 'r', encoding='UTF-8') as f:
    for i, line in enumerate(f):
        elements = [item.strip('"').split(',') for item in line.strip().split()]

        for j, item in enumerate(elements):
            elements[j] = [quitar_acentos(subitem) for subitem in item]

        corpus_otomi[i] = elements
        #corpus_otomi[i] = line.strip()
        #corpus_otomi[i] = corpus_otomi[i]

print(corpus_otomi[0][0])
print("Número de oraciones que tenemos en total para trabajar: ", len(corpus_otomi))

import requests

def get_tags_map():
    tags_raw = requests.get("https://gist.githubusercontent.com/vitojph/39c52c709a9aff2d1d24588aba7f8155/raw/af2d83bc4c2a7e2e6dbb01bd0a10a23a3a21a551/universal_tagset-ES.map").text.split("\n")
    tags_map = {line.split("\t")[0].lower(): line.split("\t")[1] for line in tags_raw}
    return tags_map

def map_tag(tag: str, tags_map=get_tags_map()) -> str:
    return tags_map.get(tag.lower(), "N/F")

def parse_tags(corpora: list[list[tuple]]) -> list[list[tuple]]:
    result = []
    for sentence in corpora:
        result.append([(word, map_tag(tag)) for word, tag in sentence])
    return result

from sklearn.model_selection import train_test_split
# Separando en dos conjuntos, uno para entrenamiento y otro para pruebas
train_data, test_data = train_test_split(corpus_otomi, test_size=0.3, random_state=42)

"""CRF's"""

!pip install -U sklearn-crfsuite
from sklearn_crfsuite import CRF
from sklearn.model_selection import train_test_split

def word_to_features(sent: list, i: int):
    word = sent[i][0]
    features = {
        'word.lower()': word.lower(),
        'word.isupper()': word.isupper(),
        #'word.istitle()': word.istitle(),
        'word.isdigit()': word.isdigit(),
        #'prefix_1': word[:1],
        #'suffix_1': word[-1:],
        'word_len': len(word)
    }
    if i > 0:
        prev_word = sent[i - 1][0]
        features.update({
            'prev_word.lower()': prev_word.lower(),
        })
    else:
        features['BOS'] = True  # Beginning of sentence

    return features

# Extract features and labels
def sent_to_features(sent):
    return [word_to_features(sent, i) for i in range(len(sent))]

def sent_to_labels(sent):
    return [label for token, label in sent]

"""Preparación"""

# Prepare data for CRF
print(corpus_otomi[0])
print(type(corpus_otomi[0]))
X = []
for sent in corpus_otomi:
  for i in range (len(corpus_otomi[sent])):
    X += [[word_to_features(corpus_otomi[sent], i)]]

Y = []
for entry_list in corpus_otomi.values():
  for entry in entry_list:
    element = []
    try:
      element.append(entry[1].strip("]"))
    except IndexError:
      element.append("N/A")
    Y.append(element)
#comprobamos
len(X[0]) == len(Y[0])

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

"""Entrenamiento"""

from inspect import Attribute
from sklearn_crfsuite import CRF
# Initialize and train the CRF tagger: https://sklearn-crfsuite.readthedocs.io/en/latest/api.html
crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100, all_possible_transitions=True, verbose=True)
try:
    crf.fit(X_train, Y_train)
except Exception as e:
    print("UnicodeEncodeError:", e)
    print("Problematic data:", X_train, Y_train)  # Print the input data causing the error

from sklearn.metrics import classification_report
y_pred = crf.predict(X_test)

# Flatten the true and predicted labels
y_test_flat = [label for sent_labels in Y_test for label in sent_labels]
y_pred_flat = [label for sent_labels in y_pred for label in sent_labels]

# Evaluate the model
report = classification_report(y_true=y_test_flat, y_pred=y_pred_flat)
print(report)

"""Métricas"""

from sklearn.metrics import accuracy_score
print("Accuracy: ", accuracy_score(y_pred_flat, y_test_flat))

from sklearn.metrics import precision_score
print("Precision: ", precision_score(y_pred_flat, y_test_flat, average="macro"))

from sklearn.metrics import recall_score
print("Recall Score: ", recall_score(y_pred_flat, y_test_flat, average="macro"))

from sklearn.metrics import f1_score
print("F1-Score: ", f1_score(y_pred_flat, y_test_flat, average="macro"))

"""Testear una frase del conjunto de prueba"""

# Sacamos una frase del conjunto de prueba
print(test_data[18])
print(crf.predict(test_data[16]))