{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUVf_6xlUYtv",
        "outputId": "6df00f5e-82c8-46b7-e623-7deb4cf06fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "OK8y_BjrV8tt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "56sQwrzJWU_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_corpus(corpus: list[str]) -> list[str]:\n",
        "    \"\"\"Función de preprocesamiento\n",
        "\n",
        "    Agrega tokens de inicio y fin, normaliza todo a minusculas\n",
        "    \"\"\"\n",
        "    preprocessed_corpus = []\n",
        "    for sent in corpus:\n",
        "        result = [word.lower() for word in sent]\n",
        "        # Al final de la oración\n",
        "        result.append(\"<EOS>\")\n",
        "        result.insert(0, \"<BOS>\")\n",
        "        preprocessed_corpus.append(result)\n",
        "    return preprocessed_corpus"
      ],
      "metadata": {
        "id": "YzXUac14WZsq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_freqs(corpus: list[list[str]]):\n",
        "    words_freqs = {}\n",
        "    for sentence in corpus:\n",
        "        for word in sentence:\n",
        "            words_freqs[word] = words_freqs.get(word, 0) + 1\n",
        "    return words_freqs"
      ],
      "metadata": {
        "id": "t4Qum1fbWihb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UNK_LABEL = \"<UNK>\"\n",
        "def get_words_indexes(words_freqs: dict) -> dict:\n",
        "    result = {}\n",
        "    for idx, word in enumerate(words_freqs.keys()):\n",
        "        # Happax legomena happends\n",
        "        if words_freqs[word] == 1:\n",
        "            # Temp index for unknowns\n",
        "            result[UNK_LABEL] = len(words_freqs)\n",
        "        else:\n",
        "            result[word] = idx\n",
        "\n",
        "    return {word: idx for idx, word in enumerate(result.keys())}, {idx: word for idx, word in enumerate(result.keys())}"
      ],
      "metadata": {
        "id": "Q9qyThoGWot7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAWhmmYoW_0a",
        "outputId": "be7d7c2e-964c-4001-86d6-9529cdd4047f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Practica8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z49CYiHMXHuZ",
        "outputId": "727aaba1-446c-4e3f-cfae-66889f727b0c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/Practica8'\n",
            "/content/drive/MyDrive/Practica8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lee_corpus(file_name: str) -> str:\n",
        "    with open(file_name, \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "quijote = lee_corpus(\"El_Quijote.txt\")\n",
        "sents = nltk.sent_tokenize(quijote)\n",
        "corpus = [nltk.word_tokenize(sent) for sent in sents]"
      ],
      "metadata": {
        "id": "KoOjLU03W5q3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = preprocess_corpus(corpus)"
      ],
      "metadata": {
        "id": "E7iqN7PkX0_B"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VE1B48D8Ffd",
        "outputId": "df97644a-9d7d-4699-b6d2-a2b0f7e89b35"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_freqs = get_words_freqs(corpus)"
      ],
      "metadata": {
        "id": "xrtZjm_YX6pK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_indexes, index_to_word = get_words_indexes(words_freqs)"
      ],
      "metadata": {
        "id": "3jf7XZFWB0Zt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_id(words_indexes: dict, word: str) -> int:\n",
        "    unk_word_id = words_indexes[UNK_LABEL]\n",
        "    return words_indexes.get(word, unk_word_id)"
      ],
      "metadata": {
        "id": "ZJuxfiUoCNq_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test_data(corpus: list[list[str]], words_indexes: dict, n: int) -> tuple[list, list]:\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for sent in corpus:\n",
        "        n_grams = ngrams(sent, n)\n",
        "        for w1, w2, w3 in n_grams:\n",
        "            x_train.append([get_word_id(words_indexes, w1), get_word_id(words_indexes, w2)])\n",
        "            y_train.append([get_word_id(words_indexes, w3)])\n",
        "    return x_train, y_train"
      ],
      "metadata": {
        "id": "8iOJeoERCaQN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cargamos bibliotecas\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time"
      ],
      "metadata": {
        "id": "HNpi5cA-CdQW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup de parametros\n",
        "EMBEDDING_DIM = 200\n",
        "CONTEXT_SIZE = 2\n",
        "BATCH_SIZE = 256\n",
        "H = 100\n",
        "torch.manual_seed(19)\n",
        "# Tamaño del Vocabulario\n",
        "V = len(words_indexes)"
      ],
      "metadata": {
        "id": "w8vo5T_bChBV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = get_train_test_data(corpus, words_indexes, n=3)"
      ],
      "metadata": {
        "id": "PW5_glDfCqvO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "# partimos los datos de entrada en batches\n",
        "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "hRpX7Bb5Ct6U"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigram Neural Network Model\n",
        "class TrigramModel(nn.Module):\n",
        "    \"\"\"Clase padre: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.linear2 = nn.Linear(h, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # x': concatenation of x1 and x2 embeddings   -->\n",
        "        #self.embeddings regresa un vector por cada uno de los índices que se les pase como entrada. view() les cambia el tamaño para concatenarlos\n",
        "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
        "        # h: tanh(W_1.x' + b)  -->\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        # W_2.h                 -->\n",
        "        out = self.linear2(out)\n",
        "        # log_softmax(W_2.h)      -->\n",
        "        # dim=1 para que opere sobre renglones, pues al usar batchs tenemos varios vectores de salida\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "wxSjthe6C0G1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "CGriNHmlC229"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Pérdida. Negative log-likelihood loss\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "#Otras opciones de función de pérdida (tendrían que usar softmax sin log):\n",
        "#nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# 2. Instanciar el modelo\n",
        "model = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
        "\n",
        "# 3. Optimización. ADAM optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
        "\n",
        "#Otras opciones de optimizador:\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
        "# En la práctica sólo correremos una epoch por restricciones de recursos\n",
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    st = time.time()\n",
        "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch))\n",
        "    for it, data_tensor in enumerate(train_loader):\n",
        "        context_tensor = data_tensor[:,0:2]\n",
        "        target_tensor = data_tensor[:,2]\n",
        "\n",
        "        model.zero_grad() #reinicializar los gradientes\n",
        "        #FORWARD:\n",
        "        # get log probabilities over next words\n",
        "        log_probs = model(context_tensor)\n",
        "\n",
        "\n",
        "        # compute loss function\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        #BACKWARD:\n",
        "        # backward pass and update gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if it % 500 == 0:\n",
        "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Time taken (s): {}\".format(it, epoch, loss.item(), (time.time()-st)))\n",
        "            st = time.time()\n",
        "            #barch_size x len(vocab)\n",
        "\n",
        "    # saving model\n",
        "    model_path = 'model_{}.dat'.format(epoch)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved for epoch={epoch} at {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5rnT7R6C4YT",
        "outputId": "e26f2827-83e8-4d64-a729-b579ae919a14"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training model Epoch: 0 ---\n",
            "Training Iteration 0 of epoch 0 complete. Loss: 9.429872512817383; Time taken (s): 0.2951540946960449\n",
            "Training Iteration 500 of epoch 0 complete. Loss: 5.698538780212402; Time taken (s): 44.24118232727051\n",
            "Training Iteration 1000 of epoch 0 complete. Loss: 5.12728214263916; Time taken (s): 42.260573625564575\n",
            "Training Iteration 1500 of epoch 0 complete. Loss: 4.884504318237305; Time taken (s): 41.96507930755615\n",
            "Model saved for epoch=0 at model_0.dat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(path: str) -> TrigramModel:\n",
        "    model_loaded = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
        "    model_loaded.load_state_dict(torch.load(path))\n",
        "    model_loaded.eval()\n",
        "    return model_loaded"
      ],
      "metadata": {
        "id": "9_fB6lE3HNc7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"model_0.dat\""
      ],
      "metadata": {
        "id": "NiNumLa_HT66"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(PATH)\n",
        "W1 = \"<BOS>\"\n",
        "W2 = \"mi\"\n",
        "\n",
        "IDX1 = get_word_id(words_indexes, W1)\n",
        "IDX2 = get_word_id(words_indexes, W2)\n",
        "\n",
        "#Obtenemos Log probabidades p(W3|W2,W1)\n",
        "probs = model(torch.tensor([[IDX1,  IDX2]])).detach().tolist()"
      ],
      "metadata": {
        "id": "ptI89vl9HcH8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos diccionario con {idx: logprob}\n",
        "model_probs = {}\n",
        "for idx, p in enumerate(probs[0]):\n",
        "  model_probs[idx] = p\n",
        "\n",
        "# Sort:\n",
        "model_probs_sorted = sorted(((prob, idx) for idx, prob in model_probs.items()), reverse=True)\n",
        "\n",
        "# Printing word  and prob (retrieving the idx):\n",
        "topcandidates = 0\n",
        "for prob, idx in model_probs_sorted:\n",
        "  #Retrieve the word associated with that idx\n",
        "  word = index_to_word[idx]\n",
        "  print(idx, word, prob)\n",
        "\n",
        "  topcandidates += 1\n",
        "\n",
        "  if topcandidates > 100:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vrJEf1e5Hl2Z",
        "outputId": "acfcbcc6-168a-41a4-c338-9a82b6b922fa"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "135 señor -2.291330337524414\n",
            "228 vida -2.7545151710510254\n",
            "300 señora -2.9274682998657227\n",
            "142 amo -3.1409263610839844\n",
            "857 padre -3.5086264610290527\n",
            "291 mujer -3.5320324897766113\n",
            "782 parecer -3.5429940223693848\n",
            "125 escudero -3.6744327545166016\n",
            "481 hija -4.088214874267578\n",
            "63 buen -4.133939743041992\n",
            "889 alma -4.155529499053955\n",
            "469 nombre -4.305261611938477\n",
            "1462 patria -4.5872039794921875\n",
            "336 casa -4.608495712280273\n",
            "188 intención -4.747317314147949\n",
            "548 consejo -4.759873390197754\n",
            "587 <UNK> -4.768007278442383\n",
            "2063 marido -4.772164344787598\n",
            "1354 ventura -4.835598945617676\n",
            "178 historia -5.045218467712402\n",
            "275 sobrina -5.1450018882751465\n",
            "2837 voluntad -5.203191757202148\n",
            "806 hijo -5.235255718231201\n",
            "143 cuerpo -5.320702075958252\n",
            "276 ama -5.438802719116211\n",
            "23 que -5.439906120300293\n",
            "315 muerte -5.449024200439453\n",
            "797 deseo -5.473878860473633\n",
            "1204 tal -5.487883567810059\n",
            "475 camino -5.503872871398926\n",
            "430 bien -5.539588928222656\n",
            "602 madre -5.557007789611816\n",
            "838 lugar -5.567862033843994\n",
            "886 amigo -5.6275739669799805\n",
            "1353 buena -5.696362495422363\n",
            "2886 hermosura -5.707764625549316\n",
            "3579 vencimiento -5.709711074829102\n",
            "1637 asno -5.714700698852539\n",
            "25 condición -5.7519989013671875\n",
            "266 parte -5.7846293449401855\n",
            "1250 cargo -5.803107738494873\n",
            "1069 hermano -5.80760383605957\n",
            "1403 fuerza -5.832108974456787\n",
            "6302 rucio -5.849595069885254\n",
            "197 gusto -5.879899501800537\n",
            "1376 salud -5.8901848793029785\n",
            "2733 ánima -5.891027927398682\n",
            "129 mal -5.894509315490723\n",
            "1828 caballo -5.928845405578613\n",
            "417 mala -5.93590784072876\n",
            "748 suerte -5.978943824768066\n",
            "2555 gente -5.981514930725098\n",
            "349 corazón -6.007129192352295\n",
            "1522 jumento -6.027872085571289\n",
            "2023 locura -6.029290199279785\n",
            "1669 hacienda -6.072624206542969\n",
            "1161 enemigo -6.079961776733398\n",
            "1574 espada -6.117434501647949\n",
            "2315 palabra -6.135819435119629\n",
            "2198 profesión -6.1641130447387695\n",
            "679 vez -6.166281700134277\n",
            "565 cual -6.189975738525391\n",
            "663 día -6.1957783699035645\n",
            "532 aldea -6.196022987365723\n",
            "33 tierra -6.2114763259887695\n",
            "2406 presencia -6.217434883117676\n",
            "935 mano -6.218969345092773\n",
            "54 desgracia -6.251810550689697\n",
            "645 razón -6.269858360290527\n",
            "6454 esposo -6.323232173919678\n",
            "1691 diferencia -6.346098899841309\n",
            "293 es -6.347113132476807\n",
            "1591 brazo -6.358863353729248\n",
            "716 autor -6.361103057861328\n",
            "322 noticia -6.372544765472412\n",
            "301 dulcinea -6.404982566833496\n",
            "73 , -6.4133076667785645\n",
            "243 verdad -6.432263374328613\n",
            "649 persona -6.433635234832764\n",
            "2247 huésped -6.4496893882751465\n",
            "4956 desdicha -6.476701736450195\n",
            "1650 lanza -6.496317386627197\n",
            "43 caballero -6.5052490234375\n",
            "745 honra -6.551352024078369\n",
            "1759 pueblo -6.562914848327637\n",
            "82 fin -6.569938659667969\n",
            "37 cuenta -6.577951431274414\n",
            "2475 virtud -6.581418514251709\n",
            "2350 boca -6.581733703613281\n",
            "1106 remedio -6.585326194763184\n",
            "1935 dama -6.603445053100586\n",
            "344 pobre -6.615699768066406\n",
            "90 más -6.647157669067383\n",
            "4 don -6.657078742980957\n",
            "2379 cortesía -6.6743059158325195\n",
            "713 primer -6.696756362915039\n",
            "422 venida -6.732017993927002\n",
            "193 discreción -6.7359724044799805\n",
            "551 fe -6.760353088378906\n",
            "2765 daño -6.768837928771973\n",
            "924 mayor -6.771142959594727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generación de lenguaje"
      ],
      "metadata": {
        "id": "rdDAooioHuW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_likely_words(model: TrigramModel, context: str, words_indexes: dict, index_to_word: dict, top_count: int=10) -> list[tuple]:\n",
        "    model_probs = {}\n",
        "    words = context.split()\n",
        "    idx_word_1 = get_word_id(words_indexes, words[0])\n",
        "    idx_word_2 = get_word_id(words_indexes, words[1])\n",
        "    probs = model(torch.tensor([[idx_word_1, idx_word_2]])).detach().tolist()\n",
        "\n",
        "    for idx, p in enumerate(probs[0]):\n",
        "        model_probs[idx] = p\n",
        "\n",
        "    # Strategy: Sort and get top-K words to generate text\n",
        "    return sorted(((prob, index_to_word[idx]) for idx, prob in model_probs.items()), reverse=True)[:top_count]"
      ],
      "metadata": {
        "id": "wIvcwPs6INFs"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_likely_words(model, \"el día\", words_indexes, index_to_word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQBtUdWwpPzs",
        "outputId": "f1a238f5-f7b4-4986-f009-0b4b20fd36eb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(-1.0571527481079102, ','), (-1.8467793464660645, 'de'), (-2.6491775512695312, 'que'), (-2.942416191101074, 'y'), (-3.1142191886901855, '.'), (-3.360020160675049, 'del'), (-3.906507968902588, ';'), (-3.9203553199768066, 'a'), (-4.243419170379639, 'en'), (-4.46452522277832, 'se')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "def get_next_word_random(words: list[tuple[float, str]]) -> str:\n",
        "    # From a top-K list of words get a random word\n",
        "    return words[randint(0, len(words)-1)][1]"
      ],
      "metadata": {
        "id": "8QqxmBBzIRRg"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "def get_next_word(words: list[tuple[float, str]]) -> str:\n",
        "    # Estrategia propuesta\n",
        "    if (randint(0,5) < 3):\n",
        "      return words[randint(0, 2)][1]\n",
        "    elif (randint(0,5) < 5):\n",
        "      return words[randint(3, 5)][1]\n",
        "    else:\n",
        "      return words[randint(6, 9)][1]\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "0OIFwVlktX1p"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 30\n",
        "TOP_COUNT = 10\n",
        "def generate_text_random(model: TrigramModel, history: str, words_indexes: dict, index_to_word: dict, tokens_count: int=0) -> None:\n",
        "    next_word = get_next_word_random(get_likely_words(model, history, words_indexes, index_to_word, top_count=TOP_COUNT))\n",
        "    print(next_word, end=\" \")\n",
        "    tokens_count += 1\n",
        "    if tokens_count == MAX_TOKENS or next_word == \"<EOS>\":\n",
        "        return\n",
        "    generate_text_random(model, history.split()[1]+ \" \" + next_word, words_indexes, index_to_word, tokens_count)"
      ],
      "metadata": {
        "id": "Uq7WucbltwoO"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 30\n",
        "TOP_COUNT = 10\n",
        "def generate_text(model: TrigramModel, history: str, words_indexes: dict, index_to_word: dict, tokens_count: int=0) -> None:\n",
        "    next_word = get_next_word(get_likely_words(model, history, words_indexes, index_to_word, top_count=TOP_COUNT))\n",
        "    print(next_word, end=\" \")\n",
        "    tokens_count += 1\n",
        "    if tokens_count == MAX_TOKENS or next_word == \"<EOS>\":\n",
        "        return\n",
        "    generate_text(model, history.split()[1]+ \" \" + next_word, words_indexes, index_to_word, tokens_count)"
      ],
      "metadata": {
        "id": "YY1HF3QuIvec"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplos"
      ],
      "metadata": {
        "id": "RkeoKzflmjJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la estrategia original de la práctica"
      ],
      "metadata": {
        "id": "bnkcUpR22Yhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo 1\n",
        "sentence = \"<BOS> Entonces\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text_random(model, sentence, words_indexes, index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3slIpQsaIyQK",
        "outputId": "d1588adb-ee8f-45b0-dc6a-26f2d6a2ac33"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BOS> Entonces , que me <UNK> , el pastor de ver si el señor bachiller . el señor : el uno <UNK> y a otros a las de los azotes en la "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo 2\n",
        "sentence = \"Había una\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text_random(model, sentence, words_indexes, index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6rLSvukml8T",
        "outputId": "ed4c4da8-2d97-4a2e-a30c-7c674cde542c"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Había una mosca de su aldea . de las armas ; porque se <UNK> el bachiller que los tres . , señor gentilhombre a un año ; porque se le ha sucedido "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo 3\n",
        "sentence = \"el señor\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text_random(model, sentence, words_indexes, index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EiN_JYImuGK",
        "outputId": "4c6a4211-099c-48b1-f1a5-c236d267d720"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el señor : el cura la vida : `` no es mi alma que , y , <UNK> de un caballero <UNK> ; y que , <UNK> el cielo don juan a "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la estrategia propuesta"
      ],
      "metadata": {
        "id": "IY79d9Ex2cF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo 1\n",
        "sentence = \"<BOS> Entonces\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text(model, sentence, words_indexes, index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1CrPfOsrn0H",
        "outputId": "c8f7250e-b198-4de5-e4c3-758f20c7fe99"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BOS> Entonces la mancha de don gregorio de su alma ; porque yo no lo consintió el mundo ; pero que no soy de la noche del mundo , porque se ha "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo 2\n",
        "sentence = \"Había una\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text(model, sentence, words_indexes, index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7flGyEedrrTf",
        "outputId": "d3b4bef7-65c0-4476-9843-b621de76bf8b"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Había una vez , como no soy la vida , que ya estaba el mundo ; porque en las cuales del mundo . de mi señora : y , por cierto , "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo 3\n",
        "sentence = \"el señor\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text(model, sentence, words_indexes, index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9Poy4Nyrxi2",
        "outputId": "0ef89603-863d-47ff-95b0-ffc907644524"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el señor , como lo es —respondió el mozo— ; mas a don gaiferos que no es bien el cura que el señor caballero del pueblo : — no me ha hecho "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Información de setup de entrenamiento\n"
      ],
      "metadata": {
        "id": "3jBqdgNK4YVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup de parametros\n",
        "Dimensión de embeddings: 200\n",
        "\n",
        "Tamaño del contexto: 2\n",
        "\n",
        "Tamaño de la capa oculta: 100\n",
        "\n",
        "Tamaño del Batch: 256\n",
        "\n",
        "Cantidad de oraciones para entrenamiento: 9513"
      ],
      "metadata": {
        "id": "o0EIYLUF52Tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estrategia propuesta\n",
        "En lugar de elegir completamente aleatorio entre las 10 opciones más probables, se elige un número aleatorio entre 0 y 5. Si es menor que 3, se elige una opción aleatoria entre las 3 más probables, si está entre 3 y 4, se elige entre las siguientes 3 más probables, y si es 5 se elegi entre las otras 4.\n",
        "\n",
        "Con esto espero que cualquiera de las opciones puedan salir, pero sea más probable que salgan las que tienen mayor probabilidad según el modelo.\n",
        "\n",
        "Al comparar los resultados de la estrategia anterior con los de esta estrategia, no parece haber una gran diferencia; los resultados me siguen pareciendo muy aleatorios, probablemente porque fueron muy pocos datos de entrenamiento, así que las probabilidades del modelo no son muy precisas."
      ],
      "metadata": {
        "id": "_FI_m1ti6Wym"
      }
    }
  ]
}