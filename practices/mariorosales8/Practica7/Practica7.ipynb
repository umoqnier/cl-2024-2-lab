{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Obtención de datos"
      ],
      "metadata": {
        "id": "VChc0aHsqsOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx34xTtcb4Zm",
        "outputId": "9cf8b927-83a1-4b87-8bf1-ae298247f26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/neural_machine_translation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEHm11asb_j6",
        "outputId": "6e0e1e27-e991-442a-8f23-1570f6eb8ce5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/neural_machine_translation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clonamos el repositorio para herramientas de preprocesamiento\n",
        "!git clone https://github.com/ymoslem/MT-Preparation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me_URgmqcEOX",
        "outputId": "788b82b3-5a88-4345-cecb-ab4117207473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MT-Preparation'...\n",
            "remote: Enumerating objects: 268, done.\u001b[K\n",
            "remote: Counting objects: 100% (268/268), done.\u001b[K\n",
            "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
            "remote: Total 268 (delta 133), reused 189 (delta 97), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (268/268), 69.06 KiB | 1.10 MiB/s, done.\n",
            "Resolving deltas: 100% (133/133), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the requirements\n",
        "!pip3 install -r MT-Preparation/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCJkwCN_cMTf",
        "outputId": "15067d92-5014-44e8-c8d0-2b49ca723498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 3)) (0.1.99)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Descargamos el dataset\n",
        "!git clone https://github.com/AmericasNLP/americasnlp2021.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXR8LGm8cSz_",
        "outputId": "76499e10-33dd-4e8e-f4ba-8cde611e8ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'americasnlp2021'...\n",
            "remote: Enumerating objects: 469, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 469 (delta 89), reused 99 (delta 87), pack-reused 333\u001b[K\n",
            "Receiving objects: 100% (469/469), 37.37 MiB | 9.79 MiB/s, done.\n",
            "Resolving deltas: 100% (218/218), done.\n",
            "Updating files: 100% (146/146), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtrado de datos"
      ],
      "metadata": {
        "id": "moQAKgaUqcPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtramos el dataset\n",
        "# Arguments: source file, target file, source language, target language\n",
        "!python3 MT-Preparation/filtering/filter.py americasnlp2021/data/nahuatl-spanish/dev.nah americasnlp2021/data/nahuatl-spanish/dev.es nah es\n",
        "!python3 MT-Preparation/filtering/filter.py americasnlp2021/data/nahuatl-spanish/train.nah americasnlp2021/data/nahuatl-spanish/train.es nah es"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfaeU64jckzf",
        "outputId": "710615fb-c0ee-4f55-87b4-bf357cbef841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape (rows, columns): (672, 2)\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 671\n",
            "--- Duplicates Deleted\t\t\t--> Rows: 671\n",
            "--- Source-Copied Rows Deleted\t\t--> Rows: 671\n",
            "--- Too Long Source/Target Deleted\t--> Rows: 613\n",
            "--- HTML Removed\t\t\t--> Rows: 613\n",
            "--- Rows will remain in true-cased\t--> Rows: 613\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 613\n",
            "--- Rows Shuffled\t\t\t--> Rows: 613\n",
            "--- Source Saved: americasnlp2021/data/nahuatl-spanish/dev.nah-filtered.nah\n",
            "--- Target Saved: americasnlp2021/data/nahuatl-spanish/dev.es-filtered.es\n",
            "Dataframe shape (rows, columns): (16145, 2)\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 16062\n",
            "--- Duplicates Deleted\t\t\t--> Rows: 15448\n",
            "--- Source-Copied Rows Deleted\t\t--> Rows: 15373\n",
            "--- Too Long Source/Target Deleted\t--> Rows: 14170\n",
            "--- HTML Removed\t\t\t--> Rows: 14170\n",
            "--- Rows will remain in true-cased\t--> Rows: 14170\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 14170\n",
            "--- Rows Shuffled\t\t\t--> Rows: 14170\n",
            "--- Source Saved: americasnlp2021/data/nahuatl-spanish/train.nah-filtered.nah\n",
            "--- Target Saved: americasnlp2021/data/nahuatl-spanish/train.es-filtered.es\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l americasnlp2021/data/nahuatl-spanish/train.es-filtered.es"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR-f3tv_qKPR",
        "outputId": "b59e8e50-5391-4170-f9da-6c80ef5888cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14170 americasnlp2021/data/nahuatl-spanish/train.es-filtered.es\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenización"
      ],
      "metadata": {
        "id": "Y2Sx-rDaqxse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Juntamos los datasets de dev y train\n",
        "!cat americasnlp2021/data/nahuatl-spanish/dev.nah-filtered.nah americasnlp2021/data/nahuatl-spanish/train.nah-filtered.nah > americasnlp2021/data/nahuatl-spanish/nah-es.nah\n",
        "!cat americasnlp2021/data/nahuatl-spanish/dev.es-filtered.es americasnlp2021/data/nahuatl-spanish/train.es-filtered.es > americasnlp2021/data/nahuatl-spanish/nah-es.es"
      ],
      "metadata": {
        "id": "d9jtITqmq1qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el modelo de subword tokenization\n",
        "!python3 MT-Preparation/subwording/1-train_unigram.py americasnlp2021/data/nahuatl-spanish/nah-es.nah americasnlp2021/data/nahuatl-spanish/nah-es.es"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP9E-RE0sIbj",
        "outputId": "6af0f423-7def-4e91-aa4f-cea17fbd7cbb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=americasnlp2021/data/nahuatl-spanish/nah-es.nah --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: americasnlp2021/data/nahuatl-spanish/nah-es.nah\n",
            "  input_format: \n",
            "  model_prefix: source\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: americasnlp2021/data/nahuatl-spanish/nah-es.nah\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 14783 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=2053564\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9528% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=87\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999528\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 14783 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=1131986\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 131964 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 14783\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 51930\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 51930 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=39106 obj=12.5167 num_tokens=110092 num_tokens/piece=2.81522\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=32625 obj=10.3851 num_tokens=111630 num_tokens/piece=3.42161\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: source.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: source.vocab\n",
            "Done, training a SentencepPiece model for the Source finished successfully!\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=americasnlp2021/data/nahuatl-spanish/nah-es.es --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: americasnlp2021/data/nahuatl-spanish/nah-es.es\n",
            "  input_format: \n",
            "  model_prefix: target\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: americasnlp2021/data/nahuatl-spanish/nah-es.es\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 14783 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=1982306\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9582% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=81\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999582\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 14783 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=1044713\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 55218 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 14783\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 26627\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 26627 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20221 obj=10.2914 num_tokens=53863 num_tokens/piece=2.66372\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16504 obj=8.50295 num_tokens=54464 num_tokens/piece=3.30005\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: target.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: target.vocab\n",
            "Done, training a SentencepPiece model for the Target finished successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subword the dataset\n",
        "!python3 MT-Preparation/subwording/2-subword.py source.model target.model americasnlp2021/data/nahuatl-spanish/nah-es.es americasnlp2021/data/nahuatl-spanish/nah-es.nah"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZVeBrRUswDB",
        "outputId": "550b6cbd-7018-4e20-a87d-95a3bde1ee25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Model: source.model\n",
            "Target Model: target.model\n",
            "Source Dataset: americasnlp2021/data/nahuatl-spanish/nah-es.es\n",
            "Target Dataset: americasnlp2021/data/nahuatl-spanish/nah-es.nah\n",
            "Done subwording the source file! Output: americasnlp2021/data/nahuatl-spanish/nah-es.es.subword\n",
            "Done subwording the target file! Output: americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training set, development set, and test set\n",
        "# Development and test sets should be between 1000 and 5000 segments (here we chose 2000)\n",
        "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 2000 2000 americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword americasnlp2021/data/nahuatl-spanish/nah-es.es.subword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "260tY15YwzFM",
        "outputId": "9a34d200-8aff-4e04-a7e5-b244721f93d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape: (14783, 2)\n",
            "--- Empty Cells Deleted --> Rows: 14783\n",
            "--- Wrote Files\n",
            "Done!\n",
            "Output files\n",
            "americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.train\n",
            "americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.train\n",
            "americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.dev\n",
            "americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.dev\n",
            "americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.test\n",
            "americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuración del modelo"
      ],
      "metadata": {
        "id": "4RMhLtYpRjxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gnbvM96kihB",
        "outputId": "147e78b1-01d1-454f-d2c0-8122c9846d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install OpenNMT-py -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63SlXFnnx7Sd",
        "outputId": "37575bb7-f89c-425c-c9fd-5d10bba2e7ac",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.5.1-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.8/262.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<2.3,>=2.1 (from OpenNMT-py)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<5,>=4 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-4.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (179.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.15.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-3.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.37 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.7.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (1.25.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.64.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3,>=2.1->OpenNMT-py) (2.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->OpenNMT-py) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3,>=2.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.1.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, triton, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ctranslate2, configargparse, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fasttext-wheel, nvidia-cusolver-cu12, torch, OpenNMT-py\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OpenNMT-py-3.5.1 colorama-0.4.6 configargparse-1.7 ctranslate2-4.2.1 fasttext-wheel-0.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 portalocker-2.8.2 pyahocorasick-2.1.0 pybind11-2.12.0 pyonmttok-1.37.1 rapidfuzz-3.9.3 sacrebleu-2.4.2 torch-2.2.2 triton-2.2.0 waitress-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación del archivo de configuración\n",
        "# train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint\n",
        "SRC_DATA_NAME = \"americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword\"\n",
        "TARGET_DATA_NAME = \"americasnlp2021/data/nahuatl-spanish/nah-es.es.subword\"\n",
        "\n",
        "config = f'''# config.yaml\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Rutas de archivos de entrenamiento\n",
        "#(previamente aplicado subword tokenization)\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: {SRC_DATA_NAME}.train\n",
        "        path_tgt: {TARGET_DATA_NAME}.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: {SRC_DATA_NAME}.dev\n",
        "        path_tgt: {TARGET_DATA_NAME}.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabularios (serán generados por `onmt_build_vocab`)\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Tamaño del vocabulario\n",
        "#(debe concordar con el parametro usado en el algoritmo de subword tokenization)\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filtrado sentencias de longitud mayor a n\n",
        "# actuara si [filtertoolong] está presente\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenizadores\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Archivos donde se guardaran los logs y los checkpoints de modelos\n",
        "log_file: train.log\n",
        "save_model: models/model.enfr\n",
        "\n",
        "# Condición de paro si no se obtienen mejoras significativas\n",
        "# despues de n validaciones\n",
        "early_stopping: 4\n",
        "\n",
        "# Guarda un checkpoint del modelo cada n steps\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# Mantiene los n ultimos checkpoints\n",
        "keep_checkpoint: 3\n",
        "\n",
        "# Reproductibilidad\n",
        "seed: 3435\n",
        "\n",
        "# Entrena el modelo maximo n steps\n",
        "# Default: 100,000\n",
        "train_steps: 3000\n",
        "\n",
        "# Corre el set de validaciones (*.dev) despues de n steps\n",
        "# Defatul: 10,000\n",
        "valid_steps: 1000\n",
        "\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Numero de GPUs y sus ids\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Configuración del optimizador\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Configuración del Modelo\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"/content/drive/MyDrive/neural_machine_translation/config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ],
      "metadata": {
        "id": "z5cUPgc0yini"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/drive/MyDrive/neural_machine_translation/config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYG1jUCumB7Q",
        "outputId": "ce9b62cd-a11e-4b74-ea8b-88b870e23646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# config.yaml\n",
            "\n",
            "## Where the samples will be written\n",
            "save_data: run\n",
            "\n",
            "# Rutas de archivos de entrenamiento\n",
            "#(previamente aplicado subword tokenization)\n",
            "data:\n",
            "    corpus_1:\n",
            "        path_src: americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.train\n",
            "        path_tgt: americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.train\n",
            "        transforms: [filtertoolong]\n",
            "    valid:\n",
            "        path_src: americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.dev\n",
            "        path_tgt: americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.dev\n",
            "        transforms: [filtertoolong]\n",
            "\n",
            "# Vocabularios (serán generados por `onmt_build_vocab`)\n",
            "src_vocab: run/source.vocab\n",
            "tgt_vocab: run/target.vocab\n",
            "\n",
            "# Tamaño del vocabulario\n",
            "#(debe concordar con el parametro usado en el algoritmo de subword tokenization)\n",
            "src_vocab_size: 50000\n",
            "tgt_vocab_size: 50000\n",
            "\n",
            "# Filtrado sentencias de longitud mayor a n\n",
            "# actuara si [filtertoolong] está presente\n",
            "src_seq_length: 150\n",
            "src_seq_length: 150\n",
            "\n",
            "# Tokenizadores\n",
            "src_subword_model: source.model\n",
            "tgt_subword_model: target.model\n",
            "\n",
            "# Archivos donde se guardaran los logs y los checkpoints de modelos\n",
            "log_file: train.log\n",
            "save_model: models/model.enfr\n",
            "\n",
            "# Condición de paro si no se obtienen mejoras significativas\n",
            "# despues de n validaciones\n",
            "early_stopping: 4\n",
            "\n",
            "# Guarda un checkpoint del modelo cada n steps\n",
            "save_checkpoint_steps: 1000\n",
            "\n",
            "# Mantiene los n ultimos checkpoints\n",
            "keep_checkpoint: 3\n",
            "\n",
            "# Reproductibilidad\n",
            "seed: 3435\n",
            "\n",
            "# Entrena el modelo maximo n steps\n",
            "# Default: 100,000\n",
            "train_steps: 3000\n",
            "\n",
            "# Corre el set de validaciones (*.dev) despues de n steps\n",
            "# Defatul: 10,000\n",
            "valid_steps: 1000\n",
            "\n",
            "warmup_steps: 1000\n",
            "report_every: 100\n",
            "\n",
            "# Numero de GPUs y sus ids\n",
            "world_size: 1\n",
            "gpu_ranks: [0]\n",
            "\n",
            "# Batching\n",
            "bucket_size: 262144\n",
            "num_workers: 0\n",
            "batch_type: \"tokens\"\n",
            "batch_size: 4096\n",
            "valid_batch_size: 2048\n",
            "max_generator_batches: 2\n",
            "accum_count: [4]\n",
            "accum_steps: [0]\n",
            "\n",
            "# Configuración del optimizador\n",
            "model_dtype: \"fp16\"\n",
            "optim: \"adam\"\n",
            "learning_rate: 2\n",
            "# warmup_steps: 8000\n",
            "decay_method: \"noam\"\n",
            "adam_beta2: 0.998\n",
            "max_grad_norm: 0\n",
            "label_smoothing: 0.1\n",
            "param_init: 0\n",
            "param_init_glorot: true\n",
            "normalization: \"tokens\"\n",
            "\n",
            "# Configuración del Modelo\n",
            "encoder_type: transformer\n",
            "decoder_type: transformer\n",
            "position_encoding: true\n",
            "enc_layers: 6\n",
            "dec_layers: 6\n",
            "heads: 8\n",
            "hidden_size: 512\n",
            "word_vec_size: 512\n",
            "transformer_ff: 2048\n",
            "dropout_steps: [0]\n",
            "dropout: [0.1]\n",
            "attention_dropout: [0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAgxZu77h3vK",
        "outputId": "cd3b5d25-690c-4ba6-a258-f4843e0e3ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-06-02 10:56:06,051 INFO] Counter vocab from -1 samples.\n",
            "[2024-06-02 10:56:06,051 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-06-02 10:56:07,615 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=305)\n",
            "\n",
            "[2024-06-02 10:56:07,618 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=328)\n",
            "\n",
            "[2024-06-02 10:56:07,642 INFO] Counters src: 5221\n",
            "[2024-06-02 10:56:07,642 INFO] Counters tgt: 5418\n",
            "CPU times: user 59 ms, sys: 16.1 ms, total: 75.1 ms\n",
            "Wall time: 9.04 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the GPU is visable to PyTorch\n",
        "\n",
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "gpu_memory = torch.cuda.mem_get_info(0)\n",
        "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ydbvSdymJ5H",
        "outputId": "29a56545-07e8-42f2-e331-e2c9fb353e5b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n",
            "Free GPU memory: 14999.0625 out of: 15102.0625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "Vvquhysli5bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyeBl4dTjI3_",
        "outputId": "3485e20a-a9f8-46f9-d2b2-4d7a3180180d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "americasnlp2021  MT-Preparation  source.model  target.model\n",
            "config.yaml\t run\t\t source.vocab  target.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!onmt_train -config config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgA8t_Y1ivkn",
        "outputId": "578deb86-ae41-42d9-bbb6-4cc183c4d29c",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-06-03 15:50:46,062 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-06-03 15:50:46,471 INFO] Parsed 2 corpora from -data.\n",
            "[2024-06-03 15:50:46,472 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-06-03 15:50:47,306 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁y', '▁', 'n', '▁.', 'i', '▁,']\n",
            "[2024-06-03 15:50:47,307 INFO] The decoder start token is: <s>\n",
            "[2024-06-03 15:50:47,307 INFO] Building model...\n",
            "[2024-06-03 15:50:48,895 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-06-03 15:50:48,895 INFO] Non quantized layer compute is fp16\n",
            "[2024-06-03 15:50:49,055 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(5232, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(5424, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=5424, bias=True)\n",
            ")\n",
            "[2024-06-03 15:50:49,060 INFO] encoder: 21566464\n",
            "[2024-06-03 15:50:49,060 INFO] decoder: 30744880\n",
            "[2024-06-03 15:50:49,060 INFO] * number of parameters: 52311344\n",
            "[2024-06-03 15:50:49,061 INFO] Trainable parameters = {'torch.float32': 52311344, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-06-03 15:50:49,062 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-06-03 15:50:49,062 INFO]  * src vocab size = 5232\n",
            "[2024-06-03 15:50:49,062 INFO]  * tgt vocab size = 5424\n",
            "[2024-06-03 15:50:49,368 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-06-03 15:50:49,368 INFO] Starting training on GPU: [0]\n",
            "[2024-06-03 15:50:49,368 INFO] Start training loop and validate every 1000 steps...\n",
            "[2024-06-03 15:50:49,369 INFO] Scoring with: ['filtertoolong']\n",
            "[2024-06-03 15:50:51,775 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2024-06-03 15:50:52,686 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2024-06-03 15:50:53,370 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2024-06-03 15:50:54,386 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2024-06-03 15:50:55,091 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2024-06-03 15:50:56,265 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2024-06-03 15:50:56,934 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2024-06-03 15:50:57,588 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2024-06-03 15:50:58,929 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2024-06-03 15:51:00,056 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2024-06-03 15:51:01,211 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2024-06-03 15:51:03,207 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2024-06-03 15:51:03,896 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "Error in sys.excepthook:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/exceptiongroup/_formatting.py\", line 68, in exceptiongroup_excepthook\n",
            "    def exceptiongroup_excepthook(\n",
            "KeyboardInterrupt\n",
            "\n",
            "Original exception was:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/train.py\", line 67, in main\n",
            "    train(opt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/train.py\", line 52, in train\n",
            "    train_process(opt, device_id=0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/train_single.py\", line 238, in main\n",
            "    trainer.train(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/trainer.py\", line 308, in train\n",
            "    for i, (batches, normalization) in enumerate(self._accum_batches(train_iter)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/trainer.py\", line 238, in _accum_batches\n",
            "    for batch, bucket_idx in iterator:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/dynamic_iterator.py\", line 341, in __iter__\n",
            "    for bucket, bucket_idx in self._bucketing():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/dynamic_iterator.py\", line 275, in _bucketing\n",
            "    for ex in self.mixer:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/dynamic_iterator.py\", line 85, in __iter__\n",
            "    item = next(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/text_corpus.py\", line 281, in __iter__\n",
            "    yield from corpus\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/text_corpus.py\", line 240, in _process\n",
            "    for i, example in enumerate(stream):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/text_corpus.py\", line 160, in load\n",
            "    yield make_ex(sline.decode(\"utf-8\"), tline, align)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/text_corpus.py\", line 120, in make_ex\n",
            "    sline, sfeats = parse_features(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/text_utils.py\", line 17, in parse_features\n",
            "    tok, *fts = token.strip(\"\\n\").split(\"￨\")\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "CPU times: user 204 ms, sys: 26 ms, total: 230 ms\n",
            "Wall time: 26 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traducción"
      ],
      "metadata": {
        "id": "zvZE-mE-j33J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WPucwH4OE-y",
        "outputId": "48f02fa7-decb-432f-f184-93ab52d0aa3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.enfr_step_1000.pt  model.enfr_step_2000.pt  model.enfr_step_3000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!onmt_translate -model models/model.enfr_step_3000.pt -src americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.test -output es.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s8LMnzdj6xv",
        "outputId": "396614bb-8f72-4c6a-cd06-43457b484def"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-06-03 15:51:51,390 INFO] Loading checkpoint from models/model.enfr_step_3000.pt\n",
            "[2024-06-03 15:52:02,409 INFO] Loading data into the model\n",
            "[2024-06-03 15:53:00,633 INFO] PRED SCORE: -0.3759, PRED PPL: 1.46 NB SENTENCES: 2000\n",
            "Time w/o python interpreter load/terminate:  69.27337193489075\n",
            "CPU times: user 452 ms, sys: 83.7 ms, total: 535 ms\n",
            "Wall time: 1min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python MT-Preparation/subwording/3-desubword.py source.model americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufMD27zwyn6R",
        "outputId": "d7cae38a-5526-4e71-b1cf-31aa3bdf57f3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python MT-Preparation/subwording/3-desubword.py source.model es.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlw2AnizzfZe",
        "outputId": "8c0f6bf9-7dab-4826-f47d-02da13d0443a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: es.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.test.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaCuOHdB0IqU",
        "outputId": "56e2a7db-a7fc-4eff-a0b1-0cfa840914e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nehhua cualli niyetoya. ¡huan yehhua yahqui nochi!\n",
            "Moztla occepa niquitaz .\n",
            "Isato nopa ueuentsi :\n",
            "Quenotimoyohuilti , nonantzin . ¿ Quentimohuica ?\n",
            "Se kinamaka tamanal ya ne Kuesalan .\n",
            "yokiyakoke in lečonti .\n",
            "Auh çan chiquacentetl metztli yn onmonec acalli yn nabío ynic achto nezca , necoccampa yc tepanahuilloya ; yn nican onehuaya Mexico tlaca yc onpanahuilloya ynic ompa huia in Tetzcoco , auh yn ompa Tetzcoco ompa | | 53 yc hualtepanahuiloya yn ompa huallehuaya Tetzcoco tlaca yc huel hualpanahuiloya ynic nican hualhuia Mexico , ynic yehuatl yca amo huecauh panolloya yn acalli .\n",
            "Ocachi tomin quipia ca tlen itechmonequi .\n",
            "Ompa▁Momochco yexpa xihuitl ohuilaloya nehuentiloz itlauhtzinco in Toteotatzin itocatzin▁Chalma . Inintzin mahuiztic teotl oconmotlazocamachililiaya ipampa tlaca mococoa , cihuatl nozo pipiltzitzintin . Zan ica oquimotenehuiliaya ocehuihualoya . Ipampa on huel miac tlacatl oquimaxilique ica▁devoción . Ihuan ica on yexpa tlaca xihuitl oanaloya . Cequi tlacatl omohuentiaya▁candelerotin ; cequi tlacatl oquixihquimiloaya achi miac xochitl . Oncuan on acizque ompa▁Chalma quimotlapalhuizque teotl quimotlalilizque , quimopatililizque xochitl inon yehuahque ihuan quitlalizque inon celic ixitlantzinco in Toteotatzin . Nochtlacatl ocuicaya zazo tlein ompa ixitlantzinco , noihqui▁cirios . Ye on oyaya mohuentizque .\n",
            "Para se dieta para se mopajtijtok , bueno se kikua , amo techiuilij mal .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail es.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTrDE8sS0Z0L",
        "outputId": "6b480a0e-9ead-4107-a1f9-9314ed71f0fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Espero que tengas una mañana maravillosa! Fue lindo chatear contigo en línea\n",
            "Llega a la tarde .\n",
            "El viejito dijo :\n",
            "¿ Cómo está su madre , ?\n",
            "Se vende en Cuetzalan .\n",
            "Ya no los mata a sus marranos .\n",
            "A éste lo sorprendió una tempestad muy fuerte que soplaba del oriente y duró mucho tiempo , de modo que no pudieron aportar en ningún sitio ; pero con su habilidades .\n",
            "Tendrás dinero .\n",
            "Y el señor de Chalma nos devolvió a nuestra mamá . Así es como nos devolvió a nuestra madre . Todavía vivió mucho . Y todos los años íbamos en peregrinación hasta los pies del Dios . Primero fuimos así : no llevamos nada . Y al siguiente año llevamos dos candeleras y ceras y flores como ofrenda por la salud de nuestra madre .\n",
            "Si no hay tomate se puede comer , se muele .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación"
      ],
      "metadata": {
        "id": "AwjePklU3ypd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ymoslem/MT-Evaluation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nODoyVx331o9",
        "outputId": "eb1fb446-132b-4db7-ae35-1bd137435785"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MT-Evaluation'...\n",
            "remote: Enumerating objects: 101, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 101 (delta 46), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (101/101), 20.17 KiB | 1.44 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r MT-Evaluation/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaivnbJ_65-9",
        "outputId": "a9d0d761-696b-45ca-a589-d556a30950ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer (from -r MT-Evaluation/requirements.txt (line 1))\n",
            "  Downloading jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r MT-Evaluation/requirements.txt (line 2)) (3.8.1)\n",
            "Collecting sacrebleu (from -r MT-Evaluation/requirements.txt (line 3))\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses (from -r MT-Evaluation/requirements.txt (line 4))\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer->-r MT-Evaluation/requirements.txt (line 1)) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer->-r MT-Evaluation/requirements.txt (line 1))\n",
            "  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r MT-Evaluation/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r MT-Evaluation/requirements.txt (line 2)) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r MT-Evaluation/requirements.txt (line 2)) (4.66.4)\n",
            "Collecting portalocker (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3))\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (1.25.2)\n",
            "Collecting colorama (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (4.9.4)\n",
            "Installing collected packages: sacremoses, rapidfuzz, portalocker, colorama, sacrebleu, jiwer\n",
            "Successfully installed colorama-0.4.6 jiwer-3.0.4 portalocker-2.8.2 rapidfuzz-3.9.3 sacrebleu-2.4.2 sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando desubword para tener la traducción de referencia\n",
        "!python MT-Preparation/subwording/3-desubword.py target.model americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-tWhdJs7FVD",
        "outputId": "94f09447-ac8e-4231-a855-1f0de9c67e38"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU"
      ],
      "metadata": {
        "id": "ln6aRARL8ucL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python MT-Evaluation/BLEU/compute-bleu.py americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.test.desubword es.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XifMSsUa8xn6",
        "outputId": "42720373-5a48-4f5a-bf28-d8a0ae5d80fa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: Donde hay árboles y ríos , durante todo el verano la juventud se divierte bañándose todos los días .\n",
            "MTed 1st sentence: Aquí están los niños , los que van en los caballos podridos .\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "BLEU:  6.926355150258824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU: 6.926355150258824"
      ],
      "metadata": {
        "id": "q4K2AuXpSEmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChrF"
      ],
      "metadata": {
        "id": "TNMEXXQZ-gYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python americasnlp2021/evaluate.py --sys americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.test.desubword --ref es.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btgvHFsD-dUC",
        "outputId": "ceed23c6-30fc-476c-ee2c-bf4e0a26bf72"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "2000\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "#### Score Report ####\n",
            "chrF2 = 32.06\n",
            "BLEU = 6.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chrF2 = 32.06"
      ],
      "metadata": {
        "id": "T2JuegtYSLSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparación con baseline"
      ],
      "metadata": {
        "id": "P134Ov3pBal-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mis resultados en BLEU fueron entre 6.92, en ChrF fue de 32.06.\n",
        "\n",
        "Los resultados de baseline en BLUE fueron 0.33 y en ChrF de 0.182.\n",
        "\n",
        "Por lo tanto el modelo que hice tuvo mejores resultados que el baseline en ambas métricas, pero más en ChrF."
      ],
      "metadata": {
        "id": "6Hp_8KlnBtgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra"
      ],
      "metadata": {
        "id": "IrFSR8XwGGcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Investigar porque se propuso la medida ChrF en el Shared Task.\n",
        "\n",
        "ChrF es una métrica a nivel de caracteres. Se propuso porque para idiomas con mucha morfología, las métricas a nivel palabras son muy problemáticas."
      ],
      "metadata": {
        "id": "foZhl6g4GJZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Como se diferencia de BLEU?\n",
        "\n",
        "ChrF se basa en comparar similitud entre n-gramas de caracteres, mientras que BLEU lo compara entre n-gramas de palabras."
      ],
      "metadata": {
        "id": "ZNnuQWagG-5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Por qué es relevante utilizar otras medidas de evaluación además de BLEU?\n",
        "\n",
        "Porque distintos idiomas tienen propiedades distintas. Para idiomas con mucha morfología, dos palabras pueden tener casi los mismos morfemas aunque sean palabras diferentes; eso no se capturaría en una métrica a nivel palabra como BLEU, pero sí en una a nivel caracter."
      ],
      "metadata": {
        "id": "3Hthg7ZeKOHH"
      }
    }
  ]
}