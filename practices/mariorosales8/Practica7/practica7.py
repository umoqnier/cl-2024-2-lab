# -*- coding: utf-8 -*-
"""Práctica7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QwjD89JdNHXLV0myF0BI3_w20dYUHlL-

# Obtención de datos
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/neural_machine_translation

# Clonamos el repositorio para herramientas de preprocesamiento
!git clone https://github.com/ymoslem/MT-Preparation.git

# Install the requirements
!pip3 install -r MT-Preparation/requirements.txt

#Descargamos el dataset
!git clone https://github.com/AmericasNLP/americasnlp2021.git

"""# Filtrado de datos"""

# Filtramos el dataset
# Arguments: source file, target file, source language, target language
!python3 MT-Preparation/filtering/filter.py americasnlp2021/data/nahuatl-spanish/dev.nah americasnlp2021/data/nahuatl-spanish/dev.es nah es
!python3 MT-Preparation/filtering/filter.py americasnlp2021/data/nahuatl-spanish/train.nah americasnlp2021/data/nahuatl-spanish/train.es nah es

!wc -l americasnlp2021/data/nahuatl-spanish/train.es-filtered.es

"""# Tokenización"""

# Juntamos los datasets de dev y train
!cat americasnlp2021/data/nahuatl-spanish/dev.nah-filtered.nah americasnlp2021/data/nahuatl-spanish/train.nah-filtered.nah > americasnlp2021/data/nahuatl-spanish/nah-es.nah
!cat americasnlp2021/data/nahuatl-spanish/dev.es-filtered.es americasnlp2021/data/nahuatl-spanish/train.es-filtered.es > americasnlp2021/data/nahuatl-spanish/nah-es.es

# Entrenamos el modelo de subword tokenization
!python3 MT-Preparation/subwording/1-train_unigram.py americasnlp2021/data/nahuatl-spanish/nah-es.nah americasnlp2021/data/nahuatl-spanish/nah-es.es

# Subword the dataset
!python3 MT-Preparation/subwording/2-subword.py source.model target.model americasnlp2021/data/nahuatl-spanish/nah-es.es americasnlp2021/data/nahuatl-spanish/nah-es.nah

# Split the dataset into training set, development set, and test set
# Development and test sets should be between 1000 and 5000 segments (here we chose 2000)
!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 2000 2000 americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword americasnlp2021/data/nahuatl-spanish/nah-es.es.subword

"""# Configuración del modelo"""

!pip3 install --upgrade pip

!pip install OpenNMT-py -U

# Creación del archivo de configuración
# train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint
SRC_DATA_NAME = "americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword"
TARGET_DATA_NAME = "americasnlp2021/data/nahuatl-spanish/nah-es.es.subword"

config = f'''# config.yaml

## Where the samples will be written
save_data: run

# Rutas de archivos de entrenamiento
#(previamente aplicado subword tokenization)
data:
    corpus_1:
        path_src: {SRC_DATA_NAME}.train
        path_tgt: {TARGET_DATA_NAME}.train
        transforms: [filtertoolong]
    valid:
        path_src: {SRC_DATA_NAME}.dev
        path_tgt: {TARGET_DATA_NAME}.dev
        transforms: [filtertoolong]

# Vocabularios (serán generados por `onmt_build_vocab`)
src_vocab: run/source.vocab
tgt_vocab: run/target.vocab

# Tamaño del vocabulario
#(debe concordar con el parametro usado en el algoritmo de subword tokenization)
src_vocab_size: 50000
tgt_vocab_size: 50000

# Filtrado sentencias de longitud mayor a n
# actuara si [filtertoolong] está presente
src_seq_length: 150
src_seq_length: 150

# Tokenizadores
src_subword_model: source.model
tgt_subword_model: target.model

# Archivos donde se guardaran los logs y los checkpoints de modelos
log_file: train.log
save_model: models/model.enfr

# Condición de paro si no se obtienen mejoras significativas
# despues de n validaciones
early_stopping: 4

# Guarda un checkpoint del modelo cada n steps
save_checkpoint_steps: 1000

# Mantiene los n ultimos checkpoints
keep_checkpoint: 3

# Reproductibilidad
seed: 3435

# Entrena el modelo maximo n steps
# Default: 100,000
train_steps: 3000

# Corre el set de validaciones (*.dev) despues de n steps
# Defatul: 10,000
valid_steps: 1000

warmup_steps: 1000
report_every: 100

# Numero de GPUs y sus ids
world_size: 1
gpu_ranks: [0]

# Batching
bucket_size: 262144
num_workers: 0
batch_type: "tokens"
batch_size: 4096
valid_batch_size: 2048
max_generator_batches: 2
accum_count: [4]
accum_steps: [0]

# Configuración del optimizador
model_dtype: "fp16"
optim: "adam"
learning_rate: 2
# warmup_steps: 8000
decay_method: "noam"
adam_beta2: 0.998
max_grad_norm: 0
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

# Configuración del Modelo
encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 6
dec_layers: 6
heads: 8
hidden_size: 512
word_vec_size: 512
transformer_ff: 2048
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]
'''

with open("/content/drive/MyDrive/neural_machine_translation/config.yaml", "w+") as config_yaml:
  config_yaml.write(config)

!cat /content/drive/MyDrive/neural_machine_translation/config.yaml

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2

# Check if the GPU is visable to PyTorch

import torch

print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

gpu_memory = torch.cuda.mem_get_info(0)
print("Free GPU memory:", gpu_memory[0]/1024**2, "out of:", gpu_memory[1]/1024**2)

"""# Entrenamiento"""

!ls

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !onmt_train -config config.yaml

"""# Traducción"""

!ls models

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !onmt_translate -model models/model.enfr_step_3000.pt -src americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.test -output es.translated -gpu 0 -min_length 1

!python MT-Preparation/subwording/3-desubword.py source.model americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.test

!python MT-Preparation/subwording/3-desubword.py source.model es.translated

!tail americasnlp2021/data/nahuatl-spanish/nah-es.nah.subword.test.desubword

!tail es.translated.desubword

"""# Evaluación"""

!git clone https://github.com/ymoslem/MT-Evaluation.git

!pip install -r MT-Evaluation/requirements.txt

# Aplicando desubword para tener la traducción de referencia
!python MT-Preparation/subwording/3-desubword.py target.model americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.test

"""BLEU"""

!python MT-Evaluation/BLEU/compute-bleu.py americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.test.desubword es.translated.desubword

"""BLEU: 6.926355150258824

ChrF
"""

!python americasnlp2021/evaluate.py --sys americasnlp2021/data/nahuatl-spanish/nah-es.es.subword.test.desubword --ref es.translated.desubword

"""chrF2 = 32.06

# Comparación con baseline

Mis resultados en BLEU fueron entre 6.92, en ChrF fue de 32.06.

Los resultados de baseline en BLUE fueron 0.33 y en ChrF de 0.182.

Por lo tanto el modelo que hice tuvo mejores resultados que el baseline en ambas métricas, pero más en ChrF.

# Extra

Investigar porque se propuso la medida ChrF en el Shared Task.

ChrF es una métrica a nivel de caracteres. Se propuso porque para idiomas con mucha morfología, las métricas a nivel palabras son muy problemáticas.

¿Como se diferencia de BLEU?

ChrF se basa en comparar similitud entre n-gramas de caracteres, mientras que BLEU lo compara entre n-gramas de palabras.

¿Por qué es relevante utilizar otras medidas de evaluación además de BLEU?

Porque distintos idiomas tienen propiedades distintas. Para idiomas con mucha morfología, dos palabras pueden tener casi los mismos morfemas aunque sean palabras diferentes; eso no se capturaría en una métrica a nivel palabra como BLEU, pero sí en una a nivel caracter.
"""