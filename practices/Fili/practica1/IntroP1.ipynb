{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f47c3c-dfb4-49ee-907f-84d002ac892c",
   "metadata": {},
   "source": [
    "## Practica 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2309b-8ae8-4c79-8ab7-6b7009fdecf6",
   "metadata": {},
   "source": [
    "### Fonologia\n",
    "- La fonología es una rama de la Lingüística que estudia como las lenguajes sistematicamente organizan los fonemas\n",
    "- Estudia como los humanos producimos y percibimos el lenguaje\n",
    "    - Producción: La forma en que producimos el lenguaje\n",
    "    - Percepción: La forma en que interpretamos el lenguaje\n",
    "\n",
    "### Fonetica\n",
    "El estudio de los sonidos físicos del discurso humano. Es la rama de la lingüística que estudia la producción y percepción de los sonidos de una lengua con respecto a sus manifestaciones físicas.\n",
    "\n",
    "**International Phonetic Alphabet (IPA)**\n",
    "\n",
    "- Las lenguas naturales tienen muchos sonidos diferentes por lo que necesitamos una forma de describirlos independientemente de las lenguas\n",
    "- Por ejemplo: Los sonidos del habla se determinan por los movimientos de la boca necesarios para producirlos\n",
    "- Las dos grandes categorías: Consonantes y Vocales\n",
    "- IPA es una representación escrita de los sonidos del habla\n",
    "\n",
    "**Dataset: IPA**\n",
    "Diccionario de palabras para varios idiomas con su representacion fonetica.\n",
    "Representacion simple, una palabra por renglon con el formato:\n",
    "\n",
    "Ejemplos: \n",
    "\n",
    "mariguana     /maɾiɣwana/  \n",
    "zyuganov's    /ˈzjuɡɑnɑvz/, /ˈzuɡɑnɑvz/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b73e0-4c76-4a6b-b3dd-07cce0a16e60",
   "metadata": {},
   "source": [
    "### Explorando el Corpus\n",
    "Este código está diseñado para trabajar con un corpus de datos que contiene palabras en un\n",
    "idioma específico junto con sus transcripciones fonéticas en el Alfabeto Fonético Internacional (AFI). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91838861-3e17-49c7-a1d2-8ae0495add5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'bout\\t/ˈbaʊt/\\n'cause\\t/kəz/\\n'course\\t/ˈkɔɹs/\\n'cuse\\t/ˈkjuz/\\n'em\\t/əm/\\n'frisco\\t/ˈfɹɪskoʊ/\\n'gain\\t/ˈɡɛn/\\n'k\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explorando el corpus\n",
    "import requests as r\n",
    "\n",
    "response = r.get(\"https://raw.githubusercontent.com/open-dict-data/ipa-dict/master/data/en_US.txt\")\n",
    "response.text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801582f3-b9df-46e4-821a-c287b85eccc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'bout\\t/ˈbaʊt/\",\n",
      " \"'cause\\t/kəz/\",\n",
      " \"'course\\t/ˈkɔɹs/\",\n",
      " \"'cuse\\t/ˈkjuz/\",\n",
      " \"'em\\t/əm/\",\n",
      " \"'frisco\\t/ˈfɹɪskoʊ/\",\n",
      " \"'gain\\t/ˈɡɛn/\",\n",
      " \"'kay\\t/ˈkeɪ/\",\n",
      " \"'m\\t/əm/\",\n",
      " \"'n\\t/ən/\",\n",
      " \"'round\\t/ˈɹaʊnd/\",\n",
      " \"'s\\t/ˈɛs/\",\n",
      " \"'til\\t/ˈtɪɫ/\",\n",
      " \"'tis\\t/ˈtɪz/\",\n",
      " \"'twas\\t/ˈtwəz/\",\n",
      " 'a\\t/ˈeɪ/, /ə/',\n",
      " 'a.\\t/ˈeɪ/',\n",
      " \"a.'s\\t/ˈeɪz/\",\n",
      " 'a.d.\\t/ˌeɪˈdi/',\n",
      " 'a.m.\\t/ˌeɪˈɛm/',\n",
      " 'a.s\\t/ˈeɪz/',\n",
      " \"a's\\t/ˈeɪz/\",\n",
      " 'aaa\\t/ˌtɹɪpəˈɫeɪ/',\n",
      " 'aaberg\\t/ˈɑbɝɡ/',\n",
      " 'aachen\\t/ˈɑkən/',\n",
      " 'aachener\\t/ˈɑkənɝ/',\n",
      " 'aaker\\t/ˈɑkɝ/',\n",
      " 'aaliyah\\t/ˌɑˈɫiˌɑ/',\n",
      " 'aalseth\\t/ˈɑɫsɛθ/',\n",
      " 'aamodt\\t/ˈɑmət/',\n",
      " 'aancor\\t/ˈɑnˌkɔɹ/',\n",
      " 'aardema\\t/ɑɹˈdɛmə/',\n",
      " 'aardvark\\t/ˈɑɹdˌvɑɹk/',\n",
      " 'aardvarks\\t/ˈɑɹdˌvɑɹks/',\n",
      " 'aargh\\t/ˈɑɹɡ/',\n",
      " 'aarhus\\t/ˌɑˈhus/',\n",
      " 'aaron\\t/ˈɛɹən/',\n",
      " \"aaron's\\t/ˈɛɹənz/\",\n",
      " 'aarons\\t/ˈɛɹənz/',\n",
      " 'aaronson\\t/ˈɑɹənsən/, /ˈɛɹənsən/',\n",
      " \"aaronson's\\t/ˈɑɹənsənz/, /ˈɛɹənsənz/\",\n",
      " 'aarti\\t/ˈɑɹˌti/',\n",
      " 'aase\\t/ˈɑs/',\n",
      " 'aasen\\t/ˈɑsən/',\n",
      " 'ab\\t/ˈæb/, /ˈeɪˈbi/',\n",
      " 'aba\\t/ˌeɪˌbiˈeɪ/',\n",
      " 'ababa\\t/ˈɑbəbə/, /əˈbɑbə/',\n",
      " 'abacha\\t/ˈæbəkə/',\n",
      " 'aback\\t/əˈbæk/',\n",
      " 'abaco\\t/ˈæbəˌkoʊ/',\n",
      " 'abacus\\t/ˈæbəkəs/',\n",
      " 'abad\\t/əˈbɑd/',\n",
      " 'abadaka\\t/əˈbædəkə/',\n",
      " 'abadi\\t/əˈbædi/',\n",
      " 'abadie\\t/əˈbædi/',\n",
      " 'abair\\t/əˈbɛɹ/',\n",
      " 'abalkin\\t/əˈbɑɫkɪn/',\n",
      " 'abalone\\t/ˌæbəˈɫoʊni/',\n",
      " 'abalones\\t/ˌæbəˈɫoʊniz/',\n",
      " 'abalos\\t/ɑˈbɑɫoʊz/',\n",
      " 'abandon\\t/əˈbændən/',\n",
      " 'abandoned\\t/əˈbændənd/',\n",
      " 'abandoning\\t/əˈbændənɪŋ/',\n",
      " 'abandonment\\t/əˈbændənmənt/',\n",
      " 'abandonments\\t/əˈbændənmənts/',\n",
      " 'abandons\\t/əˈbændənz/',\n",
      " 'abanto\\t/əˈbæntoʊ/',\n",
      " 'abarca\\t/əˈbɑɹkə/',\n",
      " 'abare\\t/ɑˈbɑɹi/',\n",
      " 'abascal\\t/ˈæbəskəɫ/',\n",
      " 'abash\\t/əˈbæʃ/',\n",
      " 'abashed\\t/əˈbæʃt/',\n",
      " 'abasia\\t/əˈbeɪʒjə/',\n",
      " 'abate\\t/əˈbeɪt/',\n",
      " 'abated\\t/əˈbeɪtɪd/',\n",
      " 'abatement\\t/əˈbeɪtmənt/',\n",
      " 'abatements\\t/əˈbeɪtmənts/',\n",
      " 'abates\\t/əˈbeɪts/',\n",
      " 'abating\\t/əˈbeɪtɪŋ/',\n",
      " 'abattoir\\t/ˌæbətˈwɑɹ/',\n",
      " 'abba\\t/ˈæbə/',\n",
      " 'abbado\\t/əˈbɑdoʊ/',\n",
      " 'abbas\\t/əˈbɑs/',\n",
      " 'abbasi\\t/ɑˈbɑsi/',\n",
      " 'abbate\\t/ˈɑbeɪt/',\n",
      " 'abbatiello\\t/ɑbɑtiˈɛɫoʊ/',\n",
      " 'abbe\\t/ˈæbi/, /æˈbeɪ/',\n",
      " 'abbenhaus\\t/ˈæbənˌhaʊs/',\n",
      " 'abbett\\t/əˈbɛt/',\n",
      " 'abbeville\\t/ˈæbvɪɫ/',\n",
      " 'abbey\\t/ˈæbi/',\n",
      " \"abbey's\\t/ˈæbiz/\",\n",
      " 'abbie\\t/ˈæbi/',\n",
      " 'abbitt\\t/ˈæbɪt/',\n",
      " 'abbot\\t/ˈæbət/',\n",
      " 'abbotstown\\t/ˈæbətˈstaʊn/',\n",
      " 'abbott\\t/ˈæbət/',\n",
      " \"abbott's\\t/ˈæbəts/\",\n",
      " 'abbottstown\\t/ˈæbətˈstaʊn/',\n",
      " 'abboud\\t/əˈbaʊd/, /əˈbud/']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pp\n",
    "ipa_data = response.text.split(\"\\n\")\n",
    "#print(ipa_data[-4:])\n",
    "ipa_data[-1]\n",
    "pp(ipa_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c398df01-d382-4f83-b752-9028366f76b3",
   "metadata": {},
   "source": [
    "-  Se importa la biblioteca requests con el alias r para realizar solicitudes HTTP.\n",
    "-  Se importa la función pprint de la biblioteca pprint con el alias pp para imprimir datos de forma más legible.\n",
    "-  Se utiliza requests.get() para obtener los datos del corpus desde una URL específica.\n",
    "-  Se divide el texto en líneas utilizando split(\"\\n\"), lo que crea una lista llamada ipa_data, donde cada elemento es una línea del archivo.\n",
    "    \n",
    "*Nota:* response.text contiene el contenido del archivo obtenido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f4584f-521e-4cd7-b98f-01f513c96bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"zyuganov's\", '/ˈzjuɡɑnɑvz/, /ˈzuɡɑnɑvz/']\n",
      "abs --> ['/ˈæbz/', '/ˈeɪˈbiˈɛs/']\n",
      "absolve --> ['/æbˈzɑɫv/', '/əbˈzɑɫv/']\n",
      "absolved --> ['/æbˈzɑɫvd/', '/əbˈzɑɫvd/']\n",
      "absolves --> ['/æbˈzɑɫvz/', '/əbˈzɑɫvz/']\n",
      "absolving --> ['/æbˈzɑɫvɪŋ/', '/əbˈzɑɫvɪŋ/']\n",
      "absorption --> ['/əbˈsɔɹpʃən/', '/əbˈzɔɹpʃən/']\n",
      "abstain --> ['/æbˈsteɪn/', '/əbˈsteɪn/']\n",
      "abstained --> ['/æbˈsteɪnd/', '/əbˈsteɪnd/']\n",
      "abstaining --> ['/æbˈsteɪnɪŋ/', '/əbˈsteɪnɪŋ/']\n",
      "abstention --> ['/æbˈstɛntʃən/', '/əbˈstɛntʃən/']\n",
      "abstentions --> ['/æbˈstɛntʃənz/', '/əbˈstɛntʃənz/']\n",
      "abstract --> ['/ˈæbˌstɹækt/', '/æbˈstɹækt/']\n",
      "abt --> ['/ˈæbt/', '/ˈeɪˈbiˈti/']\n",
      "abts --> ['/ˈæbts/', '/ˈeɪˈbiˈtiˈɛs/', '/ˈeɪˈbiˈtiz/']\n",
      "abuse --> ['/əbˈjus/', '/əbˈjuz/']\n",
      "abuses --> ['/əbˈjusɪz/', '/əbˈjuzɪz/']\n",
      "abzug --> ['/ˈæbˌzəɡ/', '/ˈæbˌzʊɡ/']\n",
      "academicians --> ['/ˌækədəˈmɪʃənz/', '/əˌkædəˈmɪʃənz/']\n",
      "accent --> ['/ˈækˌsɛnt/', '/əkˈsɛnt/']\n",
      "accept --> ['/ækˈsɛpt/', '/əkˈsɛpt/']\n",
      "acceptable --> ['/ækˈsɛptəbəɫ/', '/əkˈsɛptəbəɫ/']\n",
      "acceptably --> ['/ækˈsɛptəbɫi/', '/əkˈsɛptəbɫi/']\n",
      "acceptance --> ['/ækˈsɛptəns/', '/əkˈsɛptəns/']\n",
      "accepted --> ['/ækˈsɛptɪd/', '/əkˈsɛptɪd/']\n",
      "accepting --> ['/ækˈsɛptɪŋ/', '/əkˈsɛptɪŋ/']\n",
      "accidental --> ['/ˌæksəˈdɛnəɫ/', '/ˌæksəˈdɛntəɫ/']\n",
      "accidentally --> ['/ˌæksəˈdɛnəɫi/', '/ˌæksəˈdɛntəɫi/']\n"
     ]
    }
   ],
   "source": [
    "# Puede haber mas de una transcipcion asociada a una palabra\n",
    "print(ipa_data[-3].split(\"\\t\"))\n",
    "for data in ipa_data[300:350]:\n",
    "    word, ipa = data.split('\\t')\n",
    "    representations = ipa.split(\", \")\n",
    "    if len(representations) >= 2:\n",
    "        print(f\"{word} --> {representations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0f57e-7c72-4346-bbdc-c6a426b578a9",
   "metadata": {},
   "source": [
    "- Itera sobre un rango específico de líneas del corpus (líneas 300 a 499).\n",
    "- Para cada línea, divide la línea en dos partes usando \\t (tabulación) como separador. La primera parte es la palabra en inglés y la segunda es su transcripción fonética en AFI.\n",
    "- Divide la transcripción fonética en AFI en una lista de representaciones fonéticas individuales utilizando split(\", \").\n",
    "- Si hay dos o más representaciones fonéticas para una palabra, imprime la palabra junto con todas sus representaciones fonéticas.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2772ec5-22db-4681-a600-5fb49ef8cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_to_dict(ipa_list: list) -> dict:\n",
    "    \"\"\"Parse to dict the list of word-IPA\n",
    "\n",
    "    Each element of text have the format:\n",
    "    [WORD][TAB][IPA]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ipa_list: list\n",
    "        List with each row of ipa-dict raw dataset file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        A dictionary with the word as key and the phonetic\n",
    "        representation as value\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for item in ipa_list:\n",
    "       item_list = item.split(\"\\t\")\n",
    "       result[item_list[0]] = item_list[1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f5af324-18c0-44af-bfbd-e6150eeaff0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kəz/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_to_dict(ipa_data[:100])[\"'cause\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7355ff-ee54-486b-bf2a-5c4098490487",
   "metadata": {},
   "source": [
    "```response_to_dict(ipa_list: list) -> dict:```\n",
    "Esta función toma una lista de líneas de texto que representan las palabras y sus transcripciones \n",
    "fonéticas en el formato [PALABRA][TAB][IPA] y devuelve un diccionario donde las palabras \n",
    "son las claves y las transcripciones fonéticas son los valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25585bdb-17f9-4b95-b2d2-663208387037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ipa_dict(iso_lang: str) -> dict:\n",
    "    \"\"\"Get ipa-dict file from Github\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    iso_lang:\n",
    "        Language as iso code\n",
    "\n",
    "    Results:\n",
    "    --------\n",
    "    dict:\n",
    "        Dictionary with words as keys and phonetic representation\n",
    "        as values for a given lang code\n",
    "    \"\"\"\n",
    "    print(f\"Downloading {iso_lang}\", end=\" \")\n",
    "    response = r.get(f\"https://raw.githubusercontent.com/open-dict-data/ipa-dict/master/data/{iso_lang}.txt\") \n",
    "    raw_data = response.text.split(\"\\n\")\n",
    "    print(f\"status:{response.status_code}\")\n",
    "    return response_to_dict(raw_data[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fd67538-015f-4177-a7c4-2ed578872216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading es_MX status:200\n"
     ]
    }
   ],
   "source": [
    "es_mx_ipa = get_ipa_dict(\"es_MX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0d50f-5a60-44f2-8234-a9e69a993304",
   "metadata": {},
   "source": [
    "```get_ipa_dict(iso_lang: str) -> dict:```\n",
    "Esta función obtiene los datos del corpus desde GitHub para un idioma específico (usando el código ISO del idioma).\n",
    "Descarga el archivo de texto correspondiente al idioma especificado, lo procesa y lo convierte en un diccionario utilizando \n",
    "la función response_to_dict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78a6d923-a939-4c05-ab9c-e8eb234be5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ipa_transcriptions(word: str, dataset: dict) -> list[str]:\n",
    "    \"\"\"Search for a word in an IPA phonetics dict\n",
    " \n",
    "    Given a word this function return the IPA transcriptions\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    word: str\n",
    "        A word to search in the dataset\n",
    "    dataset: dict\n",
    "        A dataset for a given language code\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list[str]:\n",
    "        List with posible transcriptions if any, \n",
    "        else a list with the string \"NOT FOUND\" \n",
    "    \"\"\"\n",
    "    return dataset.get(word.lower(), \"NOT FOUND\").split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66d3e291-4e7f-4505-8df1-2df7e8671b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/maʝonesa/']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_ipa_transcriptions(\"mayonesa\", es_mx_ipa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1291c-f712-4c42-aaf0-b16796a82dd5",
   "metadata": {},
   "source": [
    "```query_ipa_transcriptions(word: str, dataset: dict) -> list[str]:```\n",
    "Esta función toma una palabra y un diccionario que contiene las palabras y sus transcripciones \n",
    "fonéticas, y devuelve una lista de posibles transcripciones fonéticas de la palabra dada. \n",
    "Si la palabra no se encuentra en el diccionario, devuelve una lista con el string \"NOT FOUND\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ccce02-e236-43a6-969f-0a8b304a6024",
   "metadata": {},
   "source": [
    "### Obtengamos un par de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6accf39-2275-4359-aae6-4036c288d879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading es_MX status:200\n",
      "Downloading en_US status:200\n"
     ]
    }
   ],
   "source": [
    "dataset_mx = get_ipa_dict(\"es_MX\")\n",
    "dataset_us= get_ipa_dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc420cc-ca6f-48f5-b754-7ca81f19f5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/ˈwɪmən/']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simple query\n",
    "query_ipa_transcriptions(\"women\",dataset_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "134680ac-d4bb-4d69-b949-a4700b8705a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog -> ['/ˈdɔɡ/']\n"
     ]
    }
   ],
   "source": [
    "print(f\"dog -> {query_ipa_transcriptions('dog',dataset_us)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2adefeb-5e1a-4b36-8d80-3445d26fb34e",
   "metadata": {},
   "source": [
    "**Diferentes formas de pronunciar dependiendo la lengua, aunque la ortografia se parezca. Ejemplo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "333646d1-2bc1-47ff-87e4-78357d540079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "español hotel -> ['/otel/']\n",
      "ingles hotel -> ['/hoʊˈtɛɫ/']\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo:\n",
    "print(\"español \"+ f\"hotel -> {query_ipa_transcriptions('hotel',dataset_mx)}\")\n",
    "print(\"ingles \"+ f\"hotel -> {query_ipa_transcriptions('hotel',dataset_us)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e40d4d-5b0d-47cf-b53b-9297294f9c99",
   "metadata": {},
   "source": [
    "**Obteniendo Corpues desde GitHub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9031f6ba-d126-4377-9525-37c1853b42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_codes = {\n",
    "  \"ar\": \"Arabic (Modern Standard)\",\n",
    "  \"de\": \"German\",\n",
    "  \"en_UK\": \"English (Received Pronunciation)\",\n",
    "  \"en_US\": \"English (General American)\",\n",
    "  \"eo\": \"Esperanto\",\n",
    "  \"es_ES\": \"Spanish (Spain)\",\n",
    "  \"es_MX\": \"Spanish (Mexico)\",\n",
    "  \"fa\": \"Persian\",\n",
    "  \"fi\": \"Finnish\",\n",
    "  \"fr_FR\": \"French (France)\",\n",
    "  \"fr_QC\": \"French (Québec)\",\n",
    "  \"is\": \"Icelandic\",\n",
    "  \"ja\": \"Japanese\",\n",
    "  \"jam\": \"Jamaican Creole\",\n",
    "  \"km\": \"Khmer\",\n",
    "  \"ko\": \"Korean\",\n",
    "  \"ma\": \"Malay (Malaysian and Indonesian)\",\n",
    "  \"nb\": \"Norwegian Bokmål\",\n",
    "  \"nl\": \"Dutch\",\n",
    "  \"or\": \"Odia\",\n",
    "  \"ro\": \"Romanian\",\n",
    "  \"sv\": \"Swedish\",\n",
    "  \"sw\": \"Swahili\",\n",
    "  \"tts\": \"Isan\",\n",
    "  \"vi_C\": \"Vietnamese (Central)\",\n",
    "  \"vi_N\": \"Vietnamese (Northern)\",\n",
    "  \"vi_S\": \"Vietnamese (Southern)\",\n",
    "  \"yue\": \"Cantonese\",\n",
    "  \"zh\": \"Mandarin\"\n",
    "}\n",
    "iso_lang_codes = list(lang_codes.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbdb15d-4a40-4435-acb5-8bb53d7f26f6",
   "metadata": {},
   "source": [
    "Este código define un diccionario llamado lang_codes que mapea códigos ISO de idiomas a sus nombres correspondientes. Luego, crea una lista llamada iso_lang_codes que contiene solo los códigos ISO de idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf2a6c24-bf52-4135-bf3f-7bffa2112689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ar status:200\n",
      "Downloading de status:200\n",
      "Downloading en_UK status:200\n",
      "Downloading en_US status:200\n",
      "Downloading eo status:200\n",
      "Downloading es_ES status:200\n",
      "Downloading es_MX status:200\n",
      "Downloading fa status:200\n",
      "Downloading fi status:200\n",
      "Downloading fr_FR status:200\n",
      "Downloading fr_QC status:200\n",
      "Downloading is status:200\n",
      "Downloading ja status:200\n",
      "Downloading jam status:200\n",
      "Downloading km status:200\n",
      "Downloading ko status:200\n",
      "Downloading ma status:200\n",
      "Downloading nb status:200\n",
      "Downloading nl status:200\n",
      "Downloading or status:200\n",
      "Downloading ro status:200\n",
      "Downloading sv status:200\n",
      "Downloading sw status:200\n",
      "Downloading tts status:200\n",
      "Downloading vi_C status:200\n",
      "Downloading vi_N status:200\n",
      "Downloading vi_S status:200\n",
      "Downloading yue status:200\n",
      "Downloading zh status:404\n"
     ]
    }
   ],
   "source": [
    "def get_dataset() -> dict:\n",
    "    \"\"\"Download corpora from ipa-dict github\n",
    "\n",
    "    Given a list of iso lang codes download available datasets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Lang codes as keys and dictionary with words-transcriptions\n",
    "        as values\n",
    "    \"\"\"\n",
    "    return {code: get_ipa_dict(code) for code in iso_lang_codes}\n",
    "\n",
    "dataset = get_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e4e059-ab92-4e28-8e66-c539c3a9cb0c",
   "metadata": {},
   "source": [
    "Después, se define una función llamada get_dataset(). Esta función no toma ningún argumento y tiene como objetivo descargar conjuntos de datos de un repositorio de GitHub llamado \"ipa-dict\" para cada idioma en la lista de códigos ISO de idiomas. La función devuelve un diccionario donde las claves son los códigos ISO de idiomas y los valores son los conjuntos de datos obtenidos para cada idioma.\n",
    "\n",
    "El diccionario dataset se inicializa llamando a la función get_dataset(). Por lo tanto, dataset contendrá datos de transcripción fonética para cada idioma enumerado en lang_codes. Cada valor en el diccionario dataset será otro diccionario que contiene las palabras y sus transcripciones fonéticas para el idioma correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eeab29-b53c-42c6-a8c2-6ba5652d84b0",
   "metadata": {},
   "source": [
    "```{code: get_ipa_dict(code) for code in iso_lang_codes}```\n",
    "Es una expresión de diccionario con comprensión de lista. La comprensión de lista se utiliza para construir un diccionario en una sola línea de código, utilizando un bucle for para iterar sobre una secuencia y generar pares clave-valor en el diccionario resultante.\n",
    "- {}: Indica que estamos creando un diccionario.\n",
    "- code: get_ipa_dict(code): Es la expresión que define cada par clave-valor en el diccionario. code es el nombre de la clave, y get_ipa_dict(code) es el valor asociado a esa clave. Aquí, code es un elemento de la lista iso_lang_codes y get_ipa_dict(code) es una llamada a la función get_ipa_dict() con code como argumento, lo que devuelve el conjunto de datos correspondiente al idioma code.\n",
    "- for code in iso_lang_codes: Esto itera sobre cada elemento en la lista iso_lang_codes, asignando cada elemento a la variable code.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f1230-06fa-4ee4-8f8f-10bbfceab909",
   "metadata": {},
   "source": [
    "### Creando aplicaciones con estos datos\n",
    "1. **Buesquedas basicas automatizadas**  \n",
    "       Buscador de representaciones foneticas de palabras automatizado en diferentes idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d44523c-780a-4496-a845-a8f36987e04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representación fonética de palabras\n",
      "Lenguas disponibles:\n",
      "ar: Arabic (Modern Standard)\n",
      "de: German\n",
      "en_UK: English (Received Pronunciation)\n",
      "en_US: English (General American)\n",
      "eo: Esperanto\n",
      "es_ES: Spanish (Spain)\n",
      "es_MX: Spanish (Mexico)\n",
      "fa: Persian\n",
      "fi: Finnish\n",
      "fr_FR: French (France)\n",
      "fr_QC: French (Québec)\n",
      "is: Icelandic\n",
      "ja: Japanese\n",
      "jam: Jamaican Creole\n",
      "km: Khmer\n",
      "ko: Korean\n",
      "ma: Malay (Malaysian and Indonesian)\n",
      "nb: Norwegian Bokmål\n",
      "nl: Dutch\n",
      "or: Odia\n",
      "ro: Romanian\n",
      "sv: Swedish\n",
      "sw: Swahili\n",
      "tts: Isan\n",
      "vi_C: Vietnamese (Central)\n",
      "vi_N: Vietnamese (Northern)\n",
      "vi_S: Vietnamese (Southern)\n",
      "yue: Cantonese\n",
      "zh: Mandarin\n"
     ]
    }
   ],
   "source": [
    "print(\"Representación fonética de palabras\")\n",
    "\n",
    "print(\"Lenguas disponibles:\")\n",
    "for lang_key in dataset.keys():\n",
    "    print(f\"{lang_key}: {lang_codes[lang_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2857a-54dd-4101-941d-a8b478b44169",
   "metadata": {},
   "source": [
    "Este bloque de código imprime todos los idiomas disponibles y sus nombres correspondientes, utilizando el diccionario lang_codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8802529-fefa-4ee9-8f47-c5f64a8da6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "lang>>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adios 👋🏼\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lang = input(\"lang>> \")\n",
    "print(f\"Selected language: {lang_codes[lang]}\") if lang else print(\"Adios 👋🏼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a4968f-82a1-44a6-996c-bb841e5ed19a",
   "metadata": {},
   "source": [
    "- El usuario debe ingresar el código ISO del idioma que desee seleccionar.\n",
    "-   Si el usuario no ingresa ningún código (es decir, la cadena lang está vacía), el programa imprime \"Adios 👋🏼\" y termina.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74254c30-a987-44d5-93b6-7ac97d272b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "while lang:\n",
    "    # El programa comeinza aqui\n",
    "    sub_data = dataset[lang]\n",
    "    query = input(f\"[{lang}] word>> \")\n",
    "    results = query_ipa_transcriptions(query, sub_data)\n",
    "    print(query, \" | \", results)\n",
    "    while query:\n",
    "        query = input(f\"[{lang}] word>> \")\n",
    "        if query:\n",
    "            results = query_ipa_transcriptions(query, sub_data)\n",
    "            print(query, \" | \", results)\n",
    "    lang = input(\"lang>> \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4657989-9e25-409a-b8e2-f6facdef0178",
   "metadata": {},
   "source": [
    "- Este bucle se ejecuta mientras lang no sea una cadena vacía.\n",
    "- Dentro del bucle, se obtiene el conjunto de datos (sub_data) correspondiente al idioma seleccionado.\n",
    "- Luego, el programa solicita al usuario que ingrese una palabra (query) para buscar su representación fonética en el idioma seleccionado.\n",
    "- Se utilizan las funciones input() y query_ipa_transcriptions() para obtener la entrada del usuario y buscar las transcripciones fonéticas respectivamente.\n",
    "- Las transcripciones fonéticas y la palabra consultada se imprimen en la consola.\n",
    "- Después de cada búsqueda, el programa solicita al usuario que ingrese otra palabra para buscar su representación fonética en el mismo idioma.\n",
    "- Si el usuario ingresa una cadena vacía como palabra de búsqueda, el bucle interno termina y el programa solicita al usuario que seleccione otro idioma (lang)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12ba24-0406-4f89-926a-603a8fcddfe2",
   "metadata": {},
   "source": [
    "### Encontrar las palabras que tengan terminacion similar\n",
    "Dado una oracion agrupar las palabras que tengan una pronuncacion similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81260a6b-c600-45b2-8f9e-f474b29208fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ɣo:: ['juego', 'fuego']\n",
      "on:: ['con', 'corazón']\n",
      "ʎa:: ['brilla', 'orilla']\n"
     ]
    }
   ],
   "source": [
    "#/ Utiliza transcripciones fonéticas para #\n",
    "# agrupar palabras de una oración que tienen pronunciaciones \n",
    "# similares\n",
    "from collections import defaultdict\n",
    "\n",
    "#sentence = \"There once was a cat that ate a rat and after that sat on a yellow mat\"\n",
    "#sentence = \"the cat sat on the mat and looked at the rat.\"\n",
    "#sentence = \"If you drop the ball it will fall on the doll\"\n",
    "sentence = \"cuando juego con fuego siento como brilla la orilla de mi corazón\"\n",
    "\n",
    "#lang = \"en_US\"\n",
    "lang = \"es_MX\"\n",
    "words = sentence.split(\" \")\n",
    "\n",
    "# Get words and IPA transciptions map\n",
    "word_ipa_map = {}\n",
    "for word in words:\n",
    "    ipa_transcriptions = query_ipa_transcriptions(word.lower(), dataset.get(lang))\n",
    "    ipa_transcriptions = [_.strip(\"/\") for _ in ipa_transcriptions]\n",
    "    word_ipa_map.update({word.lower(): ipa_transcriptions})\n",
    "\n",
    "patterns = defaultdict(list)\n",
    "for word, ipa_list in word_ipa_map.items():\n",
    "    for ipa in ipa_list:\n",
    "        ipa_pattern = ipa[-2:]\n",
    "        patterns[ipa_pattern].append(word)\n",
    "\n",
    "for pattern, words in patterns.items():\n",
    "    if len(set(words)) > 1:\n",
    "        print(f\"{pattern}:: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88412525-9de4-4efd-b887-f20d999f7adc",
   "metadata": {},
   "source": [
    "**Obtención de las transcripciones fonéticas para cada palabra:**\n",
    "- Se crea un diccionario llamado word_ipa_map donde las claves son las palabras y los valores son listas de sus transcripciones fonéticas en AFI.\n",
    "- Para cada palabra en la lista words, se llama a la función query_ipa_transcriptions() para obtener sus transcripciones fonéticas en el idioma específico (es_MX).\n",
    "- Se eliminan los caracteres \"/\" de las transcripciones fonéticas para limpiar los datos.\n",
    "- Las palabras y sus transcripciones fonéticas se almacenan en el diccionario word_ipa_map.\n",
    "  \n",
    "**Agrupación de palabras por patrón fonético:**\n",
    "- Se crea un diccionario llamado patterns utilizando defaultdict(list). Esto permitirá almacenar listas de palabras asociadas a un patrón fonético común.\n",
    "- Se itera sobre cada elemento del diccionario word_ipa_map, que contiene palabras y sus transcripciones fonéticas.\n",
    "- Para cada palabra, se itera sobre sus transcripciones fonéticas. Se toman las últimas dos letras de cada transcripción fonética (ipa[-2:]) como un patrón.\n",
    "- Se agrega la palabra a la lista correspondiente dentro del diccionario patterns, usando el patrón fonético como clave.\n",
    "\n",
    "**Impresion de los grupos de palabras con patrones similares**\n",
    "- Finalmente, se itera sobre el diccionario patterns.\n",
    "- Si el número de palabras únicas asociadas a un patrón fonético es mayor que uno, se imprime el patrón fonético junto con las palabras asociadas a él."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da131946-7b2d-4a88-87ef-063caebda71b",
   "metadata": {},
   "source": [
    "### Morfologia y analisis morfologico\n",
    "\n",
    "La morfología es uno de los niveles de la lengua que estudia los procesos que conforman una palabra.\n",
    "\n",
    "**Morfemas**\n",
    "\n",
    "Con la morfología podemos identificar como se modifica el significado variando la estructura de las palabras.\n",
    "\n",
    "Tenemos elementos mínimos, intercambiables que varian el significado de las palabras: morfemas\n",
    "\n",
    "**Tipos de Morfemas**\n",
    "- Bases: Subcadenas que aportan información léxica de la palabra\n",
    "    - sol\n",
    "    - frasada\n",
    "- Afijos: Subcadenas que se adhieren a las bases para añadir información (flexiva, derivativa)\n",
    "    - Prefijos\n",
    "      - in-parable\n",
    "    - Subfijos\n",
    "      - pan-ecitos,come-mos\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e1dbd-3cc6-46f4-8d2e-601dfd2286e9",
   "metadata": {},
   "source": [
    "### Aplicaciones relacionadas:\n",
    "**Analisis morfologico**\n",
    "\n",
    "La morfologia es uno de los niveles mas basicos del lenguaje\n",
    "que se puede estudiar. En ese sentido, una de las tareas\n",
    "mas basicas del NLP es el analisis morfologico.\n",
    "\n",
    "> El analisis morfologico es la determinacion de las partes que componen la palabra y su representacion linguistica, es una especie de etiquetado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a0d29-8a5e-4eb1-a931-11c2f807e393",
   "metadata": {},
   "source": [
    "Los elementos morfológicos son analizados para:\n",
    "- Determinar la función morfológica de las palabras\n",
    "- Hacer filtrado y pre-procesamiento de texto\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Parsing con expresiones regulares\n",
    "\n",
    "La estructura del sustantivo en español es: ```BASE+AFIJOS (marcas flexivas)   --> Base+DIM+GEN+NUM```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f85c7bd-b406-4cc8-bae9-d70f68932cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = [\n",
    "    'niño',\n",
    "    'niños',\n",
    "    'niñas',\n",
    "    'niñitos',\n",
    "    'gato',\n",
    "    'gatos',\n",
    "    'gatitos',\n",
    "    'perritos',\n",
    "    'paloma',\n",
    "    'palomita',\n",
    "    'palomas',\n",
    "    'flores',\n",
    "    'flor',\n",
    "    'florecita',\n",
    "    'lápiz',\n",
    "    'lápices',\n",
    "    #'chiquitititititos',\n",
    "    #'curriculum', # curricula\n",
    "    #'campus', # campi\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3e754e8-8efa-4fb3-968e-08028c415e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# esta función aplica reglas morfológicas a una lista de palabras utilizando expresiones regulares para\n",
    "# capturar ciertos morfemas y realizar un análisis morfológico. \n",
    "# Cada regla morfológica define un patrón específico que se busca y reemplaza en las palabras de entrada.\n",
    "\n",
    "def morph_parser_rules(words: list[str]) -> list[str]:\n",
    "    \"\"\"Aplica reglas morfológicas a una lista de palabras para realizar\n",
    "    un análisis morfológico.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    words : list of str\n",
    "        Lista de palabras a las que se les aplicarán las reglas morfológicas.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list of str\n",
    "        Una lista de palabras después de aplicar las reglas morfológicas.\n",
    "    \"\"\"\n",
    "\n",
    "    #Lista para guardar las palabras parseadas\n",
    "    morph_parsing = []\n",
    "\n",
    "    # Reglas que capturan ciertos morfemas\n",
    "    # {ecita, itos, as, os}\n",
    "    for w in words:\n",
    "        #ecit -> DIM\n",
    "        R0 = re.sub(r'([^ ]+)ecit([a|o|as|os])', r'\\1-DIM\\2', w)\n",
    "        #it -> DIM\n",
    "        R1 = re.sub(r'([^ ]+)it([a|o|as|os])', r'\\1-DIM\\2', R0)\n",
    "        #a(s) -> FEM\n",
    "        R2 = re.sub(r'([^ ]+)a(s)', r'\\1-FEM\\2', R1)\n",
    "        #a -> FEM\n",
    "        R3 = re.sub(r'([^ ]+)a\\b', r'\\1-FEM', R2)\n",
    "        #o(s) -> MSC\n",
    "        R4 = re.sub(r'([^ ]+)o(s)', r'\\1-MSC\\2', R3)\n",
    "        #o .> MSC\n",
    "        R5 = re.sub(r'([^ ]+)o\\b', r'\\1-MSC', R4)\n",
    "        #es -> PL\n",
    "        R6 = re.sub(r'([^ ]+)es\\b', r'\\1-PL', R5)\n",
    "        #s -> PL\n",
    "        R7 = re.sub(r'([^ ]+)s\\b', r'\\1-PL', R6)\n",
    "        #Sustituye la c por z cuando es necesario\n",
    "        parse = re.sub(r'c-', r'z-', R7)\n",
    "\n",
    "        #Guarda los parseos\n",
    "        morph_parsing.append(parse)\n",
    "    return morph_parsing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0926a894-466b-4548-8ede-ccf1ba75b9f9",
   "metadata": {},
   "source": [
    "Esta función ```morph_parser_rules``` toma una lista de palabras como entrada y aplica una serie de reglas morfológicas para realizar un análisis morfológico de las palabras.\n",
    "- Exporta re biblioteca de expresiones regulares\n",
    "- Se define la función morph_parser_rules que recibe una lista de palabras como entrada y devuelve una lista de palabras después de aplicar las reglas morfológicas.\n",
    "- Se inicializa una lista vacía morph_parsing que se utilizará para almacenar las palabras parseadas.\n",
    "- Se definen una serie de reglas morfológicas mediante expresiones regulares para capturar ciertos morfemas en las palabras:\n",
    "\n",
    "    Por ejemplo, ([^ ]+)ecit([a|o|as|os]) captura palabras que terminan en \"ecit\" seguido de \"a\", \"o\", \"as\" o \"os\".\n",
    "    Cada regla tiene un patrón similar que se utiliza para buscar y reemplazar los morfemas específicos.\n",
    "    Estos morfemas pueden ser sufijos que indican género (FEM para femenino, MSC para masculino), número (PL para plural), etc.\n",
    "- Se aplican las reglas morfológicas a cada palabra en la lista de palabras de entrada utilizando el método sub del módulo re. Cada regla se aplica secuencialmente para obtener el resultado final.\n",
    "- Se agrega la palabra parseada a la lista morph_parsing.\n",
    "- Finalmente, se devuelve la lista de palabras parseadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4975467a-f2e9-49ca-a0c7-baa8c53b04c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niño --> niñ-MSC\n",
      "niños --> niñ-MSC-PL\n",
      "niñas --> niñ-FEM-PL\n",
      "niñitos --> niñ-DIM-MSC-PL\n",
      "gato --> gat-MSC\n",
      "gatos --> gat-MSC-PL\n",
      "gatitos --> gat-DIM-MSC-PL\n",
      "perritos --> perr-DIM-MSC-PL\n",
      "paloma --> palom-FEM\n",
      "palomita --> palom-DIM-FEM\n",
      "palomas --> palom-FEM-PL\n",
      "flores --> flor-PL\n",
      "flor --> flor\n",
      "florecita --> flor-DIM-FEM\n",
      "lápiz --> lápiz\n",
      "lápices --> lápiz-PL\n"
     ]
    }
   ],
   "source": [
    "morph_parsing = morph_parser_rules(palabras)\n",
    "for palabra, parseo in zip(palabras, morph_parsing):\n",
    "    print(palabra, \"-->\", parseo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de0cb9-5ece-424e-8389-bbee58ef15f6",
   "metadata": {},
   "source": [
    "Utiliza la ```función morph_parser_rules``` para aplicar reglas morfológicas a una lista de palabras (palabras). Luego, utiliza un bucle for y la función zip para iterar sobre ambas listas (palabras y morph_parsing) simultáneamente e imprimir cada palabra junto con su parseo. Aquí está el código completo:\n",
    "- Se define una lista de palabras palabras que contiene palabras en español, algunas en singular y otras en plural, y con diferentes géneros.\n",
    "- Se llama a la función morph_parser_rules pasando la lista de palabras como argumento. Esto aplica las reglas morfológicas definidas en la función a cada palabra y devuelve una lista de palabras parseadas.\n",
    "- Se utiliza un bucle for junto con la función zip para iterar simultáneamente sobre las listas palabras y morph_parsing.\n",
    "- En cada iteración del bucle, se imprime la palabra original junto con su parseo\n",
    "\n",
    "Este código imprimirá cada palabra de la lista palabras junto con su correspondiente parseo, mostrando el análisis morfológico aplicado a cada una de ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957921c7-34ed-4282-8b39-656af850bb4c",
   "metadata": {},
   "source": [
    "### Corpus: Shared Task en segmentacion morfologica\n",
    "\n",
    "- Shared task donde se busca convertir las palabras en una secuencia de morfemas\n",
    "- Segmentacion a nivle de palabras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed3535e5-8a6b-4b5e-ac7f-18b9eff3330b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'astronómica\\tastronómico @@a\\t100\\nresignifiques\\tresignificar @@es\\t100\\nimportunamente\\timportuno @@mente'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Obtencion del corpus\n",
    "response = r.get(\"https://raw.githubusercontent.com/sigmorphon/2022SegmentationST/main/data/spa.word.test.gold.tsv\")\n",
    "response.text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171ec545-ecb0-4238-9ec1-709e46ac1d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astronómica\\tastronómico @@a\\t100',\n",
       " 'resignifiques\\tresignificar @@es\\t100',\n",
       " 'importunamente\\timportuno @@mente\\t010',\n",
       " 'conjeturaríamos\\tconjeturar @@ría @@amos\\t100',\n",
       " 'adquiridla\\tadquirir @@id @@la\\t100',\n",
       " 'deslocalizadla\\tdes @@local @@izar @@ad @@la\\t110',\n",
       " 'azafránenla\\tazafrán @@ar @@en @@la\\t110',\n",
       " 'abocinado\\tabocinar @@ado\\t100',\n",
       " 'májalo\\tmajar @@á @@lo\\t100',\n",
       " 'troquelabais\\ttroquel @@ar @@ba @@ais\\t110']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = response.text.split(\"\\n\")\n",
    "raw_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b269e7d-f0dc-4013-bc9c-e801b61a7c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astronómico', '@@a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = raw_data[0].split(\"\\t\")\n",
    "element[1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd632f4d-a8cb-4305-8ba9-18a528086fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astronómica astronómico @@a 100\n",
      "['astronómico', '@@a']\n",
      "resignifiques resignificar @@es 100\n",
      "['resignificar', '@@es']\n",
      "importunamente importuno @@mente 010\n",
      "['importuno', '@@mente']\n",
      "conjeturaríamos conjeturar @@ría @@amos 100\n",
      "['conjeturar', '@@ría', '@@amos']\n",
      "adquiridla adquirir @@id @@la 100\n",
      "['adquirir', '@@id', '@@la']\n",
      "deslocalizadla des @@local @@izar @@ad @@la 110\n",
      "['des', '@@local', '@@izar', '@@ad', '@@la']\n",
      "azafránenla azafrán @@ar @@en @@la 110\n",
      "['azafrán', '@@ar', '@@en', '@@la']\n",
      "abocinado abocinar @@ado 100\n",
      "['abocinar', '@@ado']\n",
      "májalo majar @@á @@lo 100\n",
      "['majar', '@@á', '@@lo']\n",
      "troquelabais troquel @@ar @@ba @@ais 110\n",
      "['troquel', '@@ar', '@@ba', '@@ais']\n"
     ]
    }
   ],
   "source": [
    "for row in raw_data[:10]:\n",
    "    word, morphs,category = row.split(\"\\t\")\n",
    "    print(word,morphs,category)\n",
    "    print(morphs.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d59ad8b-242f-44cb-87fa-10644b8fd10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "LANGS = {\n",
    "    \"ces\": \"Czech\",\n",
    "    \"eng\": \"English\",\n",
    "    \"fra\": \"French\",\n",
    "    \"hun\": \"Hungarian\",\n",
    "    \"spa\": \"Spanish\",\n",
    "    \"ita\": \"Italian\",\n",
    "    \"lat\": \"Latin\",\n",
    "    \"rus\": \"Russian\",\n",
    "}\n",
    "CATEGORIES = {\n",
    "    \"100\": \"Inflection\",\n",
    "    \"010\": \"Derivation\",\n",
    "    \"101\": \"Inflection, Compound\",\n",
    "    \"000\": \"Root\",\n",
    "    \"011\": \"Derivation, Compound\",\n",
    "    \"110\": \"Inflection, Derivation\",\n",
    "    \"001\": \"Compound\",\n",
    "    \"111\": \"Inflection, Derivation, Compound\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58511055-0481-4b9b-81bc-ba2d074d7766",
   "metadata": {},
   "source": [
    "Estas funciones están diseñadas para ayudar en la obtención y procesamiento de archivos de datos TSV (valores separados por tabulaciones) desde una URL base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7367ad77-06d5-4290-a233-414746413000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera los nombres de archivo basados en el idioma y el track\n",
    "def get_files(lang: str, track: str = \"word\") -> list[str]:\n",
    "    \"\"\"Genera una lista de nombres de archivo basados en el idioma y el track\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    lang : str\n",
    "        Idioma para el cual se generarán los nombres de archivo.\n",
    "    track : str, optional\n",
    "        Track del shared task de donde vienen los datos (por defecto es \"word\").\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list[str]\n",
    "        Una lista de nombres de archivo generados para el idioma y el track especificados.\n",
    "    \"\"\"\n",
    "    base = f\"{lang}.{track}\"\n",
    "    return [\n",
    "        f\"{base}.dev.tsv\",\n",
    "        #f\"{base}.train.tsv\",\n",
    "        f\"{base}.test.gold.tsv\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b77300-91e0-406b-a6de-6113a941e8e7",
   "metadata": {},
   "source": [
    "`get_files(lang: str, track: str = \"word\") -> list[str]:`\n",
    "\n",
    "Esta función genera una lista de nombres de archivo basados en el idioma y el track especificados. \n",
    "\n",
    "Toma dos argumentos: lang (el idioma para el cual se generarán los nombres de archivo) y track (el track del shared task del cual provienen los datos, por defecto es \"word\").\n",
    "\n",
    "Devuelve una lista de nombres de archivo generados para el idioma y el track especificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5675631-15bc-4e7b-85c3-933c73f1b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga y concatena los datos de estos archivos.\n",
    "def get_raw_corpus(files: list) -> list:\n",
    "    \"\"\"Descarga y concatena los datos de los archivos tsv desde una URL base.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    files : list\n",
    "        Lista de nombres de archivos (sin extensión) que se descargarán\n",
    "        y concatenarán.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        Una lista que contiene los contenidos descargados y concatenados\n",
    "        de los archivos tsv.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for file in files:\n",
    "        print(f\"Downloading {file}.tsv\")\n",
    "        response = r.get(f\"https://raw.githubusercontent.com/sigmorphon/2022SegmentationST/main/data/{file}\")\n",
    "        response_list = response.text.split(\"\\n\")\n",
    "        result.extend(response_list[:-1]) # Last element is empty string ''\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215df804-48aa-4abd-96cc-f1bbd300fe5c",
   "metadata": {},
   "source": [
    "`get_raw_corpus(files: list) -> list:`\n",
    "\n",
    "Esta función descarga y concatena los datos de los archivos TSV desde una URL base. Toma una lista de nombres de archivos (sin extensión) como entrada y devuelve una lista que contiene los contenidos descargados y concatenados de los archivos TSV. Cada elemento de la lista de entrada se considera como un nombre base de archivo que se completa con \".dev.tsv\" y \".test.gold.tsv\" para descargar los archivos correspondientes. La función descarga los archivos, los divide en líneas usando \"\\n\" como delimitador y los agrega a la lista result. Luego, devuelve esta lista que contiene los contenidos descargados y concatenados de los archivos TSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55aefd71-5f71-440a-a400-1d14b625ffcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ita.word.dev.tsv.tsv\n",
      "Downloading ita.word.test.gold.tsv.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['relassato\\trelassare @@ato\\t100',\n",
       " 'trituro\\ttritura @@are @@o\\t110',\n",
       " 'piastrellavamo\\tpiastrellare @@avamo\\t100',\n",
       " 'riaffollato\\tri @@a @@folla @@are @@ato\\t110',\n",
       " 'collose\\tcolla @@oso @@e\\t110',\n",
       " 'elettroforo\\telettro @@foro\\t010',\n",
       " 'progettiamo\\tprogettare @@iamo\\t100',\n",
       " 'islamofobico\\tislamofobia @@ico\\t010',\n",
       " 'addogava\\taddogare @@ava\\t100',\n",
       " 'disfamasse\\tdis @@fame @@are @@asse\\t110']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_raw_corpus(get_files(lang=\"ita\"))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03a860e2-bcb0-4312-8116-b4fb6088643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toma una lista de datos de corpus y un idioma como entrada,\n",
    "# y devuelve un DataFrame de pandas que contiene los datos del corpus procesados.\n",
    "def raw_corpus_to_dataframe(corpus_list: list, lang: str) -> pd.DataFrame:\n",
    "    \"\"\"Convierte una lista de datos de corpus en un DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    corpus_list : list\n",
    "        Lista de líneas del corpus a convertir en DataFrame.\n",
    "    lang : str\n",
    "        Idioma al que pertenecen los datos del corpus.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Un DataFrame de pandas que contiene los datos del corpus procesados.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for row in corpus_list:\n",
    "        try:\n",
    "            word, morphs, category = row.split(\"\\t\")\n",
    "        except ValueError:\n",
    "            # Caso donde no hay categoria\n",
    "            word, morphs = row.split(\"\\t\")\n",
    "            category = \"N/A\"\n",
    "        morphs = morphs.split()\n",
    "        data.append({\"words\": word, \"morphs\": morphs, \"category\": category, \"lang\": lang})\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"word_len\"] = df[\"words\"].apply(lambda x: len(x))\n",
    "    df[\"morphs_count\"] = df[\"morphs\"].apply(lambda x: len(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc99c8-253d-4cbc-a4e4-f11a2ce84007",
   "metadata": {},
   "source": [
    "- Se inicializa una lista vacía data que se utilizará para almacenar los datos procesados del corpus.\n",
    "- Se itera sobre cada línea en la lista de datos del corpus (corpus_list). Cada línea contiene información sobre una palabra en el corpus.\n",
    "- Se intenta dividir la línea en tres partes utilizando el carácter de tabulación como delimitador. Si la línea no tiene tres partes, se asume que no hay una categoría y se maneja el caso donde no hay categoría.\n",
    "- Se divide la cadena de morfemas en una lista de morfemas utilizando el espacio como delimitador.\n",
    "- Se agrega un diccionario a la lista data para cada palabra, que contiene la palabra, los morfemas, la categoría (o \"N/A\" si no hay categoría) y el idioma al que pertenecen los datos del corpus.\n",
    "- Se crea un DataFrame de pandas a partir de la lista data.\n",
    "- Se agrega una columna al DataFrame que contiene la longitud de cada palabra (word_len).\n",
    "- Se agrega una columna al DataFrame que contiene el número de morfemas para cada palabra (morphs_count).\n",
    "- Finalmente, se devuelve el DataFrame df que contiene los datos del corpus procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adc64ce3-7677-4ef9-8e5e-fb224ec037cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading spa.word.dev.tsv.tsv\n",
      "Downloading spa.word.test.gold.tsv.tsv\n"
     ]
    }
   ],
   "source": [
    "# Descarga, procesa y almacena los datos de un corpus en español en un DataFrame de pandas\n",
    "# para su posterior análisis y procesamiento.\n",
    "files = get_files(\"spa\")\n",
    "raw_data = get_raw_corpus(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b8456-b4d4-4309-9473-e2e36309366c",
   "metadata": {},
   "source": [
    "1. Se utiliza la función get_files(\"spa\") para obtener una lista de nombres de archivos para el idioma español y el track \"word\". La función devuelve una lista que contiene los nombres de archivo generados para el idioma y el track especificados.\n",
    "\n",
    "2. Se utiliza la función get_raw_corpus(files) para descargar y concatenar los datos de los archivos TSV correspondientes a los nombres de archivo obtenidos anteriormente. Esta función descarga los datos desde una URL base y devuelve una lista que contiene los contenidos descargados y concatenados de los archivos TSV.\n",
    "\n",
    "3. Finalmente, se utiliza la función raw_corpus_to_dataframe(raw_data, lang=\"spa\") para convertir la lista de datos del corpus en un DataFrame de pandas. Esta función procesa los datos del corpus, dividiendo cada línea en palabras, morfemas y categoría (si está presente), y crea un DataFrame con estas columnas. Además, agrega columnas adicionales al DataFrame que contienen la longitud de cada palabra y el número de morfemas para cada palabra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29fab853-69e9-420a-ac3c-58e23051533a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>morphs</th>\n",
       "      <th>category</th>\n",
       "      <th>lang</th>\n",
       "      <th>word_len</th>\n",
       "      <th>morphs_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mimbreadlas</td>\n",
       "      <td>[mimbre, @@ar, @@ad, @@las]</td>\n",
       "      <td>110</td>\n",
       "      <td>spa</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>erupcionado</td>\n",
       "      <td>[erupción, @@ar, @@ado]</td>\n",
       "      <td>110</td>\n",
       "      <td>spa</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acobíjalo</td>\n",
       "      <td>[a, @@cobijar, @@á, @@lo]</td>\n",
       "      <td>110</td>\n",
       "      <td>spa</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>holonomías</td>\n",
       "      <td>[holonomía, @@s]</td>\n",
       "      <td>100</td>\n",
       "      <td>spa</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tricotemos</td>\n",
       "      <td>[tricotar, @@emos]</td>\n",
       "      <td>100</td>\n",
       "      <td>spa</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172168</th>\n",
       "      <td>confiáremos</td>\n",
       "      <td>[confiar, @@are, @@emos]</td>\n",
       "      <td>100</td>\n",
       "      <td>spa</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172169</th>\n",
       "      <td>granaba</td>\n",
       "      <td>[grano, @@ar, @@ba]</td>\n",
       "      <td>110</td>\n",
       "      <td>spa</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172170</th>\n",
       "      <td>granicen</td>\n",
       "      <td>[grano, @@izo, @@ar, @@en]</td>\n",
       "      <td>110</td>\n",
       "      <td>spa</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172171</th>\n",
       "      <td>cacheteémoslos</td>\n",
       "      <td>[cacha, @@ete, @@ear, @@emos, @@los]</td>\n",
       "      <td>110</td>\n",
       "      <td>spa</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172172</th>\n",
       "      <td>sumárieles</td>\n",
       "      <td>[sumario, @@ar, @@e, @@les]</td>\n",
       "      <td>110</td>\n",
       "      <td>spa</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172173 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 words                                morphs category lang  \\\n",
       "0          mimbreadlas           [mimbre, @@ar, @@ad, @@las]      110  spa   \n",
       "1          erupcionado               [erupción, @@ar, @@ado]      110  spa   \n",
       "2            acobíjalo             [a, @@cobijar, @@á, @@lo]      110  spa   \n",
       "3           holonomías                      [holonomía, @@s]      100  spa   \n",
       "4           tricotemos                    [tricotar, @@emos]      100  spa   \n",
       "...                ...                                   ...      ...  ...   \n",
       "172168     confiáremos              [confiar, @@are, @@emos]      100  spa   \n",
       "172169         granaba                   [grano, @@ar, @@ba]      110  spa   \n",
       "172170        granicen            [grano, @@izo, @@ar, @@en]      110  spa   \n",
       "172171  cacheteémoslos  [cacha, @@ete, @@ear, @@emos, @@los]      110  spa   \n",
       "172172      sumárieles           [sumario, @@ar, @@e, @@les]      110  spa   \n",
       "\n",
       "        word_len  morphs_count  \n",
       "0             11             4  \n",
       "1             11             3  \n",
       "2              9             4  \n",
       "3             10             2  \n",
       "4             10             2  \n",
       "...          ...           ...  \n",
       "172168        11             3  \n",
       "172169         7             3  \n",
       "172170         8             4  \n",
       "172171        14             5  \n",
       "172172        10             4  \n",
       "\n",
       "[172173 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = raw_corpus_to_dataframe(raw_data,lang=\"spa\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57129841-c5e9-4aa1-b499-893d87e229e3",
   "metadata": {},
   "source": [
    "**Analisis Cuantitativo para el Español**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fca1ab4c-b027-411c-872e-c86952032961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "100    84377\n",
       "110    78803\n",
       "010     5710\n",
       "000     3059\n",
       "101      118\n",
       "001       58\n",
       "111       36\n",
       "011       12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ba563ef-1462-49d1-a3d6-a149bbdd89ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.236227515347935"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"morphs_count\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d808df4f-75fa-4327-b818-75679d0eb684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.688301882408972"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"word_len\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a45045-be91-422c-95f5-d1d5e6127b0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae032dc5-71ce-4a9f-9775-114114521663",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"word_len\"], bins=10, edgecolor=\"black\")\n",
    "plt.xlabel(\"Word len\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
