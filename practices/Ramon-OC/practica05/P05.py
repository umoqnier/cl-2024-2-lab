# -*- coding: utf-8 -*-
"""P05.ipynb

Automatically generated by Colab.

# Práctica 5: Reducción de la dimensionalidad
"""

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE
from gensim.models import Word2Vec
import gensim.downloader as api

"""Primero debemos de entreanar el corpus de wikipedia, se guarda en un archivo el modelo 'model.bin'"""

wikipediaCorpus = api.load('text8')
model = Word2Vec(wikipediaCorpus)
model.save("model.bin")

model = Word2Vec.load("model.bin")

# frecuencia de palabras
word_freq = sorted([(word, model.wv.key_to_index[word]) for word in model.wv.index_to_key], key=lambda x: x[1], reverse = True)[:1000]
# 1000 palabras más frecuentes
top_word = [word for word, _ in word_freq]
# 1000 vectores de las palabras más frecuentes
top_vector = model.wv[top_word]

"""Aplicar los 3 algoritmos de reduccion de dimensionalidad

*   Reducir a 2d
*   Plotear 1000 vectores de las palabras más frecuentes
"""

def show_menu():
    print("\nMenú principal:")
    print("1. Reducción PCA")
    print("2. Reducción T-SNE")
    print("3. Reducción SVD")
    print("0. Salir")

def main():
    while True:
        show_menu()
        option = input("Seleccione una opción: ")

        if option not in ["0", "1", "2", "3"]:
            print("Opción inválida. Por favor, seleccione una opción válida.")
            continue
        if option == "1": # pca
            pca = PCA(n_components = 2).fit_transform(top_vector)
            plt.figure(figsize=(12, 8))
            plt.scatter(pca[:, 0], pca[:, 1], alpha=0.7, color='lightgreen', edgecolors='k')

            for i, word in enumerate(top_word):
                plt.annotate(word, xy=(pca[i, 0], pca[i, 1]), fontsize=10, fontweight='bold')

            plt.title('Principal component analysis', fontsize=14, fontweight='bold')
            plt.grid(True, linestyle='--', alpha=0.5)
            plt.tight_layout()
            plt.show()
        elif option == "2": #t-SNE

          tsne_result = TSNE(n_components=2, random_state=42).fit_transform(top_vector)

          plt.figure(figsize=(12, 8))
          plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.7, color='skyblue', edgecolors='k')

          for i, word in enumerate(top_word):
              plt.annotate(word, xy=(tsne_result[i, 0], tsne_result[i, 1]), fontsize=10, fontweight='bold')

          plt.title('Distributed Stochastic Neighbor Embedding (t-SNE) Plot of Top 1000 Words', fontsize=14, fontweight='bold')
          plt.xlabel('t-SNE Dimension 1', fontsize=12, fontweight='bold')
          plt.ylabel('t-SNE Dimension 2', fontsize=12, fontweight='bold')
          plt.grid(True, linestyle='--', alpha=0.5)
          plt.tight_layout()
          plt.show()
        elif option == "3": # SVD
          # Reducción de dimensionalidad utilizando SVD
          svd_result = TruncatedSVD(n_components=2).fit_transform(top_vector)
          plt.figure(figsize=(12, 8))
          plt.scatter(svd_result[:, 0], svd_result[:, 1], alpha=0.7, color='salmon', edgecolors='k')

          for i, word in enumerate(top_word):
              plt.annotate(word, xy=(svd_result[i, 0], svd_result[i, 1]), fontsize=10, fontweight='bold')

          plt.title('SVD', fontsize=14, fontweight='bold')
          plt.grid(True, linestyle='--', alpha=0.5)
          plt.tight_layout()
          plt.show()
        elif option == "0":
            print("¡Hasta luego!")
            break

if __name__ == "__main__":
    main()

"""
**¿Se guardan las relaciones semánticas? si o no y ¿porqué?**

Sí, se guarda la relación semántica. Al momento de gráficar se puede ver cómo la distribución de las palabras se ve afectada dependiendo del modelo que se solicite, pero estas siguen estándo asociadas.

**¿Qué método de reducción de dimensaionalidad consideras que es mejor?**

PCA y SVD no preservan las relaciones semánticas, ya que estos están más orientadis en la variabilidad y la estructura del modelo, mientras que t-SNE  puede conservar ciertas relaciones semánticas. El método a elegir depende de los objetivos específicos y características de los datos."""