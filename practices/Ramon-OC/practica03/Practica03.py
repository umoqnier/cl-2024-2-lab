# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11EMsd8kE2JJiJM534G4SKGH03drbi8pu
"""

!pip install nltk
!pip install six

import string
from collections import defaultdict

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from six.moves import urllib
import nltk
from nltk.corpus import stopwords, cess_esp
from nltk.probability import FreqDist

from wordcloud import WordCloud

"""## Comprobar si las stopwords que encontramos en paqueterias de NLP coinciden con las palabras más comúnes obtenidas en Zipf
    - Utilizar el corpus CREA
    - Realizar una nube de palabras usando las stopwords de paqueteria y las obtenidas através de Zipf
    - Responder las siguientes preguntas:
¿Obtenemos el mismo resultado? Si o no y ¿Porqué?
"""

# URL de CREA
url = "https://raw.githubusercontent.com/Ramon-OC/cl-2024-2-lab/practica03/practices/Ramon-OC/practica03/CREA_total.TXT"

# Abre la URL y lee el contenido del archivo
with urllib.request.urlopen(url) as response:
    lines = response.read().decode("latin-1").splitlines()

# Inicializa un diccionario para almacenar los datos
data = {"rank": [], "word": [], "frequency": [], "normalized_frequency": []}

lines = lines[1:]

# Procesa cada línea del archivo
for line in lines:
    stripped_line = line.strip()
    raw_rank, word, raw_frequency, raw_normalized_frequency = stripped_line.split()
    rank = int(float(raw_rank))
    frequency = int(raw_frequency.replace(",", ""))
    normalized_frequency = float(raw_normalized_frequency)
    data["rank"].append(rank)
    data["word"].append(word)
    data["frequency"].append(frequency)
    data["normalized_frequency"].append(normalized_frequency)

# Convierte los datos a un DataFrame de pandas
dataset = pd.DataFrame(data)
print(dataset)

nltk.download('stopwords')
nltk.download('cess_esp')

words = cess_esp.words()

stop_words = stopwords.words('spanish')

freq_dist = FreqDist(words)

stop_word_freq = {word: freq_dist[word] for word in stop_words[:20]}

word_cloud = WordCloud(width=500, height=500).generate_from_frequencies(stop_word_freq)

plt.figure(figsize=(10, 10))
plt.imshow(word_cloud, interpolation='bilinear')
plt.show()

more_frequent = dataset.sort_values(by='frequency', ascending=False).head(20)

word_freq = dict(zip(more_frequent['word'], more_frequent['frequency']))

wordcloud = WordCloud(width=500, height=500).generate_from_frequencies(word_freq)

plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.show()

"""Pregunta

Si bien hay una gran cantidad de elementos en la intersección de los dos conjuntos de palabras, no necesariamente se obtienen los mismos resultados. La diferencia está en que las stopwords de una paquetería son genéricas y la distribución de frecuencia no puede ser del todo precisa, por otra parte las stopwords obtenidas mediante Zipf están directamente relacionadas con la frecuencia de las palabras.

## Comprobar si Zipf se cumple para un lenguaje artificial creado por ustedes
    - Deberán darle un nombre a su lenguaje
    - Mostrar una oración de ejemplo
    - Pueden ser una secuencia de caracteres aleatorios
    - Tambien pueden definir el tamaño de las palabras de forma aleatoria
"""

"""
Primero se debe de crear el lengauje para construir así un corpus que nos permita
comprobar si Zipf se cumple para el nuevo lenguaje.
"""

own_language = "Lengualero"
longest_word = 10

diccionary = set() # En caso de que se repita

for i in range(1, 10): # La palabra más larga será de máximo 10 carácteres

    lexicon_at_size_i = []
    for _ in range(20):
        word = np.random.choice(list(string.ascii_lowercase), i)
        lexicon_at_size_i.append(''.join(word))

    diccionary.update(lexicon_at_size_i)

diccionary = list(diccionary)


# Para mostrar una oración ejemplo, necesitamos extraer n (sentence_length)
# palabras del diccionario de manera aleatoria.
sentence_length = 5

print("Mostrar una oración de ejemplo de 'Lengualero' de longitud ",sentence_length)
# Convert the list of words to a string
sample_sentence = ' '.join(np.random.choice(diccionary, sentence_length))

print(sample_sentence)

# Creación del corpus con valores predefinidos para la longitud de las frases y parrafos

def generate_random_sentence():
    no_words = np.random.randint(5, 10)
    return " ".join(np.random.choice(diccionary, no_words))

def generate_random_paragraph():
    no_sentences = np.random.randint(5, 10)
    return ", ".join([generate_random_sentence() for _ in range(no_sentences)])

def generate_random_document():
    no_paragraphs = np.random.randint(5, 10)
    return "\n\n".join([generate_random_paragraph() for _ in range(no_paragraphs)])

no_documents = 10;
corpus = [generate_random_document() for _ in range(no_documents)]

corpus

word_counts = defaultdict(int)

for doc in corpus:
    paragraphs = doc.split("\n\n")
    for para in paragraphs:
        sentences = para.split(",")
        for sentence in sentences:
            words = sentence.split(" ")
            for word in words:
                word_counts[word] += 1

num_unique_words = sum(1 for _ in word_counts.items())

print("Palabras distintas presentes en el corpus: ", num_unique_words)

keys = list(word_counts.keys())
values = list(word_counts.values())

plt.bar(keys, values)
plt.xlabel('Palabras')
plt.ylabel('Frecuencia')
plt.title('Frecuencia de Palabras')
plt.show()