# -*- coding: utf-8 -*-
"""P06.ipynb

Automatically generated by Colab.

"""

import nltk
import random
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.util import ngrams
from collections import Counter
from math import log
import requests

def train_test_split(text, test_size=0.3):
    sentences = sent_tokenize(text)
    random.shuffle(sentences)
    split_idx = int(len(sentences) * (1 - test_size))
    train_sentences = sentences[:split_idx]
    test_sentences = sentences[split_idx:]
    train_data = [word.lower() for sentence in train_sentences for word in word_tokenize(sentence)]
    test_data = [word.lower() for sentence in test_sentences for word in word_tokenize(sentence)]
    return train_data, test_data

def create_ngram_model(data, n):
    ngrams_data = ngrams(data, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')
    ngram_freq = Counter(ngrams_data)
    vocabulary_size = len(set(data)) + 2
    total_freq = sum(ngram_freq.values())
    model = {ngram: (freq + 1) / (total_freq + vocabulary_size) for ngram, freq in ngram_freq.items()}
    return model

def calculate_perplexity(model, test_data, n):
    test_ngrams = list(ngrams(test_data, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))
    log_prob_sum = 0
    backoff_prob = 1 / (sum(model.values()) + len(model))
    for ngram in test_ngrams:
        log_prob_sum += -log(model.get(ngram, backoff_prob))
    perplexity = pow(2, log_prob_sum / len(test_ngrams))
    return perplexity

url_corpus = "https://www.gutenberg.org/files/2000/2000-0.txt"
response = requests.get(url_corpus)
corpus_text = response.text

train_data, test_data = train_test_split(corpus_text)

bigram_model = create_ngram_model(train_data, 2)
trigram_model = create_ngram_model(train_data, 3)

print("Perplejidad bigramas:", calculate_perplexity(bigram_model, test_data, 2))
print("Perplejidad trigramas:", calculate_perplexity(trigram_model, test_data, 3))

# Análisis de resultados

# El modelo de bigramas se encuentra mejor evaluado frente al modelo de trigramas
# ya que cuenta con una perplejidad menor. Esto nos indica que el modelo de bigramas
# tiene una mayor capacidad de predecir palabras sobre un contexto específico. Este
# resultado se debe a que al usar bigramas se hace una estimación de relaciones más
# simples entre secuencias palabras, en este caso solo se necesita considerar la
# palabra anterior.