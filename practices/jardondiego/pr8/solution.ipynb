{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5tWUvZ7f4Nr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "from nltk import ngrams\n",
        "from collections import Counter, defaultdict"
      ],
      "metadata": {
        "id": "SJ0Hn9p7kbXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gist.githubusercontent.com/jardondiego/f91298aa14142b505e71b9ee46d21fcf/raw/f32234be7180be78d7a045f8bf2bbb70edbf0614/quijote.txt"
      ],
      "metadata": {
        "id": "AUlTYY4AlNcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(file):\n",
        "    \"\"\"\n",
        "    Read file and return a list of lines without empty lines\n",
        "    \"\"\"\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        no_empty_lines = [line.strip() for line in lines if line.strip()]\n",
        "        no_special_chars = [re.sub(r'[^\\w\\s]', '', line) for line in no_empty_lines]\n",
        "        lowercased = [line.lower() for line in no_special_chars]\n",
        "        tokenized = [ sentence.split() for sentence in lowercased ]\n",
        "    return tokenized\n",
        "\n",
        "dataset = get_data(\"quijote.txt\")\n",
        "print(dataset[:5])\n"
      ],
      "metadata": {
        "id": "9Zf0zRZZlA6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocesamiento"
      ],
      "metadata": {
        "id": "GQzMx3BNkgdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_corpus(corpus: list[str]) -> list[str]:\n",
        "    \"\"\"Función de preprocesamiento\n",
        "\n",
        "    Agrega tokens de inicio y fin, normaliza todo a minusculas\n",
        "    \"\"\"\n",
        "    preprocessed_corpus = []\n",
        "    for sent in corpus:\n",
        "        result = [word.lower() for word in sent]\n",
        "        # Al final de la oración\n",
        "        result.append(\"<EOS>\")\n",
        "        result.insert(0, \"<BOS>\")\n",
        "        preprocessed_corpus.append(result)\n",
        "    return preprocessed_corpus"
      ],
      "metadata": {
        "id": "yCTD_uvCkdmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_freqs(corpus: list[list[str]]):\n",
        "    words_freqs = {}\n",
        "    for sentence in corpus:\n",
        "        for word in sentence:\n",
        "            words_freqs[word] = words_freqs.get(word, 0) + 1\n",
        "    return words_freqs"
      ],
      "metadata": {
        "id": "Xh5B-1SNks3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UNK_LABEL = \"<UNK>\"\n",
        "def get_words_indexes(words_freqs: dict) -> dict:\n",
        "    result = {}\n",
        "    for idx, word in enumerate(words_freqs.keys()):\n",
        "        # Happax legomena happends\n",
        "        if words_freqs[word] == 1:\n",
        "            # Temp index for unknowns\n",
        "            result[UNK_LABEL] = len(words_freqs)\n",
        "        else:\n",
        "            result[word] = idx\n",
        "\n",
        "    return {word: idx for idx, word in enumerate(result.keys())}, {idx: word for idx, word in enumerate(result.keys())}"
      ],
      "metadata": {
        "id": "cvAfVr3spIxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = preprocess_corpus(get_data(\"quijote.txt\"))"
      ],
      "metadata": {
        "id": "b_mFJJCKo0Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "lkdXaTeasChe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_freqs = get_words_freqs(corpus)"
      ],
      "metadata": {
        "id": "Q6tFRiR6pYb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_freqs[\"the\"]"
      ],
      "metadata": {
        "id": "FU2OvRa0qrS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_freqs)"
      ],
      "metadata": {
        "id": "l6UNIB2Ndqg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for word, freq in words_freqs.items():\n",
        "    if freq == 1 and count <= 10:\n",
        "        print(word, freq)\n",
        "        count += 1"
      ],
      "metadata": {
        "id": "SwjAv9UBpeeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_indexes, index_to_word = get_words_indexes(words_freqs)"
      ],
      "metadata": {
        "id": "PPcDgLLSpinR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_indexes[\"the\"]"
      ],
      "metadata": {
        "id": "CweL7VBqpfkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word[16]"
      ],
      "metadata": {
        "id": "pYm6RWBQWcP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_indexes)"
      ],
      "metadata": {
        "id": "8_Qik6p5duon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(index_to_word)"
      ],
      "metadata": {
        "id": "3Ch0CnS5WgIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_id(words_indexes: dict, word: str) -> int:\n",
        "    unk_word_id = words_indexes[UNK_LABEL]\n",
        "    return words_indexes.get(word, unk_word_id)"
      ],
      "metadata": {
        "id": "wlESyaQhpvTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtenemos trigramas"
      ],
      "metadata": {
        "id": "gRvZpX4xTfHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertiremos los trigramas obtenidos a secuencias de idx, y preparamos el conjunto de entrenamiento $x$ y $y$\n",
        "\n",
        "- x: Contexto\n",
        "- y: Predicción de la siguiente palabra"
      ],
      "metadata": {
        "id": "gMl4BlMyTjqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test_data(corpus: list[list[str]], words_indexes: dict, n: int) -> tuple[list, list]:\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for sent in corpus:\n",
        "        n_grams = ngrams(sent, n)\n",
        "        for w1, w2, w3 in n_grams:\n",
        "            x_train.append([get_word_id(words_indexes, w1), get_word_id(words_indexes, w2)])\n",
        "            y_train.append([get_word_id(words_indexes, w3)])\n",
        "    return x_train, y_train"
      ],
      "metadata": {
        "id": "AldgYb33Tbrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cargamos bibliotecas\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time"
      ],
      "metadata": {
        "id": "S7W1bDPVahid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup de parametros\n",
        "EMBEDDING_DIM = 200\n",
        "CONTEXT_SIZE = 2\n",
        "BATCH_SIZE = 256\n",
        "H = 100\n",
        "torch.manual_seed(19)\n",
        "# Tamaño del Vocabulario\n",
        "V = len(words_indexes)"
      ],
      "metadata": {
        "id": "mrUUVaf6a1jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = get_train_test_data(corpus, words_indexes, n=3)"
      ],
      "metadata": {
        "id": "GhS0a_JEa5KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "# partimos los datos de entrada en batches\n",
        "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "L9x6cmTCbHUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigram Neural Network Model\n",
        "class TrigramModel(nn.Module):\n",
        "    \"\"\"Clase padre: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.linear2 = nn.Linear(h, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # x': concatenation of x1 and x2 embeddings   -->\n",
        "        #self.embeddings regresa un vector por cada uno de los índices que se les pase como entrada. view() les cambia el tamaño para concatenarlos\n",
        "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
        "        # h: tanh(W_1.x' + b)  -->\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        # W_2.h                 -->\n",
        "        out = self.linear2(out)\n",
        "        # log_softmax(W_2.h)      -->\n",
        "        # dim=1 para que opere sobre renglones, pues al usar batchs tenemos varios vectores de salida\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "IfPJ5G8Vbape"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento"
      ],
      "metadata": {
        "id": "am58pi4Qc2-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Pérdida. Negative log-likelihood loss\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "#Otras opciones de función de pérdida (tendrían que usar softmax sin log):\n",
        "#nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# 2. Instanciar el modelo\n",
        "model = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
        "\n",
        "# 3. Optimización. ADAM optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
        "\n",
        "#Otras opciones de optimizador:\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
        "# En la práctica sólo correremos una epoch por restricciones de recursos\n",
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    st = time.time()\n",
        "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch))\n",
        "    for it, data_tensor in enumerate(train_loader):\n",
        "        context_tensor = data_tensor[:,0:2]\n",
        "        target_tensor = data_tensor[:,2]\n",
        "\n",
        "        model.zero_grad() #reinicializar los gradientes\n",
        "        #FORWARD:\n",
        "        # get log probabilities over next words\n",
        "        log_probs = model(context_tensor)\n",
        "\n",
        "\n",
        "        # compute loss function\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        #BACKWARD:\n",
        "        # backward pass and update gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if it % 500 == 0:\n",
        "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Time taken (s): {}\".format(it, epoch, loss.item(), (time.time()-st)))\n",
        "            st = time.time()\n",
        "            #barch_size x len(vocab)\n",
        "\n",
        "    # saving model\n",
        "    model_path = 'model_{}.dat'.format(epoch)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved for epoch={epoch} at {model_path}\")"
      ],
      "metadata": {
        "id": "WyUpLJIFczKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(path: str) -> TrigramModel:\n",
        "    model_loaded = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
        "    model_loaded.load_state_dict(torch.load(path))\n",
        "    model_loaded.eval()\n",
        "    return model_loaded"
      ],
      "metadata": {
        "id": "MbJ548WBYf_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"drive/MyDrive/LM_neuronal/model_0.dat\""
      ],
      "metadata": {
        "id": "ojEVrN5QS32s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(PATH)\n",
        "W1 = \"<BOS>\"\n",
        "W2 = \"mi\"\n",
        "\n",
        "IDX1 = get_word_id(words_indexes, W1)\n",
        "IDX2 = get_word_id(words_indexes, W2)\n",
        "\n",
        "#Obtenemos Log probabidades p(W3|W2,W1)\n",
        "probs = model(torch.tensor([[IDX1,  IDX2]])).detach().tolist()"
      ],
      "metadata": {
        "id": "LGGQzd8N0G8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(probs[0])"
      ],
      "metadata": {
        "id": "kv_IfrNtf_6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos diccionario con {idx: logprob}\n",
        "model_probs = {}\n",
        "for idx, p in enumerate(probs[0]):\n",
        "  model_probs[idx] = p\n",
        "\n",
        "# Sort:\n",
        "model_probs_sorted = sorted(((prob, idx) for idx, prob in model_probs.items()), reverse=True)\n",
        "\n",
        "# Printing word  and prob (retrieving the idx):\n",
        "topcandidates = 0\n",
        "for prob, idx in model_probs_sorted:\n",
        "  #Retrieve the word associated with that idx\n",
        "  word = index_to_word[idx]\n",
        "  print(idx, word, prob)\n",
        "\n",
        "  topcandidates += 1\n",
        "\n",
        "  if topcandidates > 100:\n",
        "    break"
      ],
      "metadata": {
        "id": "dGh39h041Ip8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word.get(model_probs_sorted[0][0])"
      ],
      "metadata": {
        "id": "y2hsMj_1Tak6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generacion de lenguaje"
      ],
      "metadata": {
        "id": "j3PKcCznXCBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_likely_words(model: TrigramModel, context: str, words_indexes: dict, index_to_word: dict, top_count: int=10) -> list[tuple]:\n",
        "    model_probs = {}\n",
        "    words = context.split()\n",
        "    idx_word_1 = get_word_id(words_indexes, words[0])\n",
        "    idx_word_2 = get_word_id(words_indexes, words[1])\n",
        "    probs = model(torch.tensor([[idx_word_1, idx_word_2]])).detach().tolist()\n",
        "\n",
        "    for idx, p in enumerate(probs[0]):\n",
        "        model_probs[idx] = p\n",
        "\n",
        "    # Strategy: Sort and get top-K words to generate text\n",
        "    return sorted(((prob, index_to_word[idx]) for idx, prob in model_probs.items()), reverse=True)[:top_count]"
      ],
      "metadata": {
        "id": "_-xkJK_cNYbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"esto es\"\n",
        "get_likely_words(model, sentence, words_indexes, index_to_word, 3)"
      ],
      "metadata": {
        "id": "iw0njIDfRRBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "def get_next_word(words: list[tuple[float, str]]) -> str:\n",
        "    # From a top-K list of words get a random word\n",
        "    return words[randint(0, len(words)-1)][1]"
      ],
      "metadata": {
        "id": "Ex44jU5mMkPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_next_word(get_likely_words(model, sentence, words_indexes, index_to_word))"
      ],
      "metadata": {
        "id": "i1ujC9j9TzE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 30\n",
        "TOP_COUNT = 10\n",
        "def generate_text(model: TrigramModel, history: str, words_indexes: dict, index_to_word: dict, tokens_count: int=0) -> None:\n",
        "    next_word = get_next_word(get_likely_words(model, history, words_indexes, index_to_word, top_count=TOP_COUNT))\n",
        "    print(next_word, end=\" \")\n",
        "    tokens_count += 1\n",
        "    if tokens_count == MAX_TOKENS or next_word == \"<EOS>\":\n",
        "        return\n",
        "    generate_text(model, history.split()[1]+ \" \" + next_word, words_indexes, index_to_word, tokens_count)"
      ],
      "metadata": {
        "id": "z6eW24WDUS_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"<BOS> el\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text(model, sentence, words_indexes, index_to_word)"
      ],
      "metadata": {
        "id": "Zzc9lFqAaFJE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}