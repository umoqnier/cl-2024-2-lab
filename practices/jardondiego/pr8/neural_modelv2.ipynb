{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5y4Ug7xDUqP"
      },
      "source": [
        "### Práctica 8. Modelos de lenguaje neuronales\n",
        "\n",
        "#### Actividades\n",
        "\n",
        "- Construir un modelo del lenguaje neuronal a partir de un corpus en español\n",
        "    - Corpus: El Quijote. URL: https://www.gutenberg.org/ebooks/2000\n",
        "        - NOTA: Considera los recursos de computo. Recuerda que en la practica utilizamos ~50k oraciones\n",
        "    - Modelo de trigramas con n = 3\n",
        "    - Incluye informacion sobre setup de entrenamiento:\n",
        "        - Dimension de embeddings\n",
        "        - Dimsension de capa oculta\n",
        "        - Cantidad de oraciones para entrenamiento\n",
        "        - Batch size y context size\n",
        "    - Incluye la liga de drive de tu modelo\n",
        "\n",
        "- Imprima en pantalla un tres ejemplos de generacion de texto\n",
        "    - Proponga mejoras en las estrategias de generación de texto vistas en la práctica\n",
        "    - Decriba en que consiste la estrategia propuesta\n",
        "    - Compare la estrategia de la práctica y su propuesta\n",
        "\n",
        "- Visualizar en 2D los vectores de las palabras más comunes (excluir STOP WORDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B91_P2IKDUqS",
        "outputId": "73af5de2-b987-4bae-f167-3c87a42d3287"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Install the required packages\n",
        "\"\"\"\n",
        "\n",
        "!pip install nltk numpy torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RBFs_dzDUqU",
        "outputId": "ee9b0b9f-99bf-4dc8-e645-042881b4aedd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "from nltk import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "nltk.download(\"reuters\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "def preprocess_corpus(corpus: list[str]) -> list[str]:\n",
        "    \"\"\"Función de preprocesamiento\n",
        "\n",
        "    Agrega tokens de inicio y fin, normaliza todo a minusculas\n",
        "    \"\"\"\n",
        "    preprocessed_corpus = []\n",
        "    for sent in corpus:\n",
        "        result = [word.lower() for word in sent]\n",
        "        result.append(\"<EOS>\")\n",
        "        result.insert(0, \"<BOS>\")\n",
        "        preprocessed_corpus.append(result)\n",
        "    return preprocessed_corpus\n",
        "\n",
        "def get_words_freqs(corpus: list[list[str]]):\n",
        "    words_freqs = {}\n",
        "    for sentence in corpus:\n",
        "        for word in sentence:\n",
        "            words_freqs[word] = words_freqs.get(word, 0) + 1\n",
        "    return words_freqs\n",
        "\n",
        "UNK_LABEL = \"<UNK>\"\n",
        "def get_words_indexes(words_freqs: dict) -> dict:\n",
        "    result = {}\n",
        "    for idx, word in enumerate(words_freqs.keys()):\n",
        "        if words_freqs[word] == 1:\n",
        "            result[UNK_LABEL] = len(words_freqs)\n",
        "        else:\n",
        "            result[word] = idx\n",
        "\n",
        "    return {word: idx for idx, word in enumerate(result.keys())}, {idx: word for idx, word in enumerate(result.keys())}\n",
        "\n",
        "corpus = preprocess_corpus(reuters.sents())\n",
        "words_freqs = get_words_freqs(corpus)\n",
        "\n",
        "print(len(corpus))\n",
        "print(words_freqs[\"the\"])\n",
        "print(len(words_freqs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count = 0\n",
        "for word, freq in words_freqs.items():\n",
        "    if freq == 1 and count <= 10:\n",
        "        print(word, freq)\n",
        "        count += 1\n",
        "\n",
        "words_indexes, index_to_word = get_words_indexes(words_freqs)\n",
        "words_indexes[\"the\"]\n",
        "index_to_word[16]\n",
        "len(words_indexes)\n",
        "len(index_to_word)\n",
        "\n",
        "\n",
        "def get_word_id(words_indexes: dict, word: str) -> int:\n",
        "    unk_word_id = words_indexes[UNK_LABEL]\n",
        "    return words_indexes.get(word, unk_word_id)\n",
        "\n",
        "\n",
        "def get_train_test_data(\n",
        "    corpus: list[list[str]], words_indexes: dict, n: int\n",
        ") -> tuple[list, list]:\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for sent in corpus:\n",
        "        n_grams = ngrams(sent, n)\n",
        "        for w1, w2, w3 in n_grams:\n",
        "            x_train.append(\n",
        "                [get_word_id(words_indexes, w1), get_word_id(words_indexes, w2)]\n",
        "            )\n",
        "            y_train.append([get_word_id(words_indexes, w3)])\n",
        "    return x_train, y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkq4lovtDUqV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "EMBEDDING_DIM = 200\n",
        "CONTEXT_SIZE = 2\n",
        "BATCH_SIZE = 256\n",
        "H = 100\n",
        "torch.manual_seed(19)\n",
        "V = len(words_indexes)\n",
        "\n",
        "x_train, y_train = get_train_test_data(corpus, words_indexes, n=3)\n",
        "\n",
        "train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoXaXF07DUqV",
        "outputId": "166a5f41-1aaa-41d4-c166-bb8b182a1635"
      },
      "outputs": [],
      "source": [
        "class TrigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.linear2 = nn.Linear(h, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #self.embeddings regresa un vector por cada uno de los índices que se les pase como entrada. view() les cambia el tamaño para concatenarlos\n",
        "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "# Otras opciones de función de pérdida (tendrían que usar softmax sin log):\n",
        "# nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "model = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
        "\n",
        "# Otras opciones de optimizador:\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    st = time.time()\n",
        "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch))\n",
        "    for it, data_tensor in enumerate(train_loader):\n",
        "        context_tensor = data_tensor[:, 0:2]\n",
        "        target_tensor = data_tensor[:, 2]\n",
        "\n",
        "        model.zero_grad()  # reinicializar los gradientes\n",
        "        # FORWARD:\n",
        "        log_probs = model(context_tensor)\n",
        "\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        # BACKWARD:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if it % 500 == 0:\n",
        "            print(\n",
        "                \"Training Iteration {} of epoch {} complete. Loss: {}; Time taken (s): {}\".format(\n",
        "                    it, epoch, loss.item(), (time.time() - st)\n",
        "                )\n",
        "            )\n",
        "            st = time.time()\n",
        "            # barch_size x len(vocab)\n",
        "\n",
        "    model_path = \"model_{}.dat\".format(epoch)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved for epoch={epoch} at {model_path}\")\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(path: str) -> TrigramModel:\n",
        "    model_loaded = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
        "    model_loaded.load_state_dict(torch.load(path))\n",
        "    model_loaded.eval()\n",
        "    return model_loaded\n",
        "\n",
        "\n",
        "PATH = \"drive/MyDrive/LM_neuronal/model_0.dat\"\n",
        "\n",
        "model = get_model(PATH)\n",
        "W1 = \"<BOS>\"\n",
        "W2 = \"my\"\n",
        "\n",
        "IDX1 = get_word_id(words_indexes, W1)\n",
        "IDX2 = get_word_id(words_indexes, W2)\n",
        "\n",
        "# Obtenemos Log probabidades p(W3|W2,W1)\n",
        "probs = model(torch.tensor([[IDX1, IDX2]])).detach().tolist()\n",
        "\n",
        "len(probs[0])\n",
        "\n",
        "model_probs = {}\n",
        "for idx, p in enumerate(probs[0]):\n",
        "    model_probs[idx] = p\n",
        "\n",
        "model_probs_sorted = sorted(\n",
        "    ((prob, idx) for idx, prob in model_probs.items()), reverse=True\n",
        ")\n",
        "\n",
        "topcandidates = 0\n",
        "for prob, idx in model_probs_sorted:\n",
        "    # Retrieve the word associated with that idx\n",
        "    word = index_to_word[idx]\n",
        "    print(idx, word, prob)\n",
        "\n",
        "    topcandidates += 1\n",
        "\n",
        "    if topcandidates > 100:\n",
        "        break\n",
        "\n",
        "index_to_word.get(model_probs_sorted[0][0])\n",
        "\n",
        "\n",
        "def get_likely_words(\n",
        "    model: TrigramModel,\n",
        "    context: str,\n",
        "    words_indexes: dict,\n",
        "    index_to_word: dict,\n",
        "    top_count: int = 10,\n",
        ") -> list[tuple]:\n",
        "    model_probs = {}\n",
        "    words = context.split()\n",
        "    idx_word_1 = get_word_id(words_indexes, words[0])\n",
        "    idx_word_2 = get_word_id(words_indexes, words[1])\n",
        "    probs = model(torch.tensor([[idx_word_1, idx_word_2]])).detach().tolist()\n",
        "\n",
        "    for idx, p in enumerate(probs[0]):\n",
        "        model_probs[idx] = p\n",
        "\n",
        "    return sorted(\n",
        "        ((prob, index_to_word[idx]) for idx, prob in model_probs.items()), reverse=True\n",
        "    )[:top_count]\n",
        "\n",
        "\n",
        "sentence = \"this is\"\n",
        "get_likely_words(model, sentence, words_indexes, index_to_word, 3)\n",
        "\n",
        "from random import randint\n",
        "\n",
        "\n",
        "def get_next_word(words: list[tuple[float, str]]) -> str:\n",
        "    return words[randint(0, len(words) - 1)][1]\n",
        "\n",
        "\n",
        "get_next_word(get_likely_words(model, sentence, words_indexes, index_to_word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQxxtSqxDUqW"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 30\n",
        "TOP_COUNT = 10\n",
        "\n",
        "\n",
        "def generate_text(\n",
        "    model: TrigramModel,\n",
        "    history: str,\n",
        "    words_indexes: dict,\n",
        "    index_to_word: dict,\n",
        "    tokens_count: int = 0,\n",
        ") -> None:\n",
        "    next_word = get_next_word(\n",
        "        get_likely_words(\n",
        "            model, history, words_indexes, index_to_word, top_count=TOP_COUNT\n",
        "        )\n",
        "    )\n",
        "    print(next_word, end=\" \")\n",
        "    tokens_count += 1\n",
        "    if tokens_count == MAX_TOKENS or next_word == \"<EOS>\":\n",
        "        return\n",
        "    generate_text(\n",
        "        model,\n",
        "        history.split()[1] + \" \" + next_word,\n",
        "        words_indexes,\n",
        "        index_to_word,\n",
        "        tokens_count,\n",
        "    )\n",
        "\n",
        "\n",
        "sentence = \"<BOS> the\"\n",
        "print(sentence, end=\" \")\n",
        "generate_text(model, sentence, words_indexes, index_to_word)\n",
        "\n",
        "\n",
        "word = input(\">> \")\n",
        "words_tensor = torch.LongTensor([get_word_id(words_indexes, word)])\n",
        "word_embed = model.embeddings(words_tensor)\n",
        "print(f\"embbeding (dim={len(word_embed[0])}) vec for word={word}\")\n",
        "word_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InuIkbDIDUqX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "words = [\"the\", \"gold\", \"car\", \"nation\"]\n",
        "for word in words:\n",
        "    base_word_embed = (\n",
        "        model.embeddings(torch.LongTensor([get_word_id(words_indexes, word)]))\n",
        "        .detach()\n",
        "        .numpy()\n",
        "    )\n",
        "\n",
        "    word_sims = {}\n",
        "    for other_word in words_indexes.keys():\n",
        "        if word == other_word:\n",
        "            continue\n",
        "        other_word_embed = (\n",
        "            model.embeddings(torch.LongTensor([get_word_id(words_indexes, other_word)]))\n",
        "            .detach()\n",
        "            .numpy()\n",
        "        )\n",
        "        word_sims[other_word] = cosine_similarity(base_word_embed, other_word_embed)\n",
        "\n",
        "    print(\"\\nBASE WORD =\", word)\n",
        "    for word, sim in sorted(word_sims.items(), key=lambda item: item[1], reverse=True)[\n",
        "        :10\n",
        "    ]:\n",
        "        print(f\"{word}: {sim[0][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loss_function = nn.NLLLoss()\n",
        "model = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
        "\n",
        "EPOCHS = 3\n",
        "for epoch in range(EPOCHS):\n",
        "    st = time.time()\n",
        "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch))\n",
        "    for it, data_tensor in enumerate(train_loader):\n",
        "        context_tensor = data_tensor[:,0:2].to(device)\n",
        "        target_tensor = data_tensor[:,2].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        log_probs = model(context_tensor)\n",
        "\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if it % 500 == 0:\n",
        "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Time taken (s): {}\".format(it, epoch, loss.item(), (time.time()-st)))\n",
        "            st = time.time()\n",
        "\n",
        "    model_path = 'drive/MyDrive/LM_neuronal/model_gpu_{}.dat'.format(epoch)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved for epoch={epoch} at {model_path}\")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
