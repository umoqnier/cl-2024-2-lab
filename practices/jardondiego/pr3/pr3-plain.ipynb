{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 3\n",
    "\n",
    "### Propiedades estadísticas del lenguaje\n",
    "\n",
    "## Objetives\n",
    "\n",
    "1. Comprobar si las *stopwords* que encontramos en paqueterias de *NLP* coinciden con las palabras más comúnes obtenidas en Zipf\n",
    "2. Comprobar si Zipf se cumple para un lenguaje artificial creado por ustedes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install wordcloud\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1\n",
    "\n",
    "### Comprobación de Zipf para el español\n",
    "\n",
    "1. Comprobar si las *stopwords* que encontramos en paqueterias de *NLP* coinciden con las palabras más comúnes obtenidas en Zipf\n",
    "    - Utilizar el [corpus CREA](https://corpus.rae.es/frec/CREA_total.zip)\n",
    "    - Realizar una nube de palabras usando las stopwords de paqueteria y las obtenidas através de Zipf\n",
    "    - Responder las siguientes preguntas:\n",
    "        - ¿Obtenemos el mismo resultado? Si o no y ¿Porqué?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# (rank, word, frequency, normalized_frequency)\n",
    "data = {\"rank\": [], \"word\": [], \"frequency\": [], \"normalized_frequency\": []}\n",
    "\n",
    "FILENAME = \"crea.txt\"\n",
    "file = open(FILENAME, \"r\", encoding=\"latin-1\")\n",
    "\n",
    "# skip header line\n",
    "file.readline()\n",
    "\n",
    "for line in file:\n",
    "    striped_line = line.strip()\n",
    "    raw_rank, word, raw_frequency, raw_normalized_frequency = striped_line.split()\n",
    "    rank = int(float(raw_rank))\n",
    "    frequency = int(raw_frequency.replace(\",\", \"\"))\n",
    "    normalized_frequency = float(raw_normalized_frequency)\n",
    "    data[\"rank\"].append(rank)\n",
    "    data[\"word\"].append(word)\n",
    "    data[\"frequency\"].append(frequency)\n",
    "    data[\"normalized_frequency\"].append(normalized_frequency)\n",
    "\n",
    "# close file\n",
    "file.close()\n",
    "\n",
    "# convert data to pandas DataFrame\n",
    "dataset = pd.DataFrame(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "verificar que las k = 10 primeras palabras\n",
    "sean stop words\n",
    "comparar con el español de nltk\n",
    "\"\"\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# print(\"k = 10 primeras palabras\")\n",
    "# print([word for rank, word, *_ in dataset if rank < 10])\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(f\"stop words en español de nltk \\n{stopwords.words('spanish')[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "top_10_words = dataset.sort_values(by='frequency', ascending=False).head(10)\n",
    "word_freq = dict(zip(top_10_words['word'], top_10_words['frequency']))\n",
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import cess_esp, stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('cess_esp')\n",
    "\n",
    "words = cess_esp.words()\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# Create a frequency distribution of the words in the corpus\n",
    "freq_dist = FreqDist(words)\n",
    "\n",
    "# Get the frequency of each stopword in the corpus\n",
    "stopword_freq = {word: freq_dist[word] for word in spanish_stopwords[:10]}\n",
    "\n",
    "# Create a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(stopword_freq)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import cess_esp, stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('cess_esp')\n",
    "\n",
    "words = cess_esp.words()\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# Create a frequency distribution of the words in the corpus\n",
    "freq_dist = FreqDist(words)\n",
    "\n",
    "# Get the frequency of each stopword in the corpus\n",
    "stopword_freq = {word: freq_dist[word] for word in spanish_stopwords[:10]}\n",
    "\n",
    "# Create a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(stopword_freq)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2\n",
    "\n",
    "### Zipf para un lenguaje artificial\n",
    "\n",
    "2. Comprobar si Zipf se cumple para un lenguaje artificial creado por ustedes\n",
    "    - Deberán darle un nombre a su lenguaje\n",
    "    - Mostrar una oración de ejemplo\n",
    "    - Pueden ser una secuencia de caracteres aleatorios\n",
    "    - Tambien pueden definir el tamaño de las palabras de forma aleatoria\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\"\"\"\n",
    "alternativa uno\n",
    "para crear un lenguaje\n",
    "vamos a utilizar un enfoque probabilístico\n",
    "\n",
    "a partir de los distintos 26 caracteres de ascii que son letras\n",
    "aleatoriamente vamos a generar k cadenas de longitud 1, 2, ..., n\n",
    "con k = 25 y n = 16\n",
    "\"\"\"\n",
    "\n",
    "# random + language = ranguage\n",
    "LANGUAGUE_NAME = \"Ranguage\"\n",
    "MAX_FREQUENCY = 25\n",
    "MAX_WORD_LENGTH = 16\n",
    "\n",
    "lexicon = set()\n",
    "\n",
    "for i in range(MAX_WORD_LENGTH + 1):\n",
    "    # we won't generate words of length 0 or 1\n",
    "    if i == 0 or i == 1: continue\n",
    "\n",
    "    lexicon_at_size_i = []\n",
    "    for _ in range(MAX_FREQUENCY):\n",
    "        word = np.random.choice(list(string.ascii_lowercase), i)\n",
    "        lexicon_at_size_i.append(''.join(word))\n",
    "\n",
    "    lexicon.update(lexicon_at_size_i)\n",
    "\n",
    "lexicon = list(lexicon)\n",
    "\n",
    "# print first 10 words\n",
    "print(\"randomly generated words in the lexicon\")\n",
    "print(lexicon[:10])\n",
    "print()\n",
    "\n",
    "# randomly pick 6-10 words from the lexicon to generate a sentence\n",
    "# generate 5 sentences\n",
    "\n",
    "print(\"randomly generated sentences\")\n",
    "sentences = [\n",
    "    \" \".join(np.random.choice(lexicon, np.random.randint(6, 11))) for _ in range(5)\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generar aleatoriamente un corpus de texto del lenguaje que hemos creado.\n",
    "una concatenación de párrafos de longitud aleatoria\n",
    "\n",
    "un corpus va a ser un conjunto de párrafos, y cada párrafo va a ser un conjunto de oraciones\n",
    "\"\"\"\n",
    "\n",
    "def generate_random_sentence():\n",
    "    amount = np.random.randint(6, 11)\n",
    "    return \" \".join(np.random.choice(lexicon, amount))\n",
    "\n",
    "def generate_random_paragraph():\n",
    "    amount = np.random.randint(3, 7)\n",
    "    return \", \".join([generate_random_sentence() for _ in range(amount)])\n",
    "\n",
    "def generate_random_document():\n",
    "    amount = np.random.randint(10, 25)\n",
    "    return \"\\n\\n\".join([generate_random_paragraph() for _ in range(amount)])\n",
    "\n",
    "def generate_corpus():\n",
    "    amount = np.random.randint(3, 10)\n",
    "    return [generate_random_document() for _ in range(amount)]\n",
    "\n",
    "corpus = generate_corpus()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\"\"\"\n",
    "encontrar palabras únicas del corpus\n",
    "obtener la frecuencia de cada palabra\n",
    "\"\"\"\n",
    "\n",
    "word_to_frequency = defaultdict(int)\n",
    "for document in corpus:\n",
    "    for paragraph in document.split(\"\\n\\n\"):\n",
    "        for sentence in paragraph.split(\",\"):\n",
    "            for word in sentence.split(\" \"):\n",
    "                word_to_frequency[word] += 1\n",
    "\n",
    "lexicon = [ key for key in word_to_frequency.items() ]\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "graficar palabra por frecuencia\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(len(lexicon)), [ value for key, value in word_to_frequency.items() ])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
