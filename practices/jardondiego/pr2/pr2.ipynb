{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 2\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Implementar modelo etiquetador POS para el idioma otomí.  \n",
    "A partir de una oración en otomí, obtener una secuencia de etiquetas.\n",
    "Utilizar un modelo CRF.\n",
    "\n",
    "- Definir feature functions\n",
    "- Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (1.4.1.post1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: sklearn_crfsuite in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (0.3.6)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from sklearn_crfsuite) (0.9.10)\n",
      "Requirement already satisfied: six in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from sklearn_crfsuite) (1.16.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from sklearn_crfsuite) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from sklearn_crfsuite) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from tqdm>=2.0->sklearn_crfsuite) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "configurar dependencias\n",
    "\"\"\"\n",
    "\n",
    "%pip install scikit-learn\n",
    "%pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had 1 errors\n",
      "[('bale', 'obl'), ('treintaisinko', 'cnj'), ('peso', 'obl')]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\"\"\"\n",
    "preprocesar conjunto de datos\n",
    "partir los datos en dos conjuntos\n",
    "uno para entrenar y otro para probar\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parse_raw_line(raw_string: str):\n",
    "    \"\"\"\n",
    "    parse a line of the file\n",
    "    \"\"\"\n",
    "    sample_as_list = json.loads(raw_string)\n",
    "    parsed_elements = []\n",
    "\n",
    "    for i in range(len(sample_as_list)):\n",
    "        element = sample_as_list[i]\n",
    "        *components, tag = element\n",
    "        word = \"\".join([ component[0] for component in components ])\n",
    "        parsed_elements.append((word, tag))\n",
    "\n",
    "    return parsed_elements\n",
    "\n",
    "def parse_list(phrase):\n",
    "    \"\"\"\n",
    "    parse a pharase as a list of elements\n",
    "    [ *components, tag ]\n",
    "\n",
    "    where components is a list of [ word, tag ]\n",
    "    and tag is a string\n",
    "    \"\"\"\n",
    "    parsed_elements = []\n",
    "\n",
    "    for i in range(len(phrase)):\n",
    "        element = phrase[i]\n",
    "        *components, tag = element\n",
    "        word = \"\".join([ component[0] for component in components ])\n",
    "        parsed_elements.append((word, tag))\n",
    "\n",
    "    return parsed_elements\n",
    "\n",
    "def parse_file_into_dataset():\n",
    "    \"\"\"\n",
    "    parse a file into a dataset\n",
    "    of the form\n",
    "\n",
    "    [ element ]\n",
    "    where element is a list of [ word, tag ]\n",
    "    \"\"\"\n",
    "    FILENAME = \"corpus_otomi.txt\"\n",
    "    file = open(FILENAME, \"r\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "\n",
    "    lines = data.split(\"\\n\")\n",
    "    lines = [line.strip() for line in lines]\n",
    "\n",
    "    # parsed_data = [parse_raw_line(line) for line in lines]\n",
    "    parsed_data = []\n",
    "    for line in lines:\n",
    "        error_count = 0\n",
    "        try:\n",
    "            parsed_data.append(parse_raw_line(line))\n",
    "        except:\n",
    "            error_count += 1\n",
    "\n",
    "    print(\"had {} errors\".format(error_count))\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "def detect_encoding_issues(s):\n",
    "    try:\n",
    "        s.encode(\"ascii\")\n",
    "    except UnicodeEncodeError:\n",
    "        # The string contains non-ASCII characters, which could be fine.\n",
    "        # Further checks for common misencoding patterns could go here.\n",
    "        if \"Ã\" in s or \"�\" in s:\n",
    "            return True  # Found common misencoding indicators.\n",
    "        # Check for high ordinal values that might indicate misencoding.\n",
    "        if any(ord(c) > 127 for c in s):\n",
    "            return True  # Found characters outside the standard ASCII range.\n",
    "    return False\n",
    "\n",
    "def phrase_as_single_string(phrase):\n",
    "    \"\"\"\n",
    "    convert a phrase into a single string\n",
    "    \"\"\"\n",
    "    return \" \".join([word for word, tag in phrase])\n",
    "\n",
    "\n",
    "def get_mock_dataset():\n",
    "    \"\"\"\n",
    "    generate a simple dataset for initial testing\n",
    "    \"\"\"\n",
    "\n",
    "    sample1 = [\n",
    "        [[\"bi\", \"3.cpl\"], [\"'u̱n\", \"stem\"], [\"gí\", \"1.obj\"], \"v\"],\n",
    "        [[\"yi̱\", \"det.pl\"], \"det\"],\n",
    "        [[\"mbu̱hí\", \"stem\"], \"obl\"],\n",
    "        [[\"nge\", \"stem\"], \"cnj\"],\n",
    "        [[\"hín\", \"stem\"], \"neg\"],\n",
    "        [[\"dí\", \"1.icp\"], [\"má\", \"ctrf\"], [\"né\", \"stem\"], \"v\"],\n",
    "        [[\"gwa\", \"1.icp.irr\"], [\"porá\", \"stem\"], \"v\"],\n",
    "        [[\"nge\", \"stem\"], \"cnj\"],\n",
    "        [[\"dí\", \"1.icp\"], [\"má\", \"ctrf\"], [\"dáhní\", \"stem\"], \"v\"],\n",
    "    ]\n",
    "\n",
    "    sample2 = [\n",
    "        [[\"bo\", \"3.cpl\"], [\"pihkí\", \"stem\"], \"v\"],\n",
    "        [[\"yi̱\", \"det.pl\"], \"det\"],\n",
    "        [[\"k'iñá\", \"stem\"], \"obl\"],\n",
    "    ]\n",
    "\n",
    "    samples = [sample1, sample2]\n",
    "    return [parse_list(sample) for sample in samples]\n",
    "\n",
    "mock_dataset = get_mock_dataset()\n",
    "dataset = parse_file_into_dataset()\n",
    "\n",
    "# filter out non-ascii sequences\n",
    "dataset = [sample for sample in dataset if all(ord(c) < 128 for c in sample[0][0])]\n",
    "dataset = [ sample for sample in dataset if not detect_encoding_issues(phrase_as_single_string(sample)) ]\n",
    "\n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('dayoti', 'v'), ('chichi', 'unkwn')],\n",
       " [('xo', 'cnj'), (\"dazo'wi\", 'v')],\n",
       " [('pe', 'obl'), ('ya', 'cnj'), ('limpio', 'obl')],\n",
       " [('desayuno', 'cnj'), ('xayi', 'obl'), ('ni', 'obl'), ('merienda', 'obl')],\n",
       " [('ya', 'cnj'), ('mbizo', 'v')],\n",
       " [('ya', 'cnj'), ('hin', 'neg'), (\"gohot'i\", 'v')],\n",
       " [('limpio', 'obl'), ('limpio', 'obl')],\n",
       " [('ximo', 'v')],\n",
       " [('pe', 'obl'),\n",
       "  ('xo', 'cnj'),\n",
       "  ('kasi', 'cnj'),\n",
       "  ('medio', 'obl'),\n",
       "  ('mastiyadowi', 'unkwn')]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "construir conjunto de entrenamiento y prueba\n",
    "para ello vamos a usar la función train_test_split\n",
    "de scikit-learn que nos permite dividir un conjunto\n",
    "de datos en dos conjuntos, uno para entrenar y otro para pruebas\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(sentence, word, position):\n",
    "    \"\"\"\n",
    "    construye el conjunto de funciones de características\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"bias\": 1.0,\n",
    "        \"word.lower()\": word.lower(),\n",
    "        \"word[-3:]\": word[-3:],  # Suffix\n",
    "        \"word[-2:]\": word[-2:],  # Suffix\n",
    "        \"word.isupper()\": word.isupper(),\n",
    "        \"word.istitle()\": word.istitle(),\n",
    "        \"word.isdigit()\": word.isdigit(),\n",
    "    }\n",
    "\n",
    "    if position > 0:\n",
    "        word1 = sentence[position - 1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if position < len(sentence)-1:\n",
    "        word1 = sentence[position+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_phrase_features(phrase):\n",
    "    \"\"\"\n",
    "    obtiene las características de una muestra del conjunto de datos\n",
    "    \"\"\"\n",
    "    return [get_word_features(phrase, word, i) for i, (word, tag) in enumerate(phrase)]\n",
    "\n",
    "def get_labels(phrase):\n",
    "    \"\"\"\n",
    "    obtiene las etiquetas de una muestra del conjunto de datos\n",
    "    \"\"\"\n",
    "    return [tag for _, tag in phrase]\n",
    "\n",
    "def get_tokens(phrase):\n",
    "    \"\"\"\n",
    "    obtiene las palabras de una muestra del conjunto de datos\n",
    "    \"\"\"\n",
    "    return [word for word, _ in phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "entrenar el modelo\n",
    "utilizando linear-chain CRF\n",
    "\"\"\"\n",
    "\n",
    "# preparar dataset de entrenamiento\n",
    "\n",
    "X_train = [get_phrase_features(phrase) for phrase in train]\n",
    "y_train = [get_labels(phrase) for phrase in train]\n",
    "\n",
    "# preparar dataset de pruebas\n",
    "X_test = [get_phrase_features(phrase) for phrase in test]\n",
    "y_test = [get_labels(phrase) for phrase in test]\n",
    "\n",
    "import sklearn_crfsuite\n",
    "\n",
    "# crear el modelo\n",
    "model = sklearn_crfsuite.CRF()\n",
    "\n",
    "# entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# hacer predicciones a partir del conjunto de pruebas\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nevaluación del modelo\\nutilizar una matriz de confusión\\napoyado en la función classification_report de scikit-learn\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "evaluación del modelo\n",
    "utilizar una matriz de confusión\n",
    "apoyado en la función classification_report de scikit-learn\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencia\n",
    "\n",
    "To implement a simple POS tagger using linear-chain Conditional Random Fields (CRFs) with `sklearn-crfsuite` (an extension for scikit-learn designed specifically for CRF), you will need to follow these steps. Note that `sklearn-crfsuite` is a convenient wrapper around the `python-crfsuite` library, which is specifically designed for CRF models and is well-suited for tasks like POS tagging.\n",
    "\n",
    "First, make sure you have `sklearn-crfsuite` installed. If not, you can install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install sklearn-crfsuite\n",
    "```\n",
    "\n",
    "Let's assume you have a dataset for Spanish POS tagging. The dataset should be a list of sentences, where each sentence is a list of `(word, tag)` tuples. For the sake of an example, let's define a very small dataset:\n",
    "\n",
    "```python\n",
    "sentences = [\n",
    "    [(\"Todos\", \"DET\"), (\"los\", \"DET\"), (\"hombres\", \"NOUN\"), (\"deben\", \"VERB\"), (\"morir\", \"VERB\"), (\",\", \"PUNCT\"), (\"Jon\", \"PROPN\"), (\"Nieve\", \"PROPN\"), (\".\", \"PUNCT\")],\n",
    "    [(\"¿Quién\", \"PRON\"), (\"es\", \"VERB\"), (\"John\", \"PROPN\"), (\"Galt\", \"PROPN\"), (\"?\", \"PUNCT\")]\n",
    "]\n",
    "```\n",
    "\n",
    "Next, define feature extraction functions. Feature extraction is crucial for CRFs, as it determines the information the model can use to make predictions:\n",
    "\n",
    "```python\n",
    "def word2features(sent, i):\n",
    "    \"\"\"Extract features for a given word in a sentence.\"\"\"\n",
    "    word = sent[i][0]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],  # Suffix\n",
    "        'word[-2:]': word[-2:],  # Suffix\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of Sentence\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of Sentence\n",
    "    \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    \"\"\"Extract features for all words in a sentence.\"\"\"\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    \"\"\"Extract labels for all words in a sentence.\"\"\"\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    \"\"\"Extract tokens for all words in a sentence.\"\"\"\n",
    "    return [token for token, label in sent]\n",
    "```\n",
    "\n",
    "Now, prepare the dataset for training:\n",
    "\n",
    "```python\n",
    "X_train = [sent2features(s) for s in sentences]\n",
    "y_train = [sent2labels(s) for s in sentences]\n",
    "```\n",
    "\n",
    "Train the CRF model:\n",
    "\n",
    "```python\n",
    "import sklearn_crfsuite\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "After training, you can use the trained model to predict POS tags for new sentences:\n",
    "\n",
    "```python\n",
    "test_sentence = [(\"Este\", \"DET\"), (\"es\", \"VERB\"), (\"un\", \"DET\"), (\"ejemplo\", \"NOUN\")]\n",
    "X_test = [sent2features(test_sentence)]\n",
    "y_pred = crf.predict(X_test)\n",
    "print(\"Predicted:\", y_pred)\n",
    "```\n",
    "\n",
    "This simple example demonstrates the basic process of using `sklearn-crfsuite` for POS tagging with a CRF model. For real-world applications, you would need a much larger dataset and more sophisticated feature engineering to achieve high accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
