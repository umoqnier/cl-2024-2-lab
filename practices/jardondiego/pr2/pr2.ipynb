{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 2\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Implementar modelo etiquetador POS para el idioma otomí.  \n",
    "A partir de una oración en otomí, obtener una secuencia de etiquetas.\n",
    "Utilizar un modelo CRF.\n",
    "\n",
    "- Definir feature functions\n",
    "- Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sklearn_crfsuite in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from sklearn_crfsuite) (0.9.10)\n",
      "Requirement already satisfied: six in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from sklearn_crfsuite) (1.16.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from sklearn_crfsuite) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from sklearn_crfsuite) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\diego\\downloads\\pr2-nlp\\.venv\\lib\\site-packages (from tqdm>=2.0->sklearn_crfsuite) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "configurar dependencias\n",
    "\"\"\"\n",
    "\n",
    "%pip install scikit-learn\n",
    "%pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\"\"\"\n",
    "preprocesar conjunto de datos\n",
    "partir los datos en dos conjuntos\n",
    "uno para entrenar y otro para probar\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parse_raw_line(raw_string: str):\n",
    "    \"\"\"\n",
    "    parse a line of the file\n",
    "    \"\"\"\n",
    "    sample_as_list = json.loads(raw_string)\n",
    "    parsed_elements = []\n",
    "\n",
    "    for i in range(len(sample_as_list)):\n",
    "        element = sample_as_list[i]\n",
    "        *components, tag = element\n",
    "        word = \"\".join([ component[0] for component in components ])\n",
    "        parsed_elements.append((word, tag))\n",
    "\n",
    "    return parsed_elements\n",
    "\n",
    "def parse_list(phrase):\n",
    "    \"\"\"\n",
    "    parse a pharase as a list of elements\n",
    "    [ *components, tag ]\n",
    "\n",
    "    where components is a list of [ word, tag ]\n",
    "    and tag is a string\n",
    "    \"\"\"\n",
    "    parsed_elements = []\n",
    "\n",
    "    for i in range(len(phrase)):\n",
    "        element = phrase[i]\n",
    "        *components, tag = element\n",
    "        word = \"\".join([ component[0] for component in components ])\n",
    "        parsed_elements.append((word, tag))\n",
    "\n",
    "    return parsed_elements\n",
    "\n",
    "def parse_file_into_dataset():\n",
    "    \"\"\"\n",
    "    parse a file into a dataset\n",
    "    of the form\n",
    "\n",
    "    [ element ]\n",
    "    where element is a list of [ word, tag ]\n",
    "    \"\"\"\n",
    "    FILENAME = \"corpus_otomi.txt\"\n",
    "    file = open(FILENAME, \"r\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "\n",
    "    data = data.split(\"\\n\")\n",
    "    parsed_data = [parse_raw_line(line) for line in data]\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "def get_mock_dataset():\n",
    "    \"\"\"\n",
    "    generate a simple dataset for initial testing\n",
    "    \"\"\"\n",
    "\n",
    "    sample1 = [\n",
    "        [[\"bi\", \"3.cpl\"], [\"'u̱n\", \"stem\"], [\"gí\", \"1.obj\"], \"v\"],\n",
    "        [[\"yi̱\", \"det.pl\"], \"det\"],\n",
    "        [[\"mbu̱hí\", \"stem\"], \"obl\"],\n",
    "        [[\"nge\", \"stem\"], \"cnj\"],\n",
    "        [[\"hín\", \"stem\"], \"neg\"],\n",
    "        [[\"dí\", \"1.icp\"], [\"má\", \"ctrf\"], [\"né\", \"stem\"], \"v\"],\n",
    "        [[\"gwa\", \"1.icp.irr\"], [\"porá\", \"stem\"], \"v\"],\n",
    "        [[\"nge\", \"stem\"], \"cnj\"],\n",
    "        [[\"dí\", \"1.icp\"], [\"má\", \"ctrf\"], [\"dáhní\", \"stem\"], \"v\"],\n",
    "    ]\n",
    "\n",
    "    sample2 = [\n",
    "        [[\"bo\", \"3.cpl\"], [\"pihkí\", \"stem\"], \"v\"],\n",
    "        [[\"yi̱\", \"det.pl\"], \"det\"],\n",
    "        [[\"k'iñá\", \"stem\"], \"obl\"],\n",
    "    ]\n",
    "\n",
    "    samples = [sample1, sample2]\n",
    "    return [parse_list(sample) for sample in samples]\n",
    "\n",
    "dataset = get_mock_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(\"bi'u̱ngí\", 'v'),\n",
       "  ('yi̱', 'det'),\n",
       "  ('mbu̱hí', 'obl'),\n",
       "  ('nge', 'cnj'),\n",
       "  ('hín', 'neg'),\n",
       "  ('dímáné', 'v'),\n",
       "  ('gwaporá', 'v'),\n",
       "  ('nge', 'cnj'),\n",
       "  ('dímádáhní', 'v')]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "construir conjunto de entrenamiento y prueba\n",
    "para ello vamos a usar la función train_test_split\n",
    "de scikit-learn que nos permite dividir un conjunto\n",
    "de datos en dos conjuntos, uno para entrenar y otro para pruebas\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(sentence, word, position):\n",
    "    \"\"\"\n",
    "    construye el conjunto de funciones de características\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"bias\": 1.0,\n",
    "        \"word.lower()\": word.lower(),\n",
    "        \"word[-3:]\": word[-3:],  # Suffix\n",
    "        \"word[-2:]\": word[-2:],  # Suffix\n",
    "        \"word.isupper()\": word.isupper(),\n",
    "        \"word.istitle()\": word.istitle(),\n",
    "        \"word.isdigit()\": word.isdigit(),\n",
    "    }\n",
    "\n",
    "    if position > 0:\n",
    "        word1 = sentence[position - 1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if position < len(sentence)-1:\n",
    "        word1 = sentence[position+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_phrase_features(phrase):\n",
    "    \"\"\"\n",
    "    obtiene las características de una muestra del conjunto de datos\n",
    "    \"\"\"\n",
    "    return [get_word_features(phrase, word, i) for i, (word, tag) in enumerate(phrase)]\n",
    "\n",
    "def get_labels(phrase):\n",
    "    \"\"\"\n",
    "    obtiene las etiquetas de una muestra del conjunto de datos\n",
    "    \"\"\"\n",
    "    return [tag for _, tag in phrase]\n",
    "\n",
    "def get_tokens(phrase):\n",
    "    \"\"\"\n",
    "    obtiene las palabras de una muestra del conjunto de datos\n",
    "    \"\"\"\n",
    "    return [word for word, _ in phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "entrenar el modelo\n",
    "utilizando linear-chain CRF\n",
    "\"\"\"\n",
    "\n",
    "# preparar dataset de entrenamiento\n",
    "\n",
    "X_train = [get_phrase_features(phrase) for phrase in train]\n",
    "y_train = [get_labels(phrase) for phrase in train]\n",
    "\n",
    "# preparar dataset de pruebas\n",
    "X_test = [get_phrase_features(phrase) for phrase in test]\n",
    "y_test = [get_labels(phrase) for phrase in test]\n",
    "\n",
    "import sklearn_crfsuite\n",
    "\n",
    "# crear el modelo\n",
    "model = sklearn_crfsuite.CRF()\n",
    "\n",
    "# entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# hacer predicciones a partir del conjunto de pruebas\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mevaluación del modelo\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mutilizar una matriz de confusión\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mapoyado en la función classification_report de scikit-learn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# X_test = [ [ word for word, tag in phrase ] for phrase in test ]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# y_test = [ [ tag for word, tag in phrase ] for phrase in test ]\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# y_pred = model.predict(X_test)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# print(classification_report(y_test, y_pred))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diego\\Downloads\\pr2-nlp\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\diego\\Downloads\\pr2-nlp\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2604\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   2470\u001b[0m     {\n\u001b[0;32m   2471\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2495\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2496\u001b[0m ):\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2498\u001b[0m \n\u001b[0;32m   2499\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2602\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2604\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2606\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2607\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\diego\\Downloads\\pr2-nlp\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:105\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    108\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n",
      "\u001b[1;31mValueError\u001b[0m: multiclass-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "evaluación del modelo\n",
    "utilizar una matriz de confusión\n",
    "apoyado en la función classification_report de scikit-learn\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencia\n",
    "\n",
    "To implement a simple POS tagger using linear-chain Conditional Random Fields (CRFs) with `sklearn-crfsuite` (an extension for scikit-learn designed specifically for CRF), you will need to follow these steps. Note that `sklearn-crfsuite` is a convenient wrapper around the `python-crfsuite` library, which is specifically designed for CRF models and is well-suited for tasks like POS tagging.\n",
    "\n",
    "First, make sure you have `sklearn-crfsuite` installed. If not, you can install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install sklearn-crfsuite\n",
    "```\n",
    "\n",
    "Let's assume you have a dataset for Spanish POS tagging. The dataset should be a list of sentences, where each sentence is a list of `(word, tag)` tuples. For the sake of an example, let's define a very small dataset:\n",
    "\n",
    "```python\n",
    "sentences = [\n",
    "    [(\"Todos\", \"DET\"), (\"los\", \"DET\"), (\"hombres\", \"NOUN\"), (\"deben\", \"VERB\"), (\"morir\", \"VERB\"), (\",\", \"PUNCT\"), (\"Jon\", \"PROPN\"), (\"Nieve\", \"PROPN\"), (\".\", \"PUNCT\")],\n",
    "    [(\"¿Quién\", \"PRON\"), (\"es\", \"VERB\"), (\"John\", \"PROPN\"), (\"Galt\", \"PROPN\"), (\"?\", \"PUNCT\")]\n",
    "]\n",
    "```\n",
    "\n",
    "Next, define feature extraction functions. Feature extraction is crucial for CRFs, as it determines the information the model can use to make predictions:\n",
    "\n",
    "```python\n",
    "def word2features(sent, i):\n",
    "    \"\"\"Extract features for a given word in a sentence.\"\"\"\n",
    "    word = sent[i][0]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],  # Suffix\n",
    "        'word[-2:]': word[-2:],  # Suffix\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of Sentence\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of Sentence\n",
    "    \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    \"\"\"Extract features for all words in a sentence.\"\"\"\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    \"\"\"Extract labels for all words in a sentence.\"\"\"\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    \"\"\"Extract tokens for all words in a sentence.\"\"\"\n",
    "    return [token for token, label in sent]\n",
    "```\n",
    "\n",
    "Now, prepare the dataset for training:\n",
    "\n",
    "```python\n",
    "X_train = [sent2features(s) for s in sentences]\n",
    "y_train = [sent2labels(s) for s in sentences]\n",
    "```\n",
    "\n",
    "Train the CRF model:\n",
    "\n",
    "```python\n",
    "import sklearn_crfsuite\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "After training, you can use the trained model to predict POS tags for new sentences:\n",
    "\n",
    "```python\n",
    "test_sentence = [(\"Este\", \"DET\"), (\"es\", \"VERB\"), (\"un\", \"DET\"), (\"ejemplo\", \"NOUN\")]\n",
    "X_test = [sent2features(test_sentence)]\n",
    "y_pred = crf.predict(X_test)\n",
    "print(\"Predicted:\", y_pred)\n",
    "```\n",
    "\n",
    "This simple example demonstrates the basic process of using `sklearn-crfsuite` for POS tagging with a CRF model. For real-world applications, you would need a much larger dataset and more sophisticated feature engineering to achieve high accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
