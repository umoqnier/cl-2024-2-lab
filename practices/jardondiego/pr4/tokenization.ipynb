{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h3xj-AYwbid"
      },
      "source": [
        "### Práctica 4. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxGbpc34wbim",
        "outputId": "9271e086-0f6d-4f71-f6d2-c5f1e479068a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting elotl\n",
            "  Downloading elotl-0.0.1.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from elotl) (6.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from elotl) (0.18.3)\n",
            "Collecting mock (from subword-nmt)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (4.66.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Installing collected packages: mock, elotl, subword-nmt\n",
            "Successfully installed elotl-0.0.1.16 mock-5.1.0 subword-nmt-0.3.8\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Install the necessary packages\n",
        "\"\"\"\n",
        "\n",
        "!pip install elotl subword-nmt nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "metadata": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-r6joXEwbin",
        "outputId": "767c7a9a-8817-40f5-8fde-26afec51d685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Make the necessary imports\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import math\n",
        "import elotl.corpus\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "axolotl_corpus = elotl.corpus.load(\"axolotl\")\n",
        "brown_complete = [word for word in brown.words() if re.match(\"\\w\", word)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "metadata": {},
        "id": "cY8d3v7dwbin"
      },
      "outputs": [],
      "source": [
        "def calculate_entropy(corpus: list[str]) -> float:\n",
        "    # Counter object to hold the count of each word in the corpus\n",
        "    word_counts = Counter(corpus)\n",
        "\n",
        "    # Total number of words in the corpus\n",
        "    total_word_count = len(corpus)\n",
        "\n",
        "    # Dictionary to hold the probability of each word\n",
        "    # The probability is calculated as the count of the word divided by the total number of words\n",
        "    word_probabilities = {word: count / total_word_count for word, count in word_counts.items()}\n",
        "\n",
        "    # Calculate the entropy of the corpus\n",
        "    # The entropy is the sum of the product of the probability of each word and the log base 2 of the probability\n",
        "    entropy = -sum(probability * math.log2(probability) for probability in word_probabilities.values())\n",
        "\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "metadata": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLRCLmFGwbin",
        "outputId": "074711f4-be58-4fd0-ac40-f56c9d4ec5af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating entropy for the Brown corpus with word-level tokenization...\n",
            "Entropy of the Brown corpus with word-level tokenization is:  10.638580562908293\n",
            "Calculating entropy for the Axolotl corpus with word-level tokenization...\n",
            "Entropy of the Axolotl corpus with word-level tokenization is:  11.840929856284687\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Entropía con tokenización word-level\n",
        "\"\"\"\n",
        "\n",
        "brown_corpus = brown_complete[:100000]\n",
        "h_brown_wl = calculate_entropy(brown_corpus)\n",
        "print(\"Calculating entropy for the Brown corpus with word-level tokenization...\")\n",
        "print(\"Entropy of the Brown corpus with word-level tokenization is: \", h_brown_wl)\n",
        "\n",
        "axolotl_words = [word for row in axolotl_corpus for word in row[1].lower().split()]\n",
        "h_axolotl_wl = calculate_entropy(axolotl_words)\n",
        "print(\"Calculating entropy for the Axolotl corpus with word-level tokenization...\")\n",
        "print(\"Entropy of the Axolotl corpus with word-level tokenization is: \", h_axolotl_wl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "metadata": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aPp9cqfwbin",
        "outputId": "94f44692-e383-4ea2-ca58-249d934b2055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set written to file.\n",
            "100% 500/500 [00:06<00:00, 80.23it/s] \n",
            "BPE model learned from training set.\n",
            "Testing set written to file.\n",
            "BPE model applied to testing set.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Entropía con tokenización BPE\n",
        "\"\"\"\n",
        "\n",
        "DATA_PATH = \"./\"\n",
        "\n",
        "def write_text_to_file(raw_text: str, file_name: str) -> None:\n",
        "    \"\"\"\n",
        "    This function writes a given text to a file.\n",
        "\n",
        "    Parameters:\n",
        "    raw_text (str): The text to be written to the file.\n",
        "    file_name (str): The name of the file.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Open the file in write mode. If the file does not exist, it will be created.\n",
        "    with open(f\"{file_name}.txt\", \"w\", encoding='utf-8') as file:\n",
        "        # Write the raw text to the file\n",
        "        file.write(raw_text)\n",
        "\n",
        "# Calculate the number of rows to use for training from the Axolotl corpus\n",
        "axolotl_corpus_train_rows_count = len(axolotl_words) - round(len(axolotl_words)*.30)\n",
        "\n",
        "# Split the Axolotl corpus into training and testing sets\n",
        "axolotl_corpus_train = axolotl_words[:axolotl_corpus_train_rows_count]\n",
        "axolotl_corpus_test = axolotl_words[axolotl_corpus_train_rows_count:]\n",
        "\n",
        "# Write the training set to a file\n",
        "write_text_to_file(\" \".join(axolotl_corpus_train), DATA_PATH + \"axolotl_plain\")\n",
        "print(\"Training set written to file.\")\n",
        "\n",
        "# Learn the BPE model from the training set\n",
        "!subword-nmt learn-bpe -s 500 < axolotl_plain.txt > axolotl.model\n",
        "print(\"BPE model learned from training set.\")\n",
        "\n",
        "# Write the testing set to a file\n",
        "write_text_to_file(\" \".join(axolotl_corpus_test), DATA_PATH + \"axolotl_plain_test\")\n",
        "print(\"Testing set written to file.\")\n",
        "\n",
        "# Apply the BPE model to the testing set\n",
        "!subword-nmt apply-bpe -c axolotl.model < axolotl_plain_test.txt > axolotl_tokenized.txt\n",
        "print(\"BPE model applied to testing set.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRaFi0R_wbio",
        "outputId": "38a06074-f1e2-4c94-aea0-86b8f1f69a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 20 most common tokens in the Axolotl corpus are:\n",
            "yn: 5454\n",
            "in: 4807\n",
            "i@@: 4035\n",
            "qui@@: 3206\n",
            "tla@@: 3144\n",
            "a@@: 2990\n",
            "ti@@: 2838\n",
            "o@@: 2656\n",
            ".: 2642\n",
            "mo@@: 2642\n",
            "te@@: 2473\n",
            "ca@@: 2453\n",
            ",: 2288\n",
            "to@@: 2274\n",
            "y@@: 2052\n",
            "ma@@: 1962\n",
            "l@@: 1907\n",
            "ca: 1828\n",
            "no@@: 1798\n",
            "ne@@: 1764\n",
            "The entropy of the Axolotl corpus with BPE tokenization is:  8.35031490948691\n",
            "100% 500/500 [00:01<00:00, 376.36it/s]\n"
          ]
        }
      ],
      "source": [
        "# Open the file containing the tokenized Axolotl corpus\n",
        "with open(DATA_PATH + \"axolotl_tokenized.txt\", encoding='utf-8') as file:\n",
        "    # Read the file and split the text into tokens\n",
        "    axolotl_corpus_tokens = file.read().split()\n",
        "\n",
        "# Count the occurrences of each token in the Axolotl corpus\n",
        "axolotl_corpus_token_counts = Counter(axolotl_corpus_tokens)\n",
        "\n",
        "# Print the 20 most common tokens in the Axolotl corpus\n",
        "print(\"The 20 most common tokens in the Axolotl corpus are:\")\n",
        "for token, count in axolotl_corpus_token_counts.most_common(20):\n",
        "    print(f\"{token}: {count}\")\n",
        "\n",
        "# Calculate the entropy of the Axolotl corpus\n",
        "axolotl_corpus_entropy = calculate_entropy(axolotl_corpus_tokens)\n",
        "\n",
        "# Print the entropy of the Axolotl corpus\n",
        "print(\"The entropy of the Axolotl corpus with BPE tokenization is: \", axolotl_corpus_entropy)\n",
        "\n",
        "# Calculate the number of rows to use for training from the Brown corpus\n",
        "brown_corpus_train_rows_count = len(brown_corpus) - round(len(brown_corpus)*.30)\n",
        "\n",
        "# Split the Brown corpus into training and testing sets\n",
        "brown_corpus_train = brown_corpus[:brown_corpus_train_rows_count]\n",
        "brown_corpus_test = brown_corpus[brown_corpus_train_rows_count:]\n",
        "\n",
        "# Write the training set to a file\n",
        "write_text_to_file(\" \".join(brown_corpus_train), DATA_PATH + \"brown_plain\")\n",
        "\n",
        "# Learn the BPE model from the training set\n",
        "!subword-nmt learn-bpe -s 500 < brown_plain.txt > brown.model\n",
        "\n",
        "# Write the testing set to a file\n",
        "write_text_to_file(\" \".join(brown_corpus_test), DATA_PATH + \"brown_plain_test\")\n",
        "\n",
        "# Apply the BPE model to the testing set\n",
        "!subword-nmt apply-bpe -c brown.model < brown_plain_test.txt > brown_tokenized.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdohv3Egwbio",
        "outputId": "f8f8df9b-bb14-45cf-c469-bcf90f9c81ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 20 most common tokens in the Brown corpus are:\n",
            "the: 1975\n",
            "of: 1007\n",
            "g@@: 860\n",
            "s@@: 851\n",
            "a: 833\n",
            "t@@: 820\n",
            "to: 810\n",
            "ed: 798\n",
            "l@@: 782\n",
            "m@@: 779\n",
            "and: 741\n",
            "i@@: 722\n",
            "c@@: 719\n",
            "in: 710\n",
            "p@@: 699\n",
            "f@@: 663\n",
            "d@@: 648\n",
            "b@@: 615\n",
            "ing: 610\n",
            "re@@: 594\n",
            "The entropy of the Brown corpus with BPE tokenization is:  8.35414045252538\n"
          ]
        }
      ],
      "source": [
        "# Open the file containing the tokenized Brown corpus\n",
        "with open(DATA_PATH + \"brown_tokenized.txt\", encoding='utf-8') as file:\n",
        "    # Read the file and split the text into tokens\n",
        "    brown_corpus_tokens = file.read().split()\n",
        "\n",
        "# Count the occurrences of each token in the Brown corpus\n",
        "brown_corpus_token_counts = Counter(brown_corpus_tokens)\n",
        "\n",
        "# Print the 20 most common tokens in the Brown corpus\n",
        "print(\"The 20 most common tokens in the Brown corpus are:\")\n",
        "for token, count in brown_corpus_token_counts.most_common(20):\n",
        "    print(f\"{token}: {count}\")\n",
        "\n",
        "# Calculate the entropy of the Brown corpus\n",
        "brown_corpus_entropy = calculate_entropy(brown_corpus_tokens)\n",
        "\n",
        "# Print the entropy of the Brown corpus\n",
        "print(\"The entropy of the Brown corpus with BPE tokenization is: \", brown_corpus_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "metadata": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOg2OH2rwbip",
        "outputId": "0728cd7d-c82f-4af2-f3d3-634ad6a4b410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy values for different corpora and tokenization methods:\n",
            "\n",
            "Corpus Brown:\n",
            "With word-level tokenization:\n",
            "10.638580562908293\n",
            "\n",
            "With Byte Pair Encoding (BPE):\n",
            "8.35414045252538\n",
            "\n",
            "Corpus Axolotl:\n",
            "With word-level tokenization:\n",
            "11.840929856284687\n",
            "\n",
            "With Byte Pair Encoding (BPE):\n",
            "8.35031490948691\n"
          ]
        }
      ],
      "source": [
        "print(\"Entropy values for different corpora and tokenization methods:\\n\")\n",
        "\n",
        "print(\"Corpus Brown:\")\n",
        "print(\"With word-level tokenization:\")\n",
        "print(h_brown_wl)\n",
        "\n",
        "print(\"\\nWith Byte Pair Encoding (BPE):\")\n",
        "print(brown_corpus_entropy)\n",
        "print()\n",
        "\n",
        "print(\"Corpus Axolotl:\")\n",
        "print(\"With word-level tokenization:\")\n",
        "print(h_axolotl_wl)\n",
        "\n",
        "print(\"\\nWith Byte Pair Encoding (BPE):\")\n",
        "print(axolotl_corpus_entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDjkKqqOwbip"
      },
      "source": [
        "### Preguntas\n",
        "\n",
        "**¿Aumentó o disminuyó la entropía para los corpus?**  \n",
        "En los dos casos, i.e. en ambos corpus, observamos una disminución significativa en la entropía al aplicar la tokenización con BPE (Byte Pair Encoding), en comparación con la tokenización a nivel de palabra. Esta reducción fue particularmente notable en el corpus de axolotl, donde la entropía disminuyó de 11.84 a 8.35. Esto indica que el proceso de tokenización con BPE consigue simplificar la estructura del texto al reducir la variedad de tokens necesarios para representarlo.\n",
        "\n",
        "**¿Qué significa que la entropía aumente o disminuya en un texto?**  \n",
        "La entropía en un texto se refiere a la medida de incertidumbre o impredecibilidad asociada con el lenguaje utilizado. Un aumento en la entropía indica que el texto tiene un vocabulario más amplio y estructuras más complejas, lo que aumenta la impredecibilidad. Esto puede ser deseable desde un punto de vista literario o lingüístico, ya que refleja riqueza y diversidad en el uso del lenguaje. Sin embargo, para la computación y el procesamiento de lenguaje natural, un alto nivel de entropía puede representar un desafío, ya que la variedad y la complejidad del lenguaje complican la interpretación y el análisis automáticos del texto.\n",
        "\n",
        "**¿Cómo influye la tokenización en la entropía de un texto?**  \n",
        "La tokenización es un proceso crucial en el análisis de texto que consiste en dividir el texto en unidades más pequeñas, conocidas como tokens. Este proceso puede influir considerablemente en la entropía de un texto. Al aplicar métodos de tokenización como el BPE, se simplifica el vocabulario del texto y se estandarizan las formas de las palabras, lo que generalmente resulta en una reducción de la entropía. Esta disminución facilita la tarea de procesamiento de textos, ya que un menor nivel de entropía implica menos impredecibilidad y una estructura más uniforme, lo que es beneficioso para algoritmos de procesamiento de lenguaje natural y otras aplicaciones informáticas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}