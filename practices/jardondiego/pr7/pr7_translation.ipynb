{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUjR1De9vQBG",
        "outputId": "be72be74-5902-4733-e3e9-692aa389a744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MT-Preparation'...\n",
            "remote: Enumerating objects: 268, done.\u001b[K\n",
            "remote: Counting objects: 100% (268/268), done.\u001b[K\n",
            "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
            "remote: Total 268 (delta 133), reused 189 (delta 97), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (268/268), 69.06 KiB | 2.56 MiB/s, done.\n",
            "Resolving deltas: 100% (133/133), done.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 3)) (0.1.99)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.16.0)\n",
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.5.1-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.8/262.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.1+cu121)\n",
            "Collecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<5,>=4 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-4.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (179.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.15.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-3.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.37 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.7.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (1.25.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.63.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3,>=2.1->OpenNMT-py) (2.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->OpenNMT-py) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3,>=2.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ctranslate2, configargparse, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fasttext-wheel, nvidia-cusolver-cu12, OpenNMT-py\n",
            "Successfully installed OpenNMT-py-3.5.1 colorama-0.4.6 configargparse-1.7 ctranslate2-4.2.1 fasttext-wheel-0.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 portalocker-2.8.2 pyahocorasick-2.1.0 pybind11-2.12.0 pyonmttok-1.37.1 rapidfuzz-3.9.0 sacrebleu-2.4.2 waitress-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ymoslem/MT-Preparation.git\n",
        "!pip3 install -r MT-Preparation/requirements.txt\n",
        "\n",
        "!pip install OpenNMT-py -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GVll_N26gmUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22b27b9-fcfa-42e6-d84f-d90dda5e6a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Setup Google Drive storage\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MN-bM8lQg4Lr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9817a5c9-61e5-4295-c1ae-b506d02f705c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Cloning AmericasNLP repository---\n",
            "Cloning into 'americasnlp2021'...\n",
            "remote: Enumerating objects: 469, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 469 (delta 89), reused 99 (delta 87), pack-reused 333\u001b[K\n",
            "Receiving objects: 100% (469/469), 37.37 MiB | 18.37 MiB/s, done.\n",
            "Resolving deltas: 100% (218/218), done.\n",
            "---Concatenating Spanish dev and train datasets---\n",
            "---Concatenating Nahuatl dev and train datasets---\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Fetch datasets\n",
        "\"\"\"\n",
        "\n",
        "!echo \"---Cloning AmericasNLP repository---\"\n",
        "!git clone https://github.com/AmericasNLP/americasnlp2021.git\n",
        "\n",
        "!echo \"---Concatenating Spanish dev and train datasets---\"\n",
        "!cat americasnlp2021/data/nahuatl-spanish/dev.es americasnlp2021/data/nahuatl-spanish/train.es > MT-Preparation/americasnlp2021.es-nah.es\n",
        "\n",
        "!echo \"---Concatenating Nahuatl dev and train datasets---\"\n",
        "!cat americasnlp2021/data/nahuatl-spanish/dev.nah americasnlp2021/data/nahuatl-spanish/train.nah > MT-Preparation/americasnlp2021.nah-es.nah"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gl775SeChDuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c934fecf-da54-44fe-bf94-d49192e46c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Counting lines in original Nahuatl dataset---\n",
            "16817 MT-Preparation/americasnlp2021.nah-es.nah\n",
            "---Counting lines in original Spanish dataset---\n",
            "16817 MT-Preparation/americasnlp2021.es-nah.es\n",
            "---Running filter.py script---\n",
            "Dataframe shape (rows, columns): (16817, 2)\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 16733\n",
            "--- Duplicates Deleted\t\t\t--> Rows: 16119\n",
            "--- Source-Copied Rows Deleted\t\t--> Rows: 16044\n",
            "--- Too Long Source/Target Deleted\t--> Rows: 14783\n",
            "--- HTML Removed\t\t\t--> Rows: 14783\n",
            "--- Rows will remain in true-cased\t--> Rows: 14783\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 14783\n",
            "--- Rows Shuffled\t\t\t--> Rows: 14783\n",
            "--- Source Saved: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
            "--- Target Saved: MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
            "---Counting lines in filtered Nahuatl dataset---\n",
            "14783 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
            "---Counting lines in filtered Spanish dataset---\n",
            "14783 MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Data preparation\n",
        "\"\"\"\n",
        "\n",
        "!echo \"---Counting lines in original Nahuatl dataset---\"\n",
        "!wc -l MT-Preparation/americasnlp2021.nah-es.nah\n",
        "\n",
        "!echo \"---Counting lines in original Spanish dataset---\"\n",
        "!wc -l MT-Preparation/americasnlp2021.es-nah.es\n",
        "\n",
        "!echo \"---Running filter.py script---\"\n",
        "!python3 MT-Preparation/filtering/filter.py MT-Preparation/americasnlp2021.nah-es.nah MT-Preparation/americasnlp2021.es-nah.es nah es\n",
        "\n",
        "!echo \"---Counting lines in filtered Nahuatl dataset---\"\n",
        "!wc -l MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
        "\n",
        "!echo \"---Counting lines in filtered Spanish dataset---\"\n",
        "!wc -l MT-Preparation/americasnlp2021.es-nah.es-filtered.es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8mp4_xrrhAE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef4c9ed-5381-45a0-a27b-3971ddf35362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Listing files in MT-Preparation/subwording directory---\n",
            "1-train_bpe.py\t1-train_unigram.py  2-subword.py  3-desubword.py\n",
            "---Running 1-train_unigram.py script---\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
            "  input_format: \n",
            "  model_prefix: source\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 14783 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=2053564\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9528% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=87\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999528\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 14783 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=1131929\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 131964 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 14783\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 51930\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 51930 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=39106 obj=12.5167 num_tokens=110092 num_tokens/piece=2.81522\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=32625 obj=10.3851 num_tokens=111630 num_tokens/piece=3.42161\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: source.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: source.vocab\n",
            "Done, training a SentencepPiece model for the Source finished successfully!\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=MT-Preparation/americasnlp2021.es-nah.es-filtered.es --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
            "  input_format: \n",
            "  model_prefix: target\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 14783 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=1982306\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9582% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=81\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999582\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 14783 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=1044786\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 55218 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 14783\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 26627\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 26627 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20221 obj=10.2914 num_tokens=53863 num_tokens/piece=2.66372\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16504 obj=8.50295 num_tokens=54464 num_tokens/piece=3.30005\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: target.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: target.vocab\n",
            "Done, training a SentencepPiece model for the Target finished successfully!\n"
          ]
        }
      ],
      "source": [
        "!echo \"---Listing files in MT-Preparation/subwording directory---\"\n",
        "!ls MT-Preparation/subwording/\n",
        "\n",
        "!echo \"---Running 1-train_unigram.py script---\"\n",
        "!python3 MT-Preparation/subwording/1-train_unigram.py MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah MT-Preparation/americasnlp2021.es-nah.es-filtered.es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwCXfQ8_vQBI",
        "outputId": "c90ccefc-ff29-44c5-b2cf-b593f2b15775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Listing files in current directory---\n",
            "americasnlp2021  MT-Preparation  source.model  target.model\n",
            "drive\t\t sample_data\t source.vocab  target.vocab\n",
            "---Running 2-subword.py script---\n",
            "Source Model: source.model\n",
            "Target Model: target.model\n",
            "Source Dataset: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
            "Target Dataset: MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
            "Done subwording the source file! Output: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword\n",
            "Done subwording the target file! Output: MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword\n",
            "---First 3 lines of filtered Nahuatl and Spanish datasets---\n",
            "pero yekyektsi chipauak sintli\n",
            "Auh ye quin iyomilhuiyoc yn ipan juebes yn ic - eilhuitl mani ni metztli mayo , huel ypan ylhuitzin Sancta Cruz , oquintemohuique in mimicque yn oncan tepilolcuauhtitech .\n",
            "Tehhua nican timocahuaz ,\n",
            "-----\n",
            "pero bonito y blanco el maíz\n",
            "Al día siguiente , el jueves 3 de mayo , fiesta de la Santa Cruz , bajaron a los muertos de los palos de la horca .\n",
            "Tú te quedarás aquí ,\n",
            "---First 10 lines of subworded Nahuatl and Spanish datasets---\n",
            "▁pero ▁yekyekt si ▁chipauak ▁sintli\n",
            "▁Au h ▁ye ▁quin ▁iyom ilhui yoc ▁yn ▁ipan ▁juebes ▁yn ▁ic ▁ - ▁e ilhuitl ▁mani ▁ni ▁metztli ▁mayo ▁, ▁huel ▁ypan ▁ ylhuitzin ▁Sanct a ▁Cruz ▁, ▁oquintemohu ique ▁in ▁mimic que ▁yn ▁oncan ▁tepilolcuauhti tech ▁.\n",
            "▁Tehhua ▁nican ▁timocahua z ▁,\n",
            "▁Ya ▁nopa ▁noponi ▁pejki ▁tlamina ▁nopa ▁itata\n",
            "▁Ika ▁o ui ▁ tlatski ▁ik uak uetlaxo ▁uajki no ▁.\n",
            "▁Ycome ▁unpa ▁ya ▁Quauh nauac ▁Chicomo lloçi ▁, ▁yoquich ui ▁mochiuh ▁Tey ztlaco açi ▁; ▁yn ▁ic ▁ - ▁ey ▁ytoca ▁Quetzal xillo çi ntli ▁Uaca calç intli ▁quiuallitla ▁Toltitla ▁Tiçaua çi ▁; ▁yn ▁ic naui ▁Tlacu chcue çi ▁ye ▁quiuallitla ni ▁Ce ollin tecuhtli ▁Ozto ticpac ▁; ▁yn ▁icmacuill i ▁Coatonal ci ▁ye ▁quiuallitla ni ▁Xiuh ya o tecuhtli ▁Tianquizte co ▁; ▁Mocel ciua çi ▁ uallitlan oc ▁Uax tepec ▁, ▁yoquich ui ▁mochiuh ▁Tepa quizca ci ▁; ▁y ▁Xoco ci ▁ uallitlan oc ▁Chalc o ▁Pochtla ▁, ▁yoquichhui ▁mochiuh ▁Acacit li ▁.\n",
            "▁Quihua motla ▁Cos ama lotepetl ▁, ▁conmotla ▁A lomey matli yan ▁, ▁qui huatl motla ▁Teco mal ac ▁; ▁ye ▁oncan ▁mose senmana ▁yn ▁paraxes za zin ▁Tepec cua tli c pil cac ▁, ▁Tepe polco ▁, ▁Tepe tonco ▁, ▁P acion titla ▁, ▁Qua palan tla ▁, ▁Quauh zon metla ▁, ▁Atl me ya ▁, ▁C uatl ya tlauhco ▁texcal i ▁mo yehualo a ▁atl ▁ic ▁meya ▁.\n",
            "▁Ach quan ▁tiquit to a ▁: ▁i ▁ayac ▁y miuh ▁, ▁yaya c ▁y chimal ▁.\n",
            "▁S E ▁ TAXKAL TSI N ▁S AJ ▁. ▁. ▁.\n",
            "▁I ▁Tocht li ▁xihuitl ▁, ▁ 5 0 ▁.\n",
            "---\n",
            "▁pero ▁bonito ▁y ▁blanco ▁el ▁maíz\n",
            "▁Al ▁día ▁siguiente ▁, ▁el ▁jueve s ▁ 3 ▁de ▁mayo ▁, ▁fiesta ▁de ▁la ▁Sant a ▁Cruz ▁, ▁bajar on ▁a ▁los ▁muertos ▁de ▁los ▁palo s ▁de ▁la ▁horca ▁.\n",
            "▁Tú ▁te ▁quedará s ▁aquí ▁,\n",
            "▁A llí ▁empezó ▁a ▁pesca r ▁su ▁padre ▁[ ▁del ▁Buen ▁Joven ▁]\n",
            "▁Con ▁trabajo ▁se ▁peg ó ▁su ▁piel ▁de ▁la ▁cabeza ▁y ▁así ▁.\n",
            "▁Chicom oyol lotzin ▁también ▁fue ▁a ▁Cuauh náhua c ▁, ▁donde ▁tuvo ▁por ▁marido ▁a ▁Te i ztlaco atzin ▁; ▁a ▁la ▁tercera ▁, ▁llamada ▁Quetzal xi lotzin tli ▁Huacal tzin tli ▁, ▁la ▁solicit ó ▁por ▁esposa ▁Tiza huatzin ▁de ▁Tol titlan ▁; ▁a ▁la ▁cuart a ▁, ▁llamada ▁Tlacochc ue tzin ▁, ▁la ▁solicit ó ▁por ▁esposa ▁Ceo l inteuctl i ▁de ▁Oztot í cpac ▁; ▁a ▁la ▁quinta ▁, ▁llamada ▁Coat on altzin ▁, ▁la ▁solicit ó ▁por ▁esposa ▁Xiuh yao teuctl i ▁de ▁Tianquiz tenco ▁; ▁a ▁Mocel ci huatzin ▁la ▁solicitar on ▁de ▁Huax t épec ▁, ▁y ▁tuvo ▁por ▁marido ▁a ▁Tepanquiz catzin ▁; ▁y ▁a ▁Xoco tzin ▁la ▁solicitar on ▁de ▁P ochtlan ▁Chalc o ▁, ▁donde ▁tuvo ▁por ▁marido ▁a ▁Acacitl i ▁.\n",
            "▁L uego ▁van ▁los ▁lindero s ▁en ▁direcci ón ▁a ▁Coza ma lotépe tl ▁, ▁Al omei matliya n ▁y ▁Teco má lac ▁; ▁allí ▁se ▁extiende n ▁los ▁paraje s ▁de ▁Tepecua tlic pílca c ▁, ▁Tepep olco ▁, ▁Tepet onco ▁, ▁Pas i ontitlan ▁, ▁Cuappa lantla ▁, ▁Cuauhtzon metl a ▁, ▁Atl me ya ▁, ▁y ▁Coatli atlauh co ▁, ▁donde ▁hay ▁una ▁peña ▁rodead a ▁por ▁el ▁agua ▁que ▁allí ▁man a ▁.\n",
            "▁Ahor a ▁tú ▁dice ▁: ▁nadie ▁tiene ▁flecha s ▁, ▁nadie ▁tiene ▁escudo s ▁.\n",
            "▁Una ▁tortillita ▁no ▁más ▁...\n",
            "▁ 1 ▁Tochtl i ▁, ▁ 5 0 ▁.\n"
          ]
        }
      ],
      "source": [
        "!echo \"---Listing files in current directory---\"\n",
        "!ls\n",
        "\n",
        "!echo \"---Running 2-subword.py script---\"\n",
        "!python3 MT-Preparation/subwording/2-subword.py source.model target.model MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
        "\n",
        "!echo \"---First 3 lines of filtered Nahuatl and Spanish datasets---\"\n",
        "!head -n 3 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah && echo \"-----\" && head -n 3 MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
        "\n",
        "!echo \"---First 10 lines of subworded Nahuatl and Spanish datasets---\"\n",
        "!head -n 10 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword && echo \"---\" && head -n 10 MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zok3VlyqPd8",
        "outputId": "4bf3ab46-ac2a-48b7-f781-77b6b806d7af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Running train_dev_test_split.py---\n",
            "Dataframe shape: (14783, 2)\n",
            "--- Empty Cells Deleted --> Rows: 14783\n",
            "--- Wrote Files\n",
            "Done!\n",
            "Output files\n",
            "MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.train\n",
            "MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.train\n",
            "MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.dev\n",
            "MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.dev\n",
            "MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test\n",
            "MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test\n",
            "---Counting lines in subword files---\n",
            "   1000 MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.dev\n",
            "   1000 MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test\n",
            "  12783 MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.train\n",
            "   1000 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.dev\n",
            "   1000 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test\n",
            "  12783 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.train\n",
            "  29566 total\n",
            "---First line of each file---\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.train <==\n",
            "▁pero ▁bonito ▁y ▁blanco ▁el ▁maíz\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.train <==\n",
            "▁pero ▁yekyekt si ▁chipauak ▁sintli\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.dev <==\n",
            "▁Empez aron ▁a ▁perseguirlo ▁, ▁y ▁el ▁venado ▁se ▁dirigi ó ▁hacia ▁Colhuaca n ▁, ▁pero ▁se ▁atas có ▁en ▁el ▁lo do ▁; ▁así ▁pudieron ▁fácilmente ▁tomarlo ▁y ▁atarlo ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.dev <==\n",
            "▁Nima ▁ye ▁yc ▁quitoca ▁, ▁unpa ▁quitztilti ui ▁yn ▁Colhuaca ▁, ▁yc ▁omoçoqui aquito ▁, ▁çan ▁ que mach ▁conan que ▁con quimilo que ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test <==\n",
            "▁Pero ▁es ▁un ▁requisito ▁in dispensa ble ▁para ▁vivir ▁en ▁armon ía ▁con ▁los ▁otros ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test <==\n",
            "▁No za ▁in ▁cen equiliztli ▁inic ▁occequin ▁incepan ▁cualli ▁nemizqueh ▁.\n",
            "---Last line of each file---\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.train <==\n",
            "▁El ▁lunes ▁ 2 9 ▁de ▁septi embre ▁de ▁ 1 6 0 8 ▁, ▁fiesta ▁de ▁San ▁Miguel ▁, ▁lleg ó ▁y ▁entr ó ▁a ▁la ▁ciudad ▁de ▁México ▁Tenochtitlan ▁el ▁muy ▁reverendo ▁señor ▁don ▁fra y ▁Garc ía ▁Guerr a ▁, ▁religioso ▁de ▁Santo ▁Doming o ▁y ▁arzobispo ▁de ▁México ▁, ▁que ▁ahora ▁nos ▁gobierna ▁espiritual mente ▁en ▁Tenochtitlan ▁como ▁sexto ▁arzobispo ▁de ▁México ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.train <==\n",
            "▁Au h ▁yn ▁ipan ▁axcan ▁lunes ▁yc ▁ 2 9 ▁mani ▁metztli ▁de ▁set iembre ▁de ▁ 1 6 0 8 ▁años ▁, ▁y quac ▁ypan ▁Sant ▁Miguel ▁ ylhuitzin ▁, ▁maxiti co ▁mocallaqui co ▁nican ▁y htic ▁yn ▁altepetl ▁ciu dad ▁Mexico ▁Tenochtitlan ▁in ▁yehuatzin ▁omoteneuhtzino ▁yn ▁cenca ▁mahuiztililoni ▁teoyotica ▁tlahtohuani ▁don ▁fray ▁Garc ía ▁Guerr a ▁arçobispo ▁Mexico ▁, ▁ye ▁omito ▁teopixqu i ▁Sancto ▁D omingo ▁, ▁yn ▁axcan ▁moyetztica ▁teoyotica ▁motlahtocatil ia ▁techmopa ▁ - ▁| ▁| ▁ 7 2 ▁ - ▁ chilhuia ▁Tenochtitlan ▁, ▁yc chiquacen ▁arçobispo ▁yn ▁nican ▁Mexico ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.dev <==\n",
            "▁Y ▁los ▁señores ▁de ▁Tecpayo can ▁, ▁Huá uhquil ▁y ▁Tlotli teuctl i ▁, ▁por ▁segund a ▁vez ▁se ▁fueron ▁a ▁Cohuatlic ámac ▁y ▁a ▁H uauhquil i chan ▁; ▁allá ▁fueron ▁a ▁gobernar ▁a ▁los ▁chichimeca s ▁que ▁habían ▁sali do ▁de ▁Tecpayo can ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.dev <==\n",
            "▁Au ▁i ▁Tecpayoca n ▁onoca ▁, ▁y ▁Uauhquil ▁, ▁yn ▁Tlotlitec tli ▁, ▁yn ▁icopa ▁yaque ▁y ▁moto ça uica ▁y ▁Couatl icamac ▁y ▁Uauhquil icha ▁; ▁ euatl ▁ompa ▁tlatocayoti to ▁yn ▁Tecpayoca n ▁euac ▁chichimeca ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test <==\n",
            "▁silla\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test <==\n",
            "▁ nechuikaka\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Dividir en train, dev y test\n",
        "\"\"\"\n",
        "\n",
        "!echo \"---Running train_dev_test_split.py---\"\n",
        "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 1000 1000 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword\n",
        "\n",
        "!echo \"---Counting lines in subword files---\"\n",
        "!wc -l MT-Preparation/*.subword.*\n",
        "\n",
        "!echo \"---First line of each file---\"\n",
        "!head -n 1 MT-Preparation/*.{train,dev,test}\n",
        "\n",
        "!echo \"---Last line of each file---\"\n",
        "!tail -n 1 MT-Preparation/*.{train,dev,test}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8Nzpe_3picY",
        "outputId": "321d976a-4de6-4a8a-d55f-ceada82d2e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "## Where the samples will be written\n",
            "save_data: run\n",
            "\n",
            "# Rutas de archivos de entrenamiento\n",
            "#(previamente aplicado subword tokenization)\n",
            "data:\n",
            "    corpus_1:\n",
            "        path_src: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.train\n",
            "        path_tgt: MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.train\n",
            "        transforms: [filtertoolong]\n",
            "    valid:\n",
            "        path_src: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.dev\n",
            "        path_tgt: MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.dev\n",
            "        transforms: [filtertoolong]\n",
            "\n",
            "# Vocabularios (serán generados por `onmt_build_vocab`)\n",
            "src_vocab: run/source.vocab\n",
            "tgt_vocab: run/target.vocab\n",
            "\n",
            "# Tamaño del vocabulario\n",
            "#(debe concordar con el parametro usado en el algoritmo de subword tokenization)\n",
            "src_vocab_size: 50000\n",
            "tgt_vocab_size: 50000\n",
            "\n",
            "# Filtrado sentencias de longitud mayor a n\n",
            "# actuara si [filtertoolong] está presente\n",
            "src_seq_length: 150\n",
            "src_seq_length: 150\n",
            "\n",
            "# Tokenizadores\n",
            "src_subword_model: source.model\n",
            "tgt_subword_model: target.model\n",
            "\n",
            "# Archivos donde se guardaran los logs y los checkpoints de modelos\n",
            "log_file: train.log\n",
            "save_model: models/model.nahes\n",
            "\n",
            "# Condición de paro si no se obtienen mejoras significativas\n",
            "# despues de n validaciones\n",
            "early_stopping: 4\n",
            "\n",
            "# Guarda un checkpoint del modelo cada n steps\n",
            "save_checkpoint_steps: 1000\n",
            "\n",
            "# Mantiene los n ultimos checkpoints\n",
            "keep_checkpoint: 3\n",
            "\n",
            "# Reproductibilidad\n",
            "seed: 3435\n",
            "\n",
            "# Entrena el modelo maximo n steps\n",
            "# Default: 100,000\n",
            "train_steps: 3000\n",
            "\n",
            "# Corre el set de validaciones (*.dev) despues de n steps\n",
            "# Defatul: 10,000\n",
            "valid_steps: 1000\n",
            "\n",
            "warmup_steps: 1000\n",
            "report_every: 100\n",
            "\n",
            "# Numero de GPUs y sus ids\n",
            "world_size: 1\n",
            "gpu_ranks: [0]\n",
            "\n",
            "# Batching\n",
            "bucket_size: 262144\n",
            "num_workers: 0\n",
            "batch_type: \"tokens\"\n",
            "batch_size: 4096\n",
            "valid_batch_size: 2048\n",
            "max_generator_batches: 2\n",
            "accum_count: [4]\n",
            "accum_steps: [0]\n",
            "\n",
            "# Configuración del optimizador\n",
            "model_dtype: \"fp16\"\n",
            "optim: \"adam\"\n",
            "learning_rate: 2\n",
            "# warmup_steps: 8000\n",
            "decay_method: \"noam\"\n",
            "adam_beta2: 0.998\n",
            "max_grad_norm: 0\n",
            "label_smoothing: 0.1\n",
            "param_init: 0\n",
            "param_init_glorot: true\n",
            "normalization: \"tokens\"\n",
            "\n",
            "# Configuración del Modelo\n",
            "encoder_type: transformer\n",
            "decoder_type: transformer\n",
            "position_encoding: true\n",
            "enc_layers: 6\n",
            "dec_layers: 6\n",
            "heads: 8\n",
            "hidden_size: 512\n",
            "word_vec_size: 512\n",
            "transformer_ff: 2048\n",
            "dropout_steps: [0]\n",
            "dropout: [0.1]\n",
            "attention_dropout: [0.1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "SRC_DATA_NAME = \"MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword\"\n",
        "TARGET_DATA_NAME = \"MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword\"\n",
        "\n",
        "config = f\"\"\"\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Rutas de archivos de entrenamiento\n",
        "#(previamente aplicado subword tokenization)\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: {SRC_DATA_NAME}.train\n",
        "        path_tgt: {TARGET_DATA_NAME}.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: {SRC_DATA_NAME}.dev\n",
        "        path_tgt: {TARGET_DATA_NAME}.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabularios (serán generados por `onmt_build_vocab`)\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Tamaño del vocabulario\n",
        "#(debe concordar con el parametro usado en el algoritmo de subword tokenization)\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filtrado sentencias de longitud mayor a n\n",
        "# actuara si [filtertoolong] está presente\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenizadores\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Archivos donde se guardaran los logs y los checkpoints de modelos\n",
        "log_file: train.log\n",
        "save_model: models/model.nahes\n",
        "\n",
        "# Condición de paro si no se obtienen mejoras significativas\n",
        "# despues de n validaciones\n",
        "early_stopping: 4\n",
        "\n",
        "# Guarda un checkpoint del modelo cada n steps\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# Mantiene los n ultimos checkpoints\n",
        "keep_checkpoint: 3\n",
        "\n",
        "# Reproductibilidad\n",
        "seed: 3435\n",
        "\n",
        "# Entrena el modelo maximo n steps\n",
        "# Default: 100,000\n",
        "train_steps: 3000\n",
        "\n",
        "# Corre el set de validaciones (*.dev) despues de n steps\n",
        "# Defatul: 10,000\n",
        "valid_steps: 1000\n",
        "\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Numero de GPUs y sus ids\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Configuración del optimizador\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Configuración del Modelo\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "\"\"\"\n",
        "\n",
        "# create config file\n",
        "!echo \"\" > ./config.yaml\n",
        "\n",
        "# write config to file\n",
        "with open(\"./config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)\n",
        "\n",
        "# print config\n",
        "!cat ./config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E2hX1Nfug7q1",
        "outputId": "0af7196f-f52a-4d93-fa35-245578e72987"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nVocabulary\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\"\"\"\n",
        "Vocabulary\n",
        "\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %%time\n",
        "# !onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVdIdYIavQBL",
        "outputId": "2621a17d-64dc-4676-81cb-6e9eebc4f717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-0841721f-4bed-270a-10a0-cd8a77d7f332)\n",
            "True\n",
            "Tesla T4\n",
            "Free GPU memory: 14999.0625 out of: 15102.0625\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Validate GPU support\n",
        "\"\"\"\n",
        "\n",
        "# Check if the GPU is active\n",
        "!nvidia-smi -L\n",
        "\n",
        "# Check if the GPU is visable to PyTorch\n",
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "gpu_memory = torch.cuda.mem_get_info(0)\n",
        "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training\n",
        "\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %%time\n",
        "# !onmt_train -config config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kEhxsW2svZvQ",
        "outputId": "2268f8ca-8c95-4765-ecca-64d08b29d610"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTraining\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS99_xZivQBL",
        "outputId": "4623d401-3052-4b5e-a710-48a861f7d02f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Listing models---\n",
            "ls: cannot access 'models': No such file or directory\n",
            "---Last few lines of the test dataset---\n",
            "▁Yn ▁tlatocat ▁Matlaco uatl ▁ye poualxiu itl ▁um atlactli ▁; ▁oc ▁yehoa tl ▁y ▁ualteyaca ▁yn ▁acico ▁, ▁yn ▁iquac ▁acico ▁umpa ▁Azcapotzal tonco ▁.\n",
            "▁ta ▁achto ▁xi kalaki ▁uan ▁tel ▁to motsojtsouiteki s\n",
            "▁Quenin ▁toxochimil ian ▁itual li ▁huehuey intin ▁, ▁nochipa ▁to techmonequi ▁cente ▁chichiton\n",
            "▁azo ▁cana ▁tictlan toz ▁in ▁atl ▁in ▁tepetl ▁, ▁azo ▁ye ▁cemm anyan ▁mixco ▁mocpac ▁on ▁ tlatlachia ▁.\n",
            "▁in ▁nican ▁ tiquihiyo uico ▁in ▁tic cia uico ▁in ▁toneua co ▁in ▁chichinaca co ▁in ▁moyollo ▁in ▁monacayo ▁in ▁tino cuauh ▁in ▁ tin ocelo uh ▁in ▁ti cuauh tlacatl ▁in ▁ti zacatlaca tl ▁in ▁titox onqui ▁in ▁ti ua zonqui ▁.\n",
            "▁amo ▁nicnequi c ▁nicchihuaz\n",
            "▁huan ▁techpalehuiz ▁.\n",
            "▁KUOUJ EL OT\n",
            "▁Au h ▁ynic ▁moyolle uh ▁ynic ▁cenca ▁quinec ▁| ▁| ▁ 1 0 4 r ▁huallaz ▁yn ▁quinexti quiuh ▁tlalli ▁nican ▁, ▁auh ▁yuhqui ▁in ▁nenemi a ▁ce ▁tlacatl ▁yn ▁ipan ▁hueyapan ▁yn ▁quinenemiltia ▁huey acalli ▁calac h ixqui ▁; ▁auh ▁yece ▁yn ▁itoca ▁amo ▁huel ▁momati ▁yhuan ▁campa ▁tlacat ▁amo ▁huel ▁mellahuac ▁momati ▁.\n",
            "▁ nechuikaka\n",
            "---Last few lines of the translated dataset---\n",
            "tail: cannot open 'MT-Preparation/americasnlp2021.es.practice.translated' for reading: No such file or directory\n",
            "---Running 3-desubword.py script on test dataset---\n",
            "Done desubwording! Output: MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test.desubword\n",
            "---Running 3-desubword.py script on translated dataset---\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MT-Preparation/subwording/3-desubword.py\", line 21, in <module>\n",
            "    with open(target_pred) as pred, open(target_decodeded, \"w+\") as pred_decoded:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'MT-Preparation/americasnlp2021.es.practice.translated'\n",
            "---Last few lines of the desubworded translated dataset---\n",
            "tail: cannot open 'MT-Preparation/americasnlp2021.es.practice.translated.desubword' for reading: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Translation\n",
        "\"\"\"\n",
        "\n",
        "# List the models in the models directory\n",
        "!echo \"---Listing models---\"\n",
        "!ls models\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %%time\n",
        "# !onmt_translate -model models/model.nahes_step_3000.pt -src MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test -output MT-Preparation/americasnlp2021.es.practice.translated -gpu 0 -min_length 1\n",
        "\n",
        "# Display the last few lines of the test dataset\n",
        "!echo \"---Last few lines of the test dataset---\"\n",
        "!tail MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test\n",
        "\n",
        "# Display the last few lines of the translated dataset\n",
        "!echo \"---Last few lines of the translated dataset---\"\n",
        "!tail MT-Preparation/americasnlp2021.es.practice.translated\n",
        "\n",
        "# Run the 3-desubword.py script to remove subwording from the test dataset\n",
        "!echo \"---Running 3-desubword.py script on test dataset---\"\n",
        "!python MT-Preparation/subwording/3-desubword.py source.model MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test\n",
        "\n",
        "# Run the 3-desubword.py script to remove subwording from the translated dataset\n",
        "!echo \"---Running 3-desubword.py script on translated dataset---\"\n",
        "!python MT-Preparation/subwording/3-desubword.py target.model MT-Preparation/americasnlp2021.es.practice.translated\n",
        "\n",
        "# Display the last few lines of the desubworded translated dataset\n",
        "!echo \"---Last few lines of the desubworded translated dataset---\"\n",
        "!tail MT-Preparation/americasnlp2021.es.practice.translated.desubword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdQgpl7RrIGg",
        "outputId": "aac6fe36-d4e9-4395-ad3e-312688e67cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Cloning MT-Evaluation repository---\n",
            "fatal: destination path 'MT-Evaluation' already exists and is not an empty directory.\n",
            "---Installing requirements for MT-Evaluation---\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (from -r MT-Evaluation/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r MT-Evaluation/requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from -r MT-Evaluation/requirements.txt (line 3)) (2.4.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from -r MT-Evaluation/requirements.txt (line 4)) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer->-r MT-Evaluation/requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer->-r MT-Evaluation/requirements.txt (line 1)) (3.9.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r MT-Evaluation/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r MT-Evaluation/requirements.txt (line 2)) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r MT-Evaluation/requirements.txt (line 2)) (4.66.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (4.9.4)\n",
            "---Running 3-desubword.py script---\n",
            "Done desubwording! Output: MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test.desubword\n",
            "---Running compute-bleu.py script---\n",
            "Reference 1st sentence: Pero es un requisito indispensable para vivir en armonía con los otros .\n",
            "MTed 1st sentence: Aquel cerro es muy grande .\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "BLEU:  0.21706750074414768\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Evaluation\n",
        "\"\"\"\n",
        "\n",
        "!echo \"---Cloning MT-Evaluation repository---\"\n",
        "!git clone https://github.com/ymoslem/MT-Evaluation.git\n",
        "\n",
        "!echo \"---Installing requirements for MT-Evaluation---\"\n",
        "!pip install -r MT-Evaluation/requirements.txt\n",
        "\n",
        "!echo \"---Running 3-desubword.py script---\"\n",
        "!python MT-Preparation/subwording/3-desubword.py target.model MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test\n",
        "\n",
        "!echo \"---Running compute-bleu.py script---\"\n",
        "!python MT-Evaluation/BLEU/compute-bleu.py MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test.desubword MT-Preparation/americasnlp2021.es.practice.translated.desubword"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}