{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Práctica 7**\n",
    "## Traducción con redes neuronales\n",
    "\n",
    "### Fecha de entrega\n",
    "\n",
    "5 de mayo 2024 11.59 p.m.\n",
    "\n",
    "### Actividades\n",
    "\n",
    "- Explorar los datasets disponibles en el *Shared Task de Open Machine Translation de AmericasNLP 2021*\n",
    "    - [Datasets](https://github.com/AmericasNLP/americasnlp2021/tree/main/data)\n",
    "    - [Readme](https://github.com/AmericasNLP/americasnlp2021/tree/main#readme)\n",
    "- Crear un modelo de traducción neuronal usando OpenNMT-py y siguiendo el pipeline visto en clase\n",
    "    - 0. Obtención de datos y preprocesamiento\n",
    "        - Considerar que tiene que entrenar su modelo de tokenization\n",
    "    - 1. Configuración y entrenamiento del modelo\n",
    "    - 2. Traducción\n",
    "    - 3. Evaluación\n",
    "        - Reportar BLEU\n",
    "        - Reportar ChrF (medida propuesta para el shared task)\n",
    "        - Más info: [evaluate.py](https://github.com/AmericasNLP/americasnlp2021/blob/main/evaluate.py)        \n",
    "- Comparar resultados con [baseline](https://github.com/AmericasNLP/americasnlp2021/tree/main/baseline_system#baseline-results)\n",
    "- Incluir el archivo `*.translated.desubword`\n",
    "\n",
    "### Extra\n",
    "\n",
    "- Investigar porque se propuso la medida ChrF en el Shared Task\n",
    "    - ¿Como se diferencia de BLEU?\n",
    "    - ¿Porqué es reelevante utilizar otras medidas de evaluación además de BLEU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping dev.es\n",
      "Skipping dev.nah\n",
      "Skipping test.es\n",
      "Downloading train.es\n",
      "Downloading train.nah\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fetch data sets directly from GitHub\n",
    "\"\"\"\n",
    "\n",
    "URLs = \"\"\"\n",
    "https://raw.githubusercontent.com/AmericasNLP/americasnlp2021/main/data/nahuatl-spanish/dev.es\n",
    "https://raw.githubusercontent.com/AmericasNLP/americasnlp2021/main/data/nahuatl-spanish/dev.nah\n",
    "https://raw.githubusercontent.com/AmericasNLP/americasnlp2021/main/data/nahuatl-spanish/test.es\n",
    "https://raw.githubusercontent.com/AmericasNLP/americasnlp2021/main/data/nahuatl-spanish/train.es\n",
    "https://raw.githubusercontent.com/AmericasNLP/americasnlp2021/main/data/nahuatl-spanish/train.nah\n",
    "\"\"\"\n",
    "\n",
    "# Download the data\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "formatted_URLs = [ url for url in URLs.strip().split('\\n') if url.strip() != '' ]\n",
    "for url in formatted_URLs:\n",
    "    filename = url.split('/')[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    else:\n",
    "        print(f\"Skipping {filename}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing\n",
    "\"\"\"\n",
    "\n",
    "# TODO: determine best way to preprocess the data\n",
    "# read data\n",
    "# tokenize\n",
    "# lowercase\n",
    "# remove punctuation\n",
    "# remove stopwords\n",
    "# remove numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport sacrebleu\\nimport argparse\\n\\n\\ndef calculate_score_report(sys, ref, score_only):\\n\\n    chrf = sacrebleu.corpus_chrf(sys, ref)\\n    bleu = sacrebleu.corpus_bleu(sys, ref)\\n\\n    prefix = 'BLEU = ' if score_only else ''\\n\\n    print('#### Score Report ####')\\n    print(chrf)\\n    print('{}{}'.format(prefix, bleu.format(score_only=score_only)))\\n\\n\\nif __name__ == '__main__':\\n\\n    parser = argparse.ArgumentParser()\\n\\n    parser.add_argument('--system_output', '--sys', type=str, help='File with each line-by-line model outputs')\\n    parser.add_argument('--gold_reference', '--ref', type=str, help='File with corresponding line-by-line references')\\n    parser.add_argument('--detailed_output', action='store_const', const=True, default=False, help='(sacrebleu) Print additional BLEU information (default=False)')\\n    args = parser.parse_args()\\n\\n    gold_lines = []\\n    no_translations = []\\n    with open(args.gold_reference, 'r') as f:\\n        for i, line in enumerate(f):\\n\\n            if len(line.strip()) == 0:\\n                no_translations.append(i)\\n                continue\\n\\n            gold_lines.append(line.strip())\\n\\n    system_lines = []\\n\\n    print(no_translations)\\n\\n    with open(args.system_output, 'r') as f:\\n        for i,line in enumerate(f):\\n\\n            if i in no_translations:\\n                continue\\n\\n            system_lines.append(line.strip())\\n\\n\\n    assert len(system_lines) == len(gold_lines)\\n\\n    print(len(system_lines))\\n\\n\\n\\n    calculate_score_report(system_lines, [gold_lines], score_only=not args.detailed_output)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import sacrebleu\n",
    "import argparse\n",
    "\n",
    "\n",
    "def calculate_score_report(sys, ref, score_only):\n",
    "\n",
    "    chrf = sacrebleu.corpus_chrf(sys, ref)\n",
    "    bleu = sacrebleu.corpus_bleu(sys, ref)\n",
    "\n",
    "    prefix = 'BLEU = ' if score_only else ''\n",
    "\n",
    "    print('#### Score Report ####')\n",
    "    print(chrf)\n",
    "    print('{}{}'.format(prefix, bleu.format(score_only=score_only)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--system_output', '--sys', type=str, help='File with each line-by-line model outputs')\n",
    "    parser.add_argument('--gold_reference', '--ref', type=str, help='File with corresponding line-by-line references')\n",
    "    parser.add_argument('--detailed_output', action='store_const', const=True, default=False, help='(sacrebleu) Print additional BLEU information (default=False)')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    gold_lines = []\n",
    "    no_translations = []\n",
    "    with open(args.gold_reference, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "\n",
    "            if len(line.strip()) == 0:\n",
    "                no_translations.append(i)\n",
    "                continue\n",
    "\n",
    "            gold_lines.append(line.strip())\n",
    "\n",
    "    system_lines = []\n",
    "\n",
    "    print(no_translations)\n",
    "\n",
    "    with open(args.system_output, 'r') as f:\n",
    "        for i,line in enumerate(f):\n",
    "\n",
    "            if i in no_translations:\n",
    "                continue\n",
    "\n",
    "            system_lines.append(line.strip())\n",
    "\n",
    "\n",
    "    assert len(system_lines) == len(gold_lines)\n",
    "\n",
    "    print(len(system_lines))\n",
    "\n",
    "\n",
    "\n",
    "    calculate_score_report(system_lines, [gold_lines], score_only=not args.detailed_output)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEvaluation\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation\n",
    "\"\"\"\n",
    "\n",
    "# TODO: print evaluation metrics i.e. BLEU and ChrF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigación\n",
    "\n",
    "#### ¿Qué es ChrF y cómo funciona?\n",
    "\n",
    "**ChrF**, o **Character n-gram F-score**, es una métrica que se utiliza para evaluar la calidad de las traducciones automáticas comparándolas con traducciones de referencia humanas. A diferencia de BLEU, que evalúa la coincidencia de n-gramas de palabras, ChrF evalúa las coincidencias a nivel de caracteres y n-gramas de caracteres. Esto implica que se concentra más en la similitud textual a un nivel más granular.\n",
    "\n",
    "La métrica ChrF calcula tanto la precisión como la cobertura (recall) de los n-gramas de caracteres:\n",
    "\n",
    "- **Precisión**: Proporción de n-gramas de caracteres en la traducción automática que también aparecen en la traducción de referencia.\n",
    "- **Recall (Cobertura)**: Proporción de n-gramas de caracteres en la traducción de referencia que también aparecen en la traducción automática.\n",
    "\n",
    "Estas dos medidas se combinan en una puntuación F-score, que es la media armónica de la precisión y la cobertura. La puntuación de ChrF se puede ajustar para ponderar más la precisión o la cobertura mediante un factor beta (β). Un valor común es ChrF++ (o ChrF3), que da más peso al recall para enfatizar la importancia de no perder contenido en la traducción.\n",
    "\n",
    "#### Diferencias entre ChrF y BLEU\n",
    "\n",
    "Mientras que BLEU es una métrica establecida y ampliamente usada en la evaluación de traducciones automáticas, presenta algunas limitaciones, especialmente en idiomas con estructuras morfológicas complejas o en textos donde la exactitud a nivel de palabra no captura completamente la calidad de la traducción. Las principales diferencias entre ChrF y BLEU son:\n",
    "\n",
    "- **Granularidad**: ChrF evalúa a nivel de caracteres, lo que la hace más sensible a las variaciones morfológicas y ortográficas entre la traducción y la referencia.\n",
    "- **Robustez en idiomas morfológicamente ricos**: Para idiomas como el árabe, turco o finés, donde las palabras pueden alterar significativamente su forma debido a la conjugación o la formación de palabras, ChrF puede ofrecer una evaluación más precisa que BLEU.\n",
    "- **Menos sensibilidad a los errores de tokenización**: Dado que BLEU depende de una tokenización precisa, cualquier error en este proceso puede afectar significativamente su puntuación. ChrF, al basarse en caracteres, es menos susceptible a estos problemas.\n",
    "\n",
    "#### Importancia de utilizar múltiples métricas de evaluación\n",
    "\n",
    "Depender exclusivamente de BLEU para evaluar sistemas de traducción automática puede llevar a una comprensión incompleta de su rendimiento. Cada métrica tiene sus fortalezas y debilidades, y diferentes métricas pueden resaltar diferentes aspectos de la calidad de la traducción. Al utilizar ChrF junto con BLEU, los evaluadores pueden obtener una visión más completa y matizada del desempeño de un sistema de traducción, especialmente en términos de cómo maneja las diferencias lingüísticas y culturales entre los idiomas de origen y destino."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
