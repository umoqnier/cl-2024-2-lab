# -*- coding: utf-8 -*-
"""Practica3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JNkEgYoy-bre91VJdKsmxP44tkBQjp9W

# Práctica 3: Propiedades estadísticas del lenguaje natural
Elaborado por: Alejandro Axel Rodríguez Sánchez  
Correo: ahexo@ciencias.unam.mx   
Github: @Ahexo  
Número de Cuenta: 315247697  
Institución: Facultad de Ciencias UNAM  
Asignatura: Lingüística computacional  
Semestre: 2024-2  
Grupo: 7014

Estas son las bibliotecas que vamos a emplear a lo largo de la práctica
"""

# Para uso general
import pandas as pd
import matplotlib.pyplot as plt

# Para la 1ra actividad
import nltk
from nltk.corpus import cess_esp, stopwords
from nltk.probability import FreqDist
from wordcloud import WordCloud

# Para la 2da actividad
import random
from random import randint
from collections import Counter
import re


import numpy as np
import string

"""# Actividad 1: Nube de stopwords
Para efectuar esta práctica, me he tomado la libertad de editar el corpus CREA y transformarlo en un CSV. Esto es con el fin de facilitar su importación y operación dentro de Python por medio de la biblioteca Pandas.
"""

df = pd.read_csv('crea.csv', encoding='latin1', index_col='ID')
df['frec_absoluta'] = df['frec_absoluta'].astype(int)
df['frec_normalizada'] = df['frec_normalizada'].astype(float)
print('Corpus CREA:')
print(df.head(10))

"""Importamos también el corpus de Español de la biblioteca nltk, con el cual vamos a efectuar algunas comparaciones respecto al CREA."""

nltk.download('punkt')
nltk.download('cess_esp')

"""Obtenemos las primeras 10 palabras de cada corpus:"""

stopwords_crea = df['orden'].head(10).tolist()
stopwords_nltk = stopwords.words('spanish')[:10]
print(f"Stopwords en CREA: \n{stopwords_crea}\n")
print(f"Stopwords en NLTK: \n{stopwords_nltk}\n")

"""# Nube de palabras

Generamos la nube para las stopwords del CREA:
"""

word_freq_crea = dict(zip(stopwords_crea, df.head(10)['frec_absoluta']))
print(word_freq_crea)
wordcloud_crea = WordCloud(width=800, height=400).generate_from_frequencies(word_freq_crea)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_crea, interpolation='bilinear')
plt.title("Stopwords en CREA: Nube de palabras")
plt.axis('off')
plt.show()

"""Generamos la nube para las stopwords del NLTK:"""

freq_dist = FreqDist(cess_esp.words())

word_freq_nltk = {stopword: freq_dist[stopword] for stopword in stopwords_nltk}
print(word_freq_nltk)
wordcloud_nltk = WordCloud(width=800, height=400).generate_from_frequencies(word_freq_nltk)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_nltk, interpolation='bilinear')
plt.title("Stopwords en NLTK: Nube de palabras")
plt.axis('off')
plt.show()

"""Podemos observar que ambas nubes guardan mucha similitud, salvo por los valores de distribución y que los últimos dos valores rankeados en el top 10 (stopwords) están intercambiados.

Aún con esta pequeña peculiaridad, la cual se puede deber al tamaño, calidad y escala de los corpus con los que se obtuvieron estos resultados, es bastante seguro afirmar que **la ley de Zipf se cumple para el español** si nuestra referencia son el corpus CREA y el de NLTK, pues la distribución del resto de *stopwords* es idéntica.

## Actividad 2: Zipf en un lenguaje artificial

Ahora vamos a generar una suerte de lengua artificial para comprobar si la Ley de Zipf se cumple para esta.

Por lengua artificial vamos a entender un léxico generado por medios computacionales siguiendo algún principio probabilistico, aleatorio o cualsea, sobre el cual podemos construir conjuntos de oraciones y ejecutar todas las tareas de NLP que queramos.

Para nuestro lenguaje artificial vamos a establecer unas reglas sencillas:
* Tiene un conjunto de vocales (a,u,e,o,y,x) las cuales pueden tener grafías como la dieresis (ä,ë,ö,ü,ÿ,ẍ) o el acento (á,é,ó,ú,ý, la x no tiene), las cuales representan un hipotético tono que tendría nuestro lenguaje.

* El resto de letras del alfabeto latino (b,c,d,f,g,h,i,j,k,l,m,n,p,q,r,s,t,v,z) serán nuestras consonantes.

* Las palabras del lenguaje se componen de combinaciones de sílabas (una consonante con una vocal) o de vocales sueltas.
"""

vocales = "aueoyxäëöüÿẍáéóúý"
consonantes = "bcdfghijklmnpqrstvz"

"""Nuestro lenguaje artificial se va a llamar **Xxoüris**, que es una de las palabras generadas usando estas reglas (y que personalmente me pareció mas a como se llamaría una lengua así)."""

def generar_silaba():
    """
    Las palabras del xxoüris se componen de silabas (vocal + consonante) o vocales sueltas, esta función genera aleatoriamente una silaba o una vocal sola para construir una palabra.
    """
    vocal = random.choice(vocales)

    if random.randint(0, 1) == 1:
        consonante = random.choice(consonantes)
    else:
        consonante = ""

    return vocal + consonante

def generar_palabra(min_length=1, max_length=10):
    """
    Genera una posible palabra del xxoüris generando silabas aleatorias o vocales y concatenandolas.
    """
    length = randint(min_length, max_length)
    word = ''
    for i in range(length):
        word = word.join(generar_silaba())
    return word

def generar_vocabulario(num_palabras, min_length=1, max_length=10):
    """
    Genera un conjunto de palabras del xxoüris de tamaño arbitrario.
    """
    palabras = set()

    while len(palabras) < num_palabras:
        palabra = generar_palabra(min_length, max_length)
        palabras.add(palabra)

    return palabras

def generar_oracion(lexico, lenght=1):
    """
    Genera una oración con el conjunto de palabras generado.
    """
    sentence = [random.choice(lexico) for _ in range(lenght)]
    return ' '.join(sentence)


xxouris_vocabulario = list(generar_vocabulario(100, 1, 10))
print("Muestra de vocabulario del xxoüris:")
print(xxouris_vocabulario)

print("\nMuestra de oraciones del xxoüris:")
for i in range(0,5):
  print(generar_oracion(xxouris_vocabulario, randint(2, 10)))

"""Ya que tenemos nuestro léxico y nuestro generador de oraciones, vamos a construir un corpus del cual obtener estadísticas para verificar si se cumple la ley de Zipf."""

oraciones = []
for i in range(0,160):
  oraciones.append(generar_oracion(xxouris_vocabulario, randint(2, 10)) + random.choice([', ','. ']))

corpus = ''.join(oraciones)
print(corpus)

def frequencias_por_palabra(corpus):
    words = re.findall(r'\b\w+\b', corpus.lower())
    word_freq = Counter(words)
    return word_freq

xxouris_frecuencias = frequencias_por_palabra(corpus)

# Print the word frequencies
print(xxouris_frecuencias)

"""Ahora vamos a analizar las frecuencias obtenidas, para esto haremos uso de algunas funciones dadas en la especificación de la práctica"""

def plot_frequencies(frequencies: list, title="Freq of words", log_scale=False):
    x = list(range(1, len(frequencies)+1))
    plt.plot(x, frequencies, "-v")
    plt.xlabel("Freq rank (r)")
    plt.ylabel("Freq (f)")
    if log_scale:
        plt.xscale("log")
        plt.yscale("log")
    plt.title(title)
    plt.show()

def avg_len(tokens: list) -> float:
    return sum(len(token) for token in tokens) / len(tokens)

def get_words_from_vocabulary(vocabulary: Counter, n: int, most_common=True) -> list:
    pairs = vocabulary.most_common(n) if most_common else vocabulary.most_common()[:-n-1:-1]
    return [pair[0] for pair in pairs]

xxouris_head = get_words_from_vocabulary(xxouris_frecuencias, 20)
xxouris_tail = get_words_from_vocabulary(xxouris_frecuencias, 20, most_common=False)

print("Palabras mas comunes del Xxöuris:")
print(xxouris_head)
print("Palabras menos comunes del Xxöuris:")
print(xxouris_tail)


print ("Longitud promedio de las palabras más frecuentes:", avg_len(xxouris_head))
print ("Longitud promedio de las palabras menos frecuentes:", avg_len(xxouris_tail))

plot_frequencies(xxouris_frecuencias.values(), f"Frequencias de palabras mas comunes del Xxouris", log_scale=True)

"""Se vuelve evidente que **el Xxoüris no está cumpliendo los principios de la Ley de Zipf** para las lenguas: Ni la longitud promedio de los tipos más frecuentes es mas del doble de las mas frecuentes, ni la distribución de las frecuencias respeta el patrón logaritmico.

Esto puede deberse a que el modelado del lenguaje que estamos haciendo es muy simplista: únicamente nos estamos preocupando de generar cadenas de palabras que asemejen en apariencia la estructura sintáctica de un idioma, obviando otros atributos como la semántica o la pragmática del lenguaje, los cuales determinan mas atributos que si se ven reflejados en patrones como la Ley de Zipf, de Heap, entre otros.
"""