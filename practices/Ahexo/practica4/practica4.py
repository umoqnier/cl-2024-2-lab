# -*- coding: utf-8 -*-
"""Practica4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gSPcdHFMevz83qRmkH8P0Tla0BtzAJ9Z

# Práctica 4: Tokenización
Elaborado por: Alejandro Axel Rodríguez Sánchez  
Correo: ahexo@ciencias.unam.mx   
Github: @Ahexo  
Número de Cuenta: 315247697  
Institución: Facultad de Ciencias UNAM  
Asignatura: Lingüística computacional  
Semestre: 2024-2  
Grupo: 7014

Como siempre, comenzamos importando paquetería necesaria.
"""

#!pip install elotl
#!pip install subword-nmt

import nltk
from nltk.corpus import brown
import elotl.corpus

import re
import math
import os
from collections import Counter
from tabulate import tabulate

"""Descargamos los corpus y extraemos las listas de palabras:"""

nltk.download('brown')
brown_words = [word for word in brown.words() if re.match("\w", word)]
brown_words[0:15]

axolotl = elotl.corpus.load("axolotl")
axolotl_words = [word for row in axolotl for word in row[1].lower().split()]
axolotl_words[0:15]

"""Colocamos algunas funciones auxiliares traídas de la especificación de la práctica:"""

def calculate_entropy(corpus: list[str]) -> float:
    """
    Calculamos la entropía en un corpus con la formula conocida.

    Parameters:
    corpus (list[str]): Corpus de palabras sobre el cual operar.
    """
    words_counts = Counter(corpus)
    total_words = len(corpus)
    probabilities = {word: count / total_words for word, count in words_counts.items()}
    entropy = -sum(p * math.log2(p) for p in probabilities.values())
    return entropy

"""Con esto ya podemos calcular las entropías *word-level*:"""

brown_entropia_wl = calculate_entropy(brown_words[:100000])
print(f'Entropía para brown (word-level): {brown_entropia_wl}')

axolotl_entropia_wl = calculate_entropy(axolotl_words[:100000])
print(f'Entroía para axolotl (word-level): {axolotl_entropia_wl}')

"""Ahora vamos a calcular entropía empleando una tokenización *Byte-Pair Encodinng* (BPE), para esto tenemos que patrir nuestros conjuntos de palabras en un sub-conjunto de entrenamiento y uno de evaulación, para posteriormente usar las facilidades del paquete `subword-nmt`."""

def guardar_corpus(input:list, filename:str):
  """
  Guardamos una lista de palabras en un archivo de texto plano persistente.
  Lo vamos a usar para entrenar con subword-mnt y guardar distintos corpus,
  tanto de entrenamiento como de prueba.

  Parameters:
  input (list): Lista de palabras (corpus) a escribir.
  filename (str): Nombre del archivo.
  """
  try:
    os.makedirs('./bpe', exist_ok=True)
  except OSError as e:
    print(f"Error: {e.strerror}")

  with open(f'./bpe/{filename}', 'w') as f:
      for word in input:
          f.write(f"{word}\n")

"""Partimos nuestros corpus de palabras en los sub-conjuntos necesarios y los guardamos en archivos de texto."""

brown_words_train_count = len(axolotl_words) - round(len(axolotl_words)*.30)
brown_words_train = brown_words[:brown_words_train_count]
brown_words_test = brown_words[brown_words_train_count:]
guardar_corpus(brown_words_train, 'brown_words_train.txt')
guardar_corpus(brown_words_test, 'brown_words_test.txt')

axolotl_words_train_count = len(axolotl_words) - round(len(axolotl_words)*.30)
axolotl_words_train = axolotl_words[:axolotl_words_train_count]
axolotl_words_test = axolotl_words[axolotl_words_train_count:]
guardar_corpus(axolotl_words_train, 'axolotl_words_train.txt')
guardar_corpus(axolotl_words_test, 'axolotl_words_test.txt')

"""Le pasamos nuestros corpus al subword-nmt para generar los modelos y las evaluaciones."""

# Entrenamos los modelos
os.system('subword-nmt learn-bpe -s 500 < ./bpe/brown_words_train.txt > ./bpe/brown.model')
os.system('subword-nmt learn-bpe -s 500 < ./bpe/axolotl_words_train.txt > ./bpe/axolotl.model')

# Evaluamos los modelos
os.system('subword-nmt apply-bpe -c ./bpe/brown.model < ./bpe/brown_words_test.txt > ./bpe/brown_tokenized.txt')
os.system('subword-nmt apply-bpe -c ./bpe/axolotl.model < ./bpe/axolotl_words_test.txt > ./bpe/axolotl_tokenized.txt')

"""Ahora vamos a revisar los resultados para el corpus Brown:"""

with open('./bpe/brown_tokenized.txt', 'r') as f:
    brown_tokenized = f.read().split()
    brown_token_counts = Counter(brown_tokenized)

    brown_headers = ["Número", "Token", "Conteo"]
    brown_table_data = []
    pos = 0
    for token, count in brown_token_counts.most_common(15):
      pos += 1
      brown_table_data.append([pos, token, count])

    print(f"\nTokens mas frecuentes en Brown (inglés)")
    print(tabulate(brown_table_data, headers=brown_headers, tablefmt="simple_outline"))

    brown_entropia_bpe = calculate_entropy(brown_tokenized)
    print(f'Entropía para brown (bpe): {brown_entropia_bpe}\n')

with open('./bpe/axolotl_tokenized.txt', 'r') as f:
    axolotl_tokenized = f.read().split()
    axolotl_token_counts = Counter(axolotl_tokenized)

    axolotl_headers = ["Número", "Token", "Conteo"]
    axolotl_table_data = []
    pos = 0
    for token, count in axolotl_token_counts.most_common(15):
      pos += 1
      axolotl_table_data.append([pos, token, count])

    print(f"\nTokens mas frecuentes en axolotl (náhuatl)")
    print(tabulate(axolotl_table_data, headers=axolotl_headers, tablefmt="simple_outline"))

    axolotl_entropia_bpe = calculate_entropy(axolotl_tokenized)
    print(f'Entropía para axolotl (bpe): {axolotl_entropia_bpe}\n')

"""Finalmente, reportamos todas las medidas de entropía obtenidas:"""

results_headers = ["Corpus", "Lengua", "Entropía (Word-Level)", "Entropía (BPE)"]
results_table_data = []
results_table_data.append(["Axolotl", "Náhuatl", axolotl_entropia_wl, axolotl_entropia_bpe])
results_table_data.append(["Brown", "Inglés", brown_entropia_wl, brown_entropia_bpe])
print("Resultados finales:")
print(tabulate(results_table_data, headers=results_headers, tablefmt="simple_outline"))

"""## Extra: Normalizando para el náhuatl
Vamos a hacer una evaluación mas con el corpus de náhuatl, pero normalizado con las herramientas de elotl.

Importamos el normalizador de elotl (usaremos la normalización de la SEP):
"""

import elotl.nahuatl.orthography
nahuatl_normalizer = elotl.nahuatl.orthography.Normalizer("sep")

"""Generamos nuestro corpus de palabras:"""

print("Ejecutando una evaluación extra con una normalización del corpus de náhuatl:")
axolotl_normal_words = [nahuatl_normalizer.normalize(word) for row in axolotl for word in row[1].lower().split()]
axolotl_normal_words[0:15]

"""Calculamos entropía de Word-level:"""

axolotl_normal_entropia_wl = calculate_entropy(axolotl_normal_words[:100000])
print(f'Entroía para axolotl normalizado (word-level): {axolotl_normal_entropia_wl}')

"""Partimos el corpus para entrenar BPE:"""

axolotl_normal_words_train_count = len(axolotl_words) - round(len(axolotl_words)*.30)
axolotl_normal_words_train = axolotl_normal_words[:axolotl_normal_words_train_count]
axolotl_normal_words_test = axolotl_normal_words[axolotl_normal_words_train_count:]
guardar_corpus(axolotl_normal_words_train, 'axolotl_normal_words_train.txt')
guardar_corpus(axolotl_normal_words_test, 'axolotl_normal_words_test.txt')

"""Entrenamos y evaluamos:"""

# Entrenar
os.system('subword-nmt learn-bpe -s 500 < ./bpe/axolotl_normal_words_train.txt > ./bpe/axolotl_normal.model')
# Evaluar
os.system('subword-nmt apply-bpe -c ./bpe/axolotl_normal.model < ./bpe/axolotl_normal_words_test.txt > ./bpe/axolotl_normal_tokenized.txt')

"""Revisamos los resultados de BPE para el corpus del náhuatl normalizado:"""

with open('./bpe/axolotl_normal_tokenized.txt', 'r') as f:
    axolotl_normal_tokenized = f.read().split()
    axolotl_normal_token_counts = Counter(axolotl_normal_tokenized)

    axolotl_normal_headers = ["Número", "Token", "Conteo"]
    axolotl_normal_table_data = []
    pos = 0
    for token, count in axolotl_normal_token_counts.most_common(15):
      pos += 1
      axolotl_normal_table_data.append([pos, token, count])

    print(f"\nTokens mas frecuentes en axolotl_normal (náhuatl)")
    print(tabulate(axolotl_normal_table_data, headers=axolotl_normal_headers, tablefmt="simple_outline"))

    axolotl_normal_entropia_bpe = calculate_entropy(axolotl_normal_tokenized)
    print(f'Entropía para axolotl_normal (bpe): {axolotl_normal_entropia_bpe}\n')

"""Finalmente, comparamos con nuestras evaluaciones del corpus en náhuatl sin normalizar:"""

results_headers = ["Corpus", "Lengua", "Entropía (Word-Level)", "Entropía (BPE)"]
results_table_data = []
results_table_data.append(["Axolotl", "Náhuatl", axolotl_entropia_wl, axolotl_entropia_bpe])
results_table_data.append(["Axolotl (normalizado)", "Náhuatl", axolotl_normal_entropia_wl, axolotl_normal_entropia_bpe])
print("Resultados finales:")
print(tabulate(results_table_data, headers=results_headers, tablefmt="simple_outline"))

"""## Cuestionamientos

> ¿Aumento o disminuyó la entropia para los corpus? [Luego de tokenizar con BPE]

La entropía de ambos disminuyó luego de aplicar la tokenización con BPE, aproximadamente en un 30%:

| Corpus   | Lengua   |   Entropía (Word-Level) |   Entropía (BPE) |
|----------|----------|-------------------------|------------------|
| Axolotl  | Náhuatl  |                 11.4922 |          8.35031 |
| Brown    | Inglés   |                 10.6386 |          8.35453 |

Porcentaje de reducción para Axolotl:  
$(11.4922)x = 8.35031$  
$x \approx 72.66 \% $


Porcentaje de reducción para Brown:  
$(10.6386)x = 8.35453$  
$x \approx 78.53 \% $


> ¿Qué significa que la entropia aumente o disminuya en un texto?

Que este es menos o más predecible. Generalmente, una entropía alta es sinónimo de un texto con una variedad de tipos muy amplia, lo que se traduce en un texto mas errático. Al bajar la entropía, se entiende que hemos aplicado algún método para reducir estos últimos, de modo que el texto se vuelve más predecible.

> ¿Como influye la tokenizacion en la entropía de un texto?

En que la disminuye, pues a menos tipos que tokens, el corpus se vuelve más suceptible a predicciones.

### Sobre la evaluación con el corpus de náhuatl normalizado

Los resultados son ligeramente mejores luego de efectuar una normalización del corpus del náhuatl, sin cambios drásticos.

Porcentaje de reducción para Axolotl Normalizado:  
$(11.1388)x = 8.24771$  
$x \approx 74.04 \% $

La reducción de entropía mejora apenas un 2% respecto con el corpus sin normalizar.

Los tokens si vieron cambios mas sustanciales, lo cual es comprensible dado que la normalización suele modificar fuertemente las grafías del texto.

| Número | Normalizado | Original |
|--------|--------|--------|
| 1      | in     | yn     |
| 2      | ki@@   | in     |
| 3      | i@@    | i@@    |
| 4      | tla@@  | qui@@  |
| 5      | ka@@   | tla@@  |
| 6      | ti@@   | a@@    |
| 7      | a@@    | ti@@   |
| 8      | mo@@   | o@@    |
| 9      | o@@    | .      |
| 10     | s      | mo@@   |
| 11     | j@@    | te@@   |
| 12     | te@@   | ca@@   |
| 13     | .      | ,      |
| 14     | l@@    | to@@   |
| 15     | to@@   | y@@    |
"""